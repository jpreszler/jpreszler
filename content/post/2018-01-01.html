---
title: "Thoughts on Severe Class Imbalance "
author: "Jason Preszler"
date: 2018-01-02
categories: ["emergent-reducibility"]
tags: ["R","machine-learning","emergent-reducibility"]
---



<p>Besides lots of family time and the creation of this blog/website, this is what I’ve been thinking about over the winter break.</p>
<div id="background" class="section level1">
<h1>Background</h1>
<p>As part of my research in <em>emergent reducibility</em>, I’ve had to face a binary classification situation with severe class imbalance. Among brute-force searches, it seems that there’s roughly 1 case of emergent reducibility (what I’m looking for) for every 1 million irreducible cubic polynomials. It is known that there are infinitely many cubic polynomials with emergent reducibility.</p>
<p>One standard way of dealing with class imbalance is to artificially increase the incidence of positive cases in the training data, but I’ve seen very little about how to decide how much to adjust the ratio of the two classes - that’s what this post is about.</p>
</div>
<div id="training-data" class="section level1">
<h1>Training Data</h1>
<p>To examine the relationship of class imbalance on several classifiers, I build 21 training sets each with the same 52 cases of emergent reducibility and between 500 and 2500 (by 100 increments) polynomials without emergent reducibility. Each training set was used to train a variety of logristic regression, random forest, naive Bayes, and k-nearest neighbor models via caret.</p>
</div>
<div id="confusion-matrices" class="section level1">
<h1>Confusion Matrices</h1>
<p>Once the models were trained, they were all tested against the same data set with 23 cases of emergent reducibility (no overlap with training data) and 8000 cases without emergent reducibility. For each model and training set combination, a confusion “matrix” was build, this is in the file <a href="/post/confMats.csv">confMats.csv</a>. Let’s read that into R and add another variable, <em>mdlType</em> that’s either <em>logistic</em>, <em>RF</em>, or <em>other</em>. This is to facet some graphs later.</p>
<pre class="r"><code>confMats &lt;- read.csv(&quot;../../static/post/confMats.csv&quot;, header=TRUE)

logLocations &lt;- grep(&quot;lr&quot;, confMats$mdl)
rfLocations &lt;- grep(&quot;rf&quot;, confMats$mdl)

confMats$mdlType &lt;- vector(mode=&quot;character&quot;, length=length(confMats$mdl))

confMats[logLocations,]$mdlType &lt;- &quot;Logistic&quot;
confMats[rfLocations,]$mdlType &lt;- &quot;RF&quot;
confMats[!(1:length(confMats$mdl) %in% c(logLocations,rfLocations)),]$mdlType&lt;-&quot;Other&quot;</code></pre>
</div>
<div id="roc-plots" class="section level1">
<h1>ROC Plots</h1>
<p>Now we’ll plot our confusion matrices in ROC space, each point is a model and training set combo. I’ve facetted by model type for readability.</p>
<pre class="r"><code>library(ggplot2)

#11 distinct colors, courtesy of colorbrewer2.org
cb11&lt;-c(&#39;#a6cee3&#39;,&#39;#1f78b4&#39;,&#39;#b2df8a&#39;,&#39;#33a02c&#39;,&#39;#fb9a99&#39;,&#39;#e31a1c&#39;,&#39;#fdbf6f&#39;,&#39;#ff7f00&#39;,&#39;#cab2d6&#39;,&#39;#6a3d9a&#39;,&#39;#ffff99&#39;)
ggplot(confMats,aes(x=FP/(FP+TN),y=TP/(TP+FN),col=mdl))+geom_point()+facet_wrap(~mdlType)+scale_color_manual(values=cb11)+ggtitle(&quot;ROC Plots of Models and Class Imbalance &quot;)</code></pre>
<p><img src="/post/2018-01-01_files/figure-html/scatter-1.png" width="672" /></p>
<p>The model <em>max</em> seems to find the most, but this simply marks a polynomial as having emergent reducibility if any other model says it does. This indicates some models find cases that others miss (I have some nice heatmaps showing this also, for another day). The logistic regression models have much more irregular variation than I was expecting.</p>
<p>To see how varying the number of non-emergent reducibile polynomials impacts performance, I’ll throw in some animation:</p>
<pre class="r"><code>library(gganimate)

pathPlot &lt;- ggplot(confMats,aes(x=FP/(FP+TN),y=TP/(TP+FN),col=mdl,frame=ner))+geom_path(aes(cumulative=TRUE, group=mdl))+facet_wrap(~mdlType)+scale_color_manual(values=cb11)+ggtitle(&quot;Animated ROC Paths&quot;)

gganimate(pathPlot, &quot;../../static/post/pathPlot.gif&quot;)</code></pre>
<p>I’m saving the gif and then displaying it outside the code chunk. This is because animated graphs seem to be turned pink inside code chunks.</p>
<div class="figure">
<img src="/post/pathPlot.gif" alt="pathPlot.gif" />
<p class="caption">pathPlot.gif</p>
</div>
<p>The random forest and knn models seem pretty stable as the number of non-emergent reducible case changes. Looking at the number of true positives we see a gradual decline as <em>ner</em> increases:</p>
<pre class="r"><code>library(knitr)
nerRF.tab &lt;- xtabs(TP~ner+mdl, data=confMats[confMats$mdl %in% c(&quot;rfs&quot;,&quot;rfp&quot;,&quot;rfpp&quot;,&quot;rfsq&quot;,&quot;knn&quot;),], drop.unused.levels = TRUE)
kable(nerRF.tab)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">knn</th>
<th align="right">rfp</th>
<th align="right">rfpp</th>
<th align="right">rfs</th>
<th align="right">rfsq</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">500</td>
<td align="right">21</td>
<td align="right">19</td>
<td align="right">15</td>
<td align="right">20</td>
<td align="right">16</td>
</tr>
<tr class="even">
<td align="left">600</td>
<td align="right">21</td>
<td align="right">17</td>
<td align="right">13</td>
<td align="right">19</td>
<td align="right">15</td>
</tr>
<tr class="odd">
<td align="left">700</td>
<td align="right">20</td>
<td align="right">20</td>
<td align="right">13</td>
<td align="right">18</td>
<td align="right">17</td>
</tr>
<tr class="even">
<td align="left">800</td>
<td align="right">19</td>
<td align="right">17</td>
<td align="right">13</td>
<td align="right">18</td>
<td align="right">15</td>
</tr>
<tr class="odd">
<td align="left">900</td>
<td align="right">18</td>
<td align="right">18</td>
<td align="right">10</td>
<td align="right">18</td>
<td align="right">17</td>
</tr>
<tr class="even">
<td align="left">1000</td>
<td align="right">18</td>
<td align="right">16</td>
<td align="right">11</td>
<td align="right">16</td>
<td align="right">14</td>
</tr>
<tr class="odd">
<td align="left">1100</td>
<td align="right">17</td>
<td align="right">17</td>
<td align="right">12</td>
<td align="right">16</td>
<td align="right">14</td>
</tr>
<tr class="even">
<td align="left">1200</td>
<td align="right">17</td>
<td align="right">17</td>
<td align="right">12</td>
<td align="right">18</td>
<td align="right">16</td>
</tr>
<tr class="odd">
<td align="left">1300</td>
<td align="right">17</td>
<td align="right">18</td>
<td align="right">10</td>
<td align="right">15</td>
<td align="right">14</td>
</tr>
<tr class="even">
<td align="left">1400</td>
<td align="right">17</td>
<td align="right">13</td>
<td align="right">10</td>
<td align="right">16</td>
<td align="right">12</td>
</tr>
<tr class="odd">
<td align="left">1500</td>
<td align="right">14</td>
<td align="right">13</td>
<td align="right">11</td>
<td align="right">14</td>
<td align="right">10</td>
</tr>
<tr class="even">
<td align="left">1600</td>
<td align="right">15</td>
<td align="right">15</td>
<td align="right">10</td>
<td align="right">15</td>
<td align="right">13</td>
</tr>
<tr class="odd">
<td align="left">1700</td>
<td align="right">16</td>
<td align="right">16</td>
<td align="right">9</td>
<td align="right">16</td>
<td align="right">13</td>
</tr>
<tr class="even">
<td align="left">1800</td>
<td align="right">15</td>
<td align="right">15</td>
<td align="right">9</td>
<td align="right">14</td>
<td align="right">12</td>
</tr>
<tr class="odd">
<td align="left">1900</td>
<td align="right">16</td>
<td align="right">13</td>
<td align="right">9</td>
<td align="right">14</td>
<td align="right">13</td>
</tr>
<tr class="even">
<td align="left">2000</td>
<td align="right">14</td>
<td align="right">10</td>
<td align="right">8</td>
<td align="right">14</td>
<td align="right">10</td>
</tr>
<tr class="odd">
<td align="left">2100</td>
<td align="right">14</td>
<td align="right">11</td>
<td align="right">9</td>
<td align="right">13</td>
<td align="right">12</td>
</tr>
<tr class="even">
<td align="left">2200</td>
<td align="right">13</td>
<td align="right">12</td>
<td align="right">8</td>
<td align="right">11</td>
<td align="right">13</td>
</tr>
<tr class="odd">
<td align="left">2300</td>
<td align="right">16</td>
<td align="right">11</td>
<td align="right">9</td>
<td align="right">13</td>
<td align="right">9</td>
</tr>
<tr class="even">
<td align="left">2400</td>
<td align="right">13</td>
<td align="right">11</td>
<td align="right">7</td>
<td align="right">11</td>
<td align="right">9</td>
</tr>
<tr class="odd">
<td align="left">2500</td>
<td align="right">14</td>
<td align="right">11</td>
<td align="right">8</td>
<td align="right">11</td>
<td align="right">11</td>
</tr>
</tbody>
</table>
<p>The logistic regression models show the odd variation:</p>
<pre class="r"><code>TPnerLR.tab &lt;- xtabs(TP~ner+mdl, data=confMats[confMats$mdlType == &quot;Logistic&quot;,], drop.unused.levels = TRUE)
kable(TPnerLR.tab)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">lrp</th>
<th align="right">lrs</th>
<th align="right">lrsq</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">500</td>
<td align="right">10</td>
<td align="right">8</td>
<td align="right">12</td>
</tr>
<tr class="even">
<td align="left">600</td>
<td align="right">8</td>
<td align="right">2</td>
<td align="right">13</td>
</tr>
<tr class="odd">
<td align="left">700</td>
<td align="right">10</td>
<td align="right">0</td>
<td align="right">4</td>
</tr>
<tr class="even">
<td align="left">800</td>
<td align="right">3</td>
<td align="right">0</td>
<td align="right">6</td>
</tr>
<tr class="odd">
<td align="left">900</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">9</td>
</tr>
<tr class="even">
<td align="left">1000</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">11</td>
</tr>
<tr class="odd">
<td align="left">1100</td>
<td align="right">2</td>
<td align="right">0</td>
<td align="right">3</td>
</tr>
<tr class="even">
<td align="left">1200</td>
<td align="right">2</td>
<td align="right">0</td>
<td align="right">10</td>
</tr>
<tr class="odd">
<td align="left">1300</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left">1400</td>
<td align="right">6</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">1500</td>
<td align="right">12</td>
<td align="right">0</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left">1600</td>
<td align="right">4</td>
<td align="right">0</td>
<td align="right">3</td>
</tr>
<tr class="odd">
<td align="left">1700</td>
<td align="right">12</td>
<td align="right">0</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left">1800</td>
<td align="right">7</td>
<td align="right">0</td>
<td align="right">17</td>
</tr>
<tr class="odd">
<td align="left">1900</td>
<td align="right">3</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">2000</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="left">2100</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">2200</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">2</td>
</tr>
<tr class="odd">
<td align="left">2300</td>
<td align="right">18</td>
<td align="right">0</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="left">2400</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">8</td>
</tr>
<tr class="odd">
<td align="left">2500</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p>The variation across elements of the confusion matrices is perhaps best seen in the following plot:</p>
<pre class="r"><code>library(tidyr)
library(dplyr)

gather(confMats, key=Type, value=Count, -c(ner, mdl, mdlType)) %&gt;% ggplot(aes(x=ner, y=Count, col=mdl))+geom_line()+facet_wrap(~Type, scales = &quot;free_y&quot;)+ggtitle(&quot;Confusion Matrix Visual as Training Class Imbalance Changes&quot;)</code></pre>
<p><img src="/post/2018-01-01_files/figure-html/CMplot-1.png" width="672" /></p>
<p>Clearly, there’s something in the <em>ner</em> 1500,1700,1800, and 2300 training sets that really helps logistic models but not other model types. This is something to look into.</p>
<p>However, I’m still left wondering <em>What is the best ratio of classes in a training set?</em></p>
</div>
