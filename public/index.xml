<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jason I. Preszler on Jason I. Preszler</title>
    <link>/</link>
    <description>Recent content in Jason I. Preszler on Jason I. Preszler</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Jason Preszler</copyright>
    <lastBuildDate>Wed, 18 Sep 2019 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Rise Above the Noise</title>
      <link>/post/2019-09-19-elevation/</link>
      <pubDate>Wed, 18 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-09-19-elevation/</guid>
      <description>


&lt;p&gt;I’ve done some analysis of my gps running data &lt;a href=&#34;&#34;&gt;before&lt;/a&gt;, but mostly just some mapping. I’ve always wanted to bring in some more sophisticated analysis such as identifying runs with similar geographic features (e.g. track workouts) or identifying, categorizing, and comparing hills. To really get into either of these things, I first needed good elevation data which isn’t provided by my forerunner 220. In this post I’ll show some of the problems with the elevation data coming from my garmin 220, how to get elevation data from the RaceMap API (and compare a few other elevation api’s), and then examine how good the new elevation data is.&lt;/p&gt;
&lt;p&gt;First, let’s load the running data and standard libraries.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

pts = pd.read_csv(&amp;#39;../../static/files/points.csv&amp;#39;)
pts.info()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
## RangeIndex: 60920 entries, 0 to 60919
## Data columns (total 8 columns):
## Unnamed: 0    60920 non-null int64
## @lat          60920 non-null float64
## @lon          60920 non-null float64
## ele           60920 non-null float64
## time          60920 non-null object
## speed         60920 non-null float64
## pointID       60920 non-null int64
## runID         60920 non-null object
## dtypes: float64(4), int64(2), object(2)
## memory usage: 3.7+ MB&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;forerunner-elevation-issues&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Forerunner Elevation Issues&lt;/h2&gt;
&lt;p&gt;When I first looked into the elevation data off my garmin, two big issues jumped out and made me question how reliable it was:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;‘fake’ hills - large (30 to 60 meter) drops and returns to elevation over a less than a minute.&lt;/li&gt;
&lt;li&gt;disconnected loops - the start and end elevation of runs being dramatically off (20+ meters).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There is also a lot of general noise and at low elevation (near sea level) it often appears that I’m under water. Here are a few examples.&lt;/p&gt;
&lt;div id=&#34;o81129&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;76O81129&lt;/h3&gt;
&lt;p&gt;This run took place in Boise, and over a fairly flat area. Here’s the elevation profile and a lon/lat plot.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;sevensix = pts[pts[&amp;#39;runID&amp;#39;]==&amp;#39;76O81129&amp;#39;]
fig, ax = plt.subplots(1,2)
sevensix.plot(&amp;#39;pointID&amp;#39;, &amp;#39;ele&amp;#39;, ax=ax[0])
ax[0].set_title(&amp;#39;Elevation Profile 76O81129&amp;#39;)
ax[0].set_ylabel(&amp;#39;Elevation (m)&amp;#39;)
sevensix.plot(&amp;#39;@lon&amp;#39;, &amp;#39;@lat&amp;#39;, ax=ax[1], legend=False)
ax[1].set_title(&amp;#39;Lon/Lat Map 76O81129&amp;#39;)
plt.tight_layout()
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-09-19-elevation_files/figure-html/unnamed-chunk-2-1.png&#34; /&gt;&lt;!-- --&gt;
The start of the elevation profile shows the ‘fake hill’ type of problem of approximately 50 meters! Furthermore, the map clearly shows what appears to be a track (see the oval?) but the elevation profile fails to show a long nearly flat segment.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;b2312&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;638B2312&lt;/h3&gt;
&lt;p&gt;This run was an out and back along the beach in the Outer Banks. This is very flat near the shoreline. Here’s the elevation profile and a lon/lat plot.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;sixthree = pts[pts[&amp;#39;runID&amp;#39;]==&amp;#39;638B2312&amp;#39;]
fig, ax = plt.subplots(1,2)
sixthree.plot(&amp;#39;pointID&amp;#39;, &amp;#39;ele&amp;#39;, ax=ax[0])
ax[0].set_title(&amp;#39;Elevation Profile 638B2312&amp;#39;)
ax[0].set_ylabel(&amp;#39;Elevation (m)&amp;#39;)
sixthree.plot(&amp;#39;@lon&amp;#39;, &amp;#39;@lat&amp;#39;, ax=ax[1], legend=False)
ax[1].set_title(&amp;#39;Lon/Lat Map 638B2312&amp;#39;)
plt.tight_layout()
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-09-19-elevation_files/figure-html/unnamed-chunk-3-1.png&#34; /&gt;&lt;!-- --&gt;
The mean elevation of the first 5 points is &lt;span class=&#34;math inline&#34;&gt;\(-5.8\)&lt;/span&gt; meters, while the last 5 points is &lt;span class=&#34;math inline&#34;&gt;\(-10.9\)&lt;/span&gt; meters. So we ended 5 meters lower than we started. The garmin records data at 6 second intervals, so these are two 30 second average elevations. In reality the elevation wasn’t changing and I was at sea level, both values should be nearly 0 with much less variation then we’re seeing.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;elevation-apis-to-the-rescue&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Elevation API’s to the Rescue&lt;/h2&gt;
&lt;p&gt;Some time ago, I had heard the Google maps had an elevation api that would give you the elevation of a given lat/lon pair. This seemed like a good way to get reliable elevation data, however Google’s elevation api is no longer free. So I began looking for a replacement, which is apparently a popular thing. I found a number of possibilities, with the two most promising being &lt;a href=&#34;http://elevation-api.io&#34;&gt;elevation-api.io&lt;/a&gt; and &lt;a href=&#34;https://racemap.com&#34;&gt;racemap&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Elevation-api.io is simple, but to get the resolution I needed it would have cost money. The costs are low, and I could have gotten elevations for all my points for a few dollars at most, but I wanted to find a free version if possible. If nothing free could be found, this is what I would use though.&lt;/p&gt;
&lt;p&gt;Racemap came through with a free api at good resolution. Like elevation-api.io, it works with simple get or post queries from json arrays of latitude and longitude. After a little experimentation, I wrote the following &lt;a href=&#34;https://github.com/jpreszler/runalytics/blob/master/RM-elevation.py&#34;&gt;script&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
import pandas as pd
import requests

def get_good_elevation(run_id):
    rmurl = &amp;#39;https://elevation.racemap.com/api&amp;#39;
    runll = pts[pts[&amp;#39;runID&amp;#39;]==run_id][[&amp;#39;@lat&amp;#39;,&amp;#39;@lon&amp;#39;]].to_json(orient=&amp;#39;values&amp;#39;)
    post_resp = requests.post(url=rmurl, data=runll)

    return(post_resp.json())


if __name__ == &amp;#39;__main__&amp;#39;:
    pts = pd.read_csv(&amp;#39;data/points.csv&amp;#39;)
    elevDF = pd.DataFrame(columns=[&amp;#39;runID&amp;#39;, &amp;#39;pointID&amp;#39;, &amp;#39;RMelev&amp;#39;])
    for rid in pts.runID.unique():
        runDF = pts[pts[&amp;#39;runID&amp;#39;]==rid][[&amp;#39;runID&amp;#39;,&amp;#39;pointID&amp;#39;]]
        runDF[&amp;#39;RMelev&amp;#39;] = get_good_elevation(rid)
        elevDF = pd.concat([elevDF, runDF], axis=0)

    elevDF.to_csv(&amp;#39;data/pt-elevation.csv&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s pretty self explanatory, with the only interesting thing being pandas &lt;code&gt;.to_json(orient=&#39;values&#39;)&lt;/code&gt;. This dataframe method converts the dataframe to a json object. The &lt;code&gt;values&lt;/code&gt; orientation makes the object and array of arrays, where each inner array is a row of the dataframe. That is, it converts the latitude and longitude columns into an array of points. Everything else if just subsetting, making the post request, merging results together. I’m making the post request for each run because there’s a payload limit of around 10,000 points and I have 60,000. Making the code work for a single run seemed to be the most modular approach.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;great-elevation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Great Elevation?&lt;/h2&gt;
&lt;p&gt;At this point, we’ve seen problems in the Garmin data and found free data online that fixes the problem. Well, let’s not be too quick to trust everything we find on the internet.&lt;/p&gt;
&lt;p&gt;Here are the two earlier runs, but not with the racemap elevation in the elevation profile:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;elev = pd.read_csv(&amp;#39;../../static/files/pt-elevation.csv&amp;#39;)
ptelev = pd.merge(pts, elev, how=&amp;#39;inner&amp;#39;, on=[&amp;#39;runID&amp;#39;, &amp;#39;pointID&amp;#39;])

sevensix = ptelev[ptelev[&amp;#39;runID&amp;#39;]==&amp;#39;76O81129&amp;#39;]
fig, ax = plt.subplots(1,2)
sevensix.plot(&amp;#39;pointID&amp;#39;, [&amp;#39;ele&amp;#39;, &amp;#39;RMelev&amp;#39;], ax=ax[0])
ax[0].set_title(&amp;#39;Elevation Profile 76O81129&amp;#39;)
ax[0].set_ylabel(&amp;#39;Elevation (m)&amp;#39;)
sevensix.plot(&amp;#39;@lon&amp;#39;, &amp;#39;@lat&amp;#39;, ax=ax[1], legend=False)
ax[1].set_title(&amp;#39;Lon/Lat Map 76O81129&amp;#39;)
plt.tight_layout()
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-09-19-elevation_files/figure-html/unnamed-chunk-5-1.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;sixthree = ptelev[ptelev[&amp;#39;runID&amp;#39;]==&amp;#39;638B2312&amp;#39;]
fig, ax = plt.subplots(1,2)
sixthree.plot(&amp;#39;pointID&amp;#39;, [&amp;#39;ele&amp;#39;, &amp;#39;RMelev&amp;#39;], ax=ax[0])
ax[0].set_title(&amp;#39;Elevation Profile 638B2312&amp;#39;)
ax[0].set_ylabel(&amp;#39;Elevation (m)&amp;#39;)
sixthree.plot(&amp;#39;@lon&amp;#39;, &amp;#39;@lat&amp;#39;, ax=ax[1], legend=False)
ax[1].set_title(&amp;#39;Lon/Lat Map 638B2312&amp;#39;)
plt.tight_layout()
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-09-19-elevation_files/figure-html/unnamed-chunk-6-1.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Both seem to be substantial improvements: no crazy hills, no running under water, and in the first it even looks like there’s a track towards the end!&lt;/p&gt;
&lt;p&gt;Looking at the elevation profiles for all runs, most get smoothed out with obvious problems fixed by the racemap elevation data. However, some runs have clear anomalies or look like totally different runs.&lt;/p&gt;
&lt;p&gt;For example, run &lt;code&gt;63L63409&lt;/code&gt; seems like the two sets of elevation data correspond to almost completely different paths:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;sixL = ptelev[ptelev[&amp;#39;runID&amp;#39;]==&amp;#39;63L63409&amp;#39;]
fig, ax = plt.subplots(1,2)
sixL.plot(&amp;#39;pointID&amp;#39;, [&amp;#39;ele&amp;#39;, &amp;#39;RMelev&amp;#39;], ax=ax[0])
ax[0].set_title(&amp;#39;Elevation Profile 63L63409&amp;#39;)
ax[0].set_ylabel(&amp;#39;Elevation (m)&amp;#39;)
sixL.plot(&amp;#39;@lon&amp;#39;, &amp;#39;@lat&amp;#39;, ax=ax[1], legend=False)
ax[1].set_title(&amp;#39;Lon/Lat Map 63L63409&amp;#39;)
plt.tight_layout()
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-09-19-elevation_files/figure-html/unnamed-chunk-7-1.png&#34; /&gt;&lt;!-- --&gt;
This is a run from Chicago while I was at a conference. It’s mostly along the Lake Front trail (flat) with the horizontal stretch being the Navy Pier. Even the big differences seem to support the racemap data.&lt;/p&gt;
&lt;p&gt;Another example, run &lt;code&gt;53GB3110&lt;/code&gt; has the largest average absolute difference between the garmin and racemap elevation data. Looking at the map and profile, it’s clear why (but only if you no Tacoma geography like I do!):&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fivethree = ptelev[ptelev[&amp;#39;runID&amp;#39;]==&amp;#39;53GB3110&amp;#39;]
fig, ax = plt.subplots(1,2)
fivethree.plot(&amp;#39;pointID&amp;#39;, [&amp;#39;ele&amp;#39;, &amp;#39;RMelev&amp;#39;], ax=ax[0])
ax[0].set_title(&amp;#39;Elevation Profile 53GB3110&amp;#39;)
ax[0].set_ylabel(&amp;#39;Elevation (m)&amp;#39;)
fivethree.plot(&amp;#39;@lon&amp;#39;, &amp;#39;@lat&amp;#39;, ax=ax[1], legend=False)
ax[1].set_title(&amp;#39;Lon/Lat Map 53GB3110&amp;#39;)
plt.tight_layout()
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-09-19-elevation_files/figure-html/unnamed-chunk-8-1.png&#34; /&gt;&lt;!-- --&gt;
The huge differences where racemap flattens to 0 elevation is the Tacoma Narrows Bridge. While racemap fixes the beginning of the run nicely, and smooths everything, the garmin is given a better measurement of the elevation while on the bridge. This possess a big challenge going forward - usually the racemap elevation is best, but at least sometimes I’ll need the original garmin. Can we make a smart system that knows when to use each? That’s one goal of &lt;code&gt;Runalytics&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Poisson Process Simulation</title>
      <link>/post/2019-09-09-poisson-simulation/</link>
      <pubDate>Mon, 09 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-09-09-poisson-simulation/</guid>
      <description>


&lt;p&gt;While continuing to work through BDA3, and decided to revisit some of the earlier exercises that I had done in &lt;code&gt;R&lt;/code&gt;. Problem 9 of chapter 1 asks to simulate a medical clinic with 3 doctors, patients arriving according to an exponential distribution with rate 10 minutes between 9AM and 4PM and each patient needing an appointment length uniformly distributed between 5 and 10 minutes. We are interested in things like the number of patients seen, average wait time, number of patients who had to wait, and when the clinic closes based on 1 simulated day and 100 simulated days (with intervals of each aggregation). These questions are pretty simple if you can simulate the process. I had done this in &lt;code&gt;R&lt;/code&gt; long ago, and recall a solution similar to &lt;a href=&#34;https://www.r-bloggers.com/bda3-chapter-1-exercise-9/&#34;&gt;Brian Callander’s&lt;/a&gt;, (I can’t find my own, probably done on an old work computer). Brian’s solution shows how easy it is to generate fairly complex data in R, but doesn’t use any date/time structure.&lt;/p&gt;
&lt;p&gt;I decided this would be a good thing to construct a python solution for, and made a general &lt;a href=&#34;../../static/queSimClass.py&#34;&gt;&lt;code&gt;QueueSimulator&lt;/code&gt; class&lt;/a&gt; since this seems like a problem that may need to be simulated often due to the diversity of Poisson processes one can encounter. First, I’ll present an overview of the class, then show it’s use to solve this particular problem.&lt;/p&gt;
&lt;div id=&#34;queuesimulator&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;QueueSimulator&lt;/h2&gt;
&lt;div id=&#34;constructor&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Constructor&lt;/h3&gt;
&lt;p&gt;First off, dependencies. This class loads &lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;pandas&lt;/code&gt;, &lt;code&gt;datetime&lt;/code&gt;, and &lt;code&gt;scipy.stats&lt;/code&gt; for fairly obvious reasons. Next, our &lt;code&gt;__init__&lt;/code&gt; function with header: &lt;code&gt;def __init__(self, numQueues, rate, start_hour, end_hour, appt_low, appt_high):&lt;/code&gt;. This initializes the data storage for the class and needs the basic info to set-up the simulation. Here’s the docstring for parameter descriptions:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;&amp;quot;&amp;quot;&amp;quot;
Arguments
------
numQueues: integer number of queues to handle arrivals
rate: float, &amp;#39;scale&amp;#39; parameter of exponential distribution, time between arrivals
start_hour: integer 0:24, hour of day that arrivals can start
end_hour: integer 0:24, hour of day that arrivals stop (no arrivals after, last arrival may be before)
appt_low: integer, lower bound of number of minutes needed to process an arrival
appt_high: integer, upper bound of number of minutes needed to process an arrival

Description
------
This class will simulate a Poisson process where items arrive between &amp;#39;start_hour&amp;#39; and &amp;#39;end_hour&amp;#39;,
with an item arriving every &amp;#39;rate&amp;#39; minutes on average. Each arrival requires between &amp;#39;appt_low&amp;#39;
and &amp;#39;appt_high&amp;#39; minutes to be processed (uniformly random) and there are &amp;#39;numQueues&amp;#39; available
to process the arrivals.

Example
-----
&amp;#39;simulation = QueueSimulator(3, 10, 9, 16, 5, 21)&amp;#39;
can be thought of as a clinic with 3 doctors, patients arriving every 10 minutes (exponentially dist.)
between 9AM and 4PM, and each patient requires an appointment between 5 and 20 minutes (uniformly dist.).
The appointment length is a property of each patient, and varies accordingly.

Calling &amp;#39;simulation.run_simulation(1)&amp;#39; will simulate one day at the clinic. Results stored in the
dataframe &amp;#39;simulation.results&amp;#39;
&amp;quot;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To actually accomplish this set-up, there are a few details worth getting into. To work with datetime objects directly, I wanted to minimize work for the user so they just provide &lt;code&gt;start_hour&lt;/code&gt; and &lt;code&gt;end_hour&lt;/code&gt; as integers, the constructor then runs:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;self.start = datetime.datetime.combine(datetime.date.today(), datetime.time(start_hour,0,0))
self.end = datetime.datetime.combine(datetime.date.today(), datetime.time(end_hour,0,0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This creates a &lt;code&gt;time&lt;/code&gt; object from the hours, then combines with today’s date to result in a full &lt;code&gt;datetime&lt;/code&gt; object. This was necessary for later use with time arithmetic and &lt;code&gt;timedelta&lt;/code&gt; objects which need a full &lt;code&gt;datetime&lt;/code&gt; not just a time.&lt;/p&gt;
&lt;p&gt;The next issue is that we don’t know how many patients will come each day so we create an &lt;code&gt;expected_count&lt;/code&gt; attribute which gives an upper bound of how many patients to simulate arriving. It’s an upper bound because we want to have enough coming in that when we cut-off at the &lt;code&gt;end_hour&lt;/code&gt; we haven’t already run out of patients. Here’s the main use of &lt;code&gt;scipy.stats&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;minutes_for_new_items = (end_hour-start_hour)*60 #minutes new patients seen
time_between_items = rate #exponential dist. time parameter
self.expected_count = int(np.ceil(stats.poisson.ppf(.9999, minutes_for_new_items/time_between_items)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, we need the queues to handle patients (i.e. the doctors) which uses the &lt;code&gt;datetime.datetime.combine&lt;/code&gt; tactic from the start and end time, as well as a list comprehension. This results in a list of when each doctor is available next. Initially this is the &lt;code&gt;start_hour&lt;/code&gt; and will be updated as we run the simulation.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;self.ques = [datetime.datetime.combine(datetime.datetime.today(), datetime.time(start_hour,0,0)) for i in range(0, self.numQueues)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Everything else is just initializing class attributes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;run-simulation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Run Simulation&lt;/h3&gt;
&lt;p&gt;The other ‘public’ method is &lt;code&gt;run_simulation&lt;/code&gt; which will populate a &lt;code&gt;results&lt;/code&gt; attribute dataframe with one row per ‘day’ we simulate, the number of days is the only parameter (default is 1). The results dataframe has the info we need to answer the problems questions. This is primarily a driver function that loops over the number of simulations, resetting the doctors queues, generating a single day of results, aggregating the single run and merging into the &lt;code&gt;results&lt;/code&gt; dataframe. The helper functions are: &lt;code&gt;__single_sim_results(self):&lt;/code&gt; which calls &lt;code&gt;__wait_time_update(self, item):&lt;/code&gt; on each item that arrives.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;single-simulation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Single Simulation&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;__single_sim_results&lt;/code&gt; function first generates all possible patients, arrival delays between patients (exponentially distributed), and then a list of actual arrival times.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;itemID = np.arange(0, self.expected_count)
minutes_to_arrival = np.random.exponential(scale = self.rate, size = self.expected_count)
arrival_times = [self.start+datetime.timedelta(minutes = i) for i in minutes_to_arrival.cumsum()]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we construct an &lt;code&gt;arrivals&lt;/code&gt; dataframe with one row per patient. This includes generating the appointment length the patient will need, cutting off any who arrive after 4PM, and initializing some other columns:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;arrivals = pd.DataFrame({
      &amp;#39;id&amp;#39;:itemID,
      &amp;#39;min_btwn_arrival&amp;#39;: minutes_to_arrival,
      &amp;#39;arrival_time&amp;#39;: arrival_times,
      &amp;#39;appt_length&amp;#39;: np.random.uniform(low=self.appt_low, high=self.appt_high, size=self.expected_count)
      })
arrivals = arrivals[arrivals[&amp;#39;arrival_time&amp;#39;]&amp;lt;=self.end]
arrivals[&amp;#39;appt_length_minutes&amp;#39;] = arrivals.appt_length.apply(lambda x: datetime.timedelta(minutes=x))
arrivals[&amp;#39;queue&amp;#39;] = np.nan
arrivals[&amp;#39;wait_time&amp;#39;] = datetime.timedelta(minutes=0)
arrivals[&amp;#39;appt_start_time&amp;#39;] = arrivals[&amp;#39;arrival_time&amp;#39;]
arrivals[&amp;#39;appt_end_time&amp;#39;] = arrivals[&amp;#39;arrival_time&amp;#39;]+arrivals[&amp;#39;appt_length_minutes&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You’ll notice we set the &lt;code&gt;appt_end_time&lt;/code&gt; to the arrival time plus the appointment length. Now we have to update this if the patient has to wait.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wait-time-update&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Wait Time Update&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;__wait_time_update&lt;/code&gt; routine adjusts the patients &lt;code&gt;appt_end_time&lt;/code&gt; and the doctors queue of availability. First, we find the next available doctor and assign the patient to them:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;first_que_avail = self.ques.index(min(self.ques))
item[&amp;#39;queue&amp;#39;] = first_que_avail&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If &lt;code&gt;min(self.ques)&lt;/code&gt; (next time the doctor is available) is earlier or at the patients &lt;code&gt;arrival_time&lt;/code&gt;, then we can just change the doctors availability to be at the already created &lt;code&gt;appt_end_time&lt;/code&gt; since there is no wait. If there is a wait, we must adjust the &lt;code&gt;appt_start_time&lt;/code&gt; and &lt;code&gt;appt_end_time&lt;/code&gt; and then update the doctors availability tot he appointment end time.&lt;/p&gt;
&lt;p&gt;We run all the &lt;code&gt;__wait_time_update&lt;/code&gt; via a simple &lt;code&gt;apply&lt;/code&gt; call in &lt;code&gt;__single_sim_results&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;arrivals = arrivals.apply(lambda x: self.__wait_time_update(x), axis=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The important thing about this is the &lt;code&gt;axis=1&lt;/code&gt; argument, which took a little debugging to fix. The &lt;code&gt;__wait_time_update&lt;/code&gt; function must be run on each row, and usually the default (&lt;code&gt;axis=0&lt;/code&gt;) accomplishes this when we hand a single column to apply (i.e. &lt;code&gt;df.col_name.apply(lambda x: 2*x)&lt;/code&gt; would double the values in &lt;code&gt;col_name&lt;/code&gt;). The &lt;a href=&#34;https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html&#34;&gt;pandas documentation&lt;/a&gt; has a nice example, showing that &lt;code&gt;df.apply(func, axis=0)&lt;/code&gt; will apply &lt;code&gt;func&lt;/code&gt; to each column’s values but &lt;code&gt;df.apply(func, axis=1)&lt;/code&gt; applies &lt;code&gt;func&lt;/code&gt; to each row’s values. I find this backwards from the usual “axis=1 is columns” standard.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;actual-simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Actual Simulation&lt;/h2&gt;
&lt;p&gt;Now we can use the class to produce simulated data and examine results. First I’ll import the class and initialize a simulation.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from queSimClass import QueueSimulator

first_sim = QueueSimulator(3, 10, 9, 16, 5, 21)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’m deviating from the directions a bit here as I’m allowing appointments to be between 5 and 20 minutes (the 21 is because upper bounds aren’t inclusive in python, which is possibly the most standard thing I’ve encountered in the language).&lt;/p&gt;
&lt;p&gt;Here’s the result of simulating one day:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;first_sim.run_simulation(1)
print(first_sim.results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   simulation num_items  ...                 close_time last_appt_to_close_minutes
## 0          0        50  ... 2019-09-09 16:17:33.068984                   17.55115
## 
## [1 rows x 6 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we want data on 100 or 1000 simulated days. Rather than numeric summaries, I’ll make some graphs:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;first_sim.run_simulation(100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## /home/jpreszler/github/jpreszler/content/post/queSimClass.py:112: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version
## of pandas will change to not sort by default.
## 
## To accept the future behavior, pass &amp;#39;sort=False&amp;#39;.
## 
## To retain the current behavior and silence the warning, pass &amp;#39;sort=True&amp;#39;.
## 
##   self.results = pd.concat([self.results, run_results], ignore_index=True)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import matplotlib.pyplot as plt
import seaborn as sns

sns.distplot(first_sim.results[&amp;#39;wait_count&amp;#39;])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-09-09-poisson-simulation_files/figure-html/unnamed-chunk-11-1.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Generally less than half of all patients have to wait at all, but to see nice graphs of the wait time and close time, we need to get out of datetime format by applying &lt;code&gt;total_seconds&lt;/code&gt; and dividing by 60 to convert to minutes.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;sns.distplot(first_sim.results.avg_wait_time.dt.total_seconds().div(60))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-09-09-poisson-simulation_files/figure-html/unnamed-chunk-12-1.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Similarly, we can plot the distribution of closing times as minutes after 4PM:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import datetime

sns.distplot((first_sim.results.close_time-datetime.datetime.combine(datetime.date.today(),datetime.time(16,0,0))).dt.total_seconds().div(60))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-09-09-poisson-simulation_files/figure-html/unnamed-chunk-13-1.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;The negative times mean doctors were available to see patients, but no new patients came between the last appointment end and 4PM.&lt;/p&gt;
&lt;p&gt;Next, we should incorporate some realism into our simulation - doctors need some breaks and likely pauses between patients to maintain charts, but I’ll leave that as an exercise to the reader…&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Mass Shooting Changepoint</title>
      <link>/post/2019-08-23-mass-shoot/</link>
      <pubDate>Fri, 23 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-08-23-mass-shoot/</guid>
      <description>


&lt;p&gt;Every time there’s news about a mass shooting I feel like doing some type of data analysis about gun violence. With the shootings in Dayton and El Paso, as well as news of several likely shootings being prevented, I thought I would actually follow through with some analysis. Having been a senior in high school (in California) when the Columbine shooting took place, and also living in Salt Lake during the Trolley Square shooting I’ve seen the impacts of these tragedies and feel as though they are happening more frequently. This seemed like a natural thing to quantify: have mass shooting become more common or more severe?&lt;/p&gt;
&lt;div id=&#34;data-exploration&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Exploration&lt;/h2&gt;
&lt;p&gt;Mother Jones maintains a dataset on &lt;a href=&#34;https://www.motherjones.com/politics/2012/12/mass-shootings-mother-jones-full-data/&#34;&gt;mass shootings&lt;/a&gt;, and there are many other datasets related to gun violence in general. I’m going to focus on the mass shooting data for this post. First, we’ll load the standard python packages and read in the data:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

sns.set_style(&amp;#39;ticks&amp;#39;)
sns.set_context(&amp;quot;paper&amp;quot;)

ms = pd.read_csv(&amp;#39;../../static/files/MJ-mass-shoot.csv&amp;#39;, parse_dates=[2])
ms.info()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
## RangeIndex: 114 entries, 0 to 113
## Data columns (total 24 columns):
## case                                114 non-null object
## location                            114 non-null object
## date                                114 non-null datetime64[ns]
## summary                             114 non-null object
## fatalities                          114 non-null int64
## injured                             114 non-null int64
## total_victims                       114 non-null int64
## location.1                          114 non-null object
## age_of_shooter                      114 non-null int64
## prior_signs_mental_health_issues    114 non-null object
## mental_health_details               114 non-null object
## weapons_obtained_legally            114 non-null object
## where_obtained                      114 non-null object
## weapon_type                         114 non-null object
## weapon_details                      114 non-null object
## race                                114 non-null object
## gender                              114 non-null object
## sources                             114 non-null object
## mental_health_sources               114 non-null object
## sources_additional_age              114 non-null object
## latitude                            114 non-null float64
## longitude                           114 non-null float64
## type                                114 non-null object
## year                                114 non-null int64
## dtypes: datetime64[ns](1), float64(2), int64(5), object(16)
## memory usage: 21.5+ KB&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In my transition from R to Python, I’ve missed the basic variable info of RStudio and the &lt;code&gt;info()&lt;/code&gt; call on a pandas dataframe at least gives the basic type data for each variable.&lt;/p&gt;
&lt;p&gt;I’m interested in frequency and severity of these shootings, so let’s first look at fatalities over time:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;ms.plot(x=&amp;#39;date&amp;#39;, y=&amp;#39;fatalities&amp;#39;)
plt.ylabel(&amp;#39;Fatalities&amp;#39;)
plt.title(&amp;#39;Mass Shooting Fatalities&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-08-23-mass-shoot_files/figure-html/unnamed-chunk-1-1.png&#34; /&gt;&lt;!-- --&gt;
I would have also looked at injuries, but the Las Vegas shooting of 2017 makes a plot of both unreadable. The above plot does indicate both a frequency and severity increase.&lt;/p&gt;
&lt;p&gt;To investigate change over time, I’m going to group the data by year and aggregate some of the features.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;annualized = pd.DataFrame(ms.groupby(&amp;#39;year&amp;#39;).case.agg(&amp;#39;count&amp;#39;))
annualized.columns = [&amp;#39;count&amp;#39;]
annualized[&amp;#39;total_fatalities&amp;#39;] = ms.groupby(&amp;#39;year&amp;#39;).fatalities.agg(&amp;#39;sum&amp;#39;)
annualized[&amp;#39;total_injuries&amp;#39;] = ms.groupby(&amp;#39;year&amp;#39;).injured.agg(&amp;#39;sum&amp;#39;)
annualized[&amp;#39;total_victims&amp;#39;] = ms.groupby(&amp;#39;year&amp;#39;).total_victims.agg(&amp;#39;sum&amp;#39;)
annualized[&amp;#39;fatalities_per_shooting&amp;#39;] = annualized.total_fatalities/annualized[&amp;#39;count&amp;#39;]
annualized[&amp;#39;mean_fatality_rate&amp;#39;] = annualized[&amp;#39;fatalities_per_shooting&amp;#39;]/annualized[&amp;#39;total_victims&amp;#39;]
annualized[&amp;#39;mean_shooter_age&amp;#39;] = ms.groupby(&amp;#39;year&amp;#39;).age_of_shooter.agg(&amp;#39;mean&amp;#39;)
annualized.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       count  total_fatalities  ...  mean_fatality_rate  mean_shooter_age
## year                           ...                                      
## 1982      1                 8  ...            0.727273              51.0
## 1984      2                28  ...            0.291667              40.0
## 1986      1                15  ...            0.714286              44.0
## 1987      1                 6  ...            0.300000              59.0
## 1988      1                 7  ...            0.636364              39.0
## 
## [5 rows x 7 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this dataframe, I can now look at some of the aggregated data.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;annualized = annualized.reset_index()
fig = plt.figure()
ax1 = fig.add_subplot(121)
annualized.plot(x=&amp;#39;year&amp;#39;, y=[&amp;#39;total_fatalities&amp;#39;, &amp;#39;fatalities_per_shooting&amp;#39;], ax=ax1)
ax1.set_ylabel(&amp;#39;Fatalities&amp;#39;)
ax2 = fig.add_subplot(122)
annualized.plot(x=&amp;#39;year&amp;#39;, y=&amp;#39;count&amp;#39;, ax=ax2)
ax2.set_ylabel(&amp;#39;Number of Shootings&amp;#39;)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-08-23-mass-shoot_files/figure-html/unnamed-chunk-3-1.png&#34; /&gt;&lt;!-- --&gt;
The relatively flat &lt;code&gt;fatalities_per_shooting&lt;/code&gt; line seems to indicate that the severity increase is largely coming from an increase in the number of mass shootings as opposed to each shooting being more severe (some type of upward trend). The &lt;code&gt;count&lt;/code&gt; graph is separate because of it’s much smaller scale, but it’s shape and pattern look very similar to the &lt;code&gt;total_fatalities&lt;/code&gt; line. The &lt;code&gt;count&lt;/code&gt; graph makes it seem that the frequency of mass shootings has increased. Before getting into any serious analysis, let’s make sure that we have data for each year:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(set(range(1982, 2020)).difference(set(annualized.year)))

#fill in the 3 years with no shootings&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## {1985, 2002, 1983}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;no_shoot = pd.DataFrame({&amp;#39;year&amp;#39;:[1983, 1985, 2002]})
annualized = annualized.merge(no_shoot, on=&amp;#39;year&amp;#39;, how=&amp;#39;outer&amp;#39;)
annualized = annualized.reset_index(drop=True).fillna(0)
annualized = annualized.sort_values(&amp;#39;year&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;change-point-introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Change Point Introduction&lt;/h2&gt;
&lt;p&gt;It seems fairly reasonable to assume that the number of mass shootings in the US in a year should be Poisson distributed. The Poisson distribution models “small” count data over time or space and has been used to describe deaths from horse-kick in the Prussian Cavalry, the number of tornadoes in a year, the number of wrongful convictions in a year (Poisson’s original use of the distribution), the number of visitors to a website, or aphids on a leaf. It is characterized by a single rate parameter that describes how frequently these events occur “on average”. If mass shootings have become more frequent, then there would a rate at which they occurred for a period of time, followed by a new rate for another period of time. This change is a “change point” and we would like to (a) determine if a change point took place (and when) and (b) what the rates before and after the change point are.&lt;/p&gt;
&lt;p&gt;Prior to 2010, the count of mass shooting graph seems like the same Poisson rate is likely driving things, so it’s reasonable to look for a single change point. To find the change point and estimate the rates on either side, I’m going to use a Bayesian model and &lt;a href=&#34;https://docs.pymc.io&#34;&gt;pymc3&lt;/a&gt;, which I developed some familiarity with when developing my &lt;a href=&#34;https://github.com/jpreszler/realallocator_full&#34;&gt;Insight project&lt;/a&gt;. There are also numerous examples of this type of model looking at coal mine accident data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mass-shooting-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mass Shooting Model&lt;/h2&gt;
&lt;p&gt;Building a Bayesian model for change points requires a prior on when the changepoint is, priors on the rate before and after, and a Poisson likelihood using these rates and observed data. In pymc3, this is done as follows:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pymc3 as pm

with pm.Model() as cpm:
    T = pm.Uniform(&amp;#39;changepoint&amp;#39;, 1982, 2019)
    rates = pm.HalfNormal(&amp;#39;rate&amp;#39;, sd=5, shape=2)
    idx = (annualized[&amp;#39;year&amp;#39;].values &amp;gt; T)*1
    count_obs = pm.Poisson(&amp;#39;count_obs&amp;#39;, mu=rates[idx], observed=annualized[&amp;#39;count&amp;#39;].values)
    step =  pm.Slice()
    trace = pm.sample(step=step, cores=4, progressbar=False, random_state=57)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; is the possible change point, uniformly distributed over interval from 1982 to 2019. Each rate is assumed to have the same shape, the positive half of a normal distribution with mean 0 and standard deviation 5. The &lt;code&gt;shape=2&lt;/code&gt; parameter creates a length 2 array of rates. The &lt;code&gt;idx&lt;/code&gt; variable is an index for the &lt;code&gt;rates&lt;/code&gt; array, either 0 or 1 depending on whether data is from before or after the change point. The likelihood is build by the &lt;code&gt;count_obs&lt;/code&gt; variable and the next two lines build the posterior distribution via a &lt;code&gt;Slice&lt;/code&gt; step-sampler (as opposed to the default &lt;code&gt;NUTS&lt;/code&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-diagnostics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Diagnostics&lt;/h2&gt;
&lt;p&gt;Whenever you do MCMC, you need to check convergence. The two simplest ways in pymc3 are (a) a traceplot and (b) posterior summary. The traceplot plots the trace object that was just created. Each of the MCMC chains graphed should show similar structure on the left and we should not see patterns or autocorrelation in the “fuzzy caterpillars” on the right.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;pm.traceplot(trace)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## array([[&amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f372eefa190&amp;gt;,
##         &amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f372ee4f450&amp;gt;],
##        [&amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f372ee7fe90&amp;gt;,
##         &amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f372ee0ca10&amp;gt;]],
##       dtype=object)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-08-23-mass-shoot_files/figure-html/unnamed-chunk-7-1.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;The posterior summary provides means, standard deviations, and high density intervals for each variable in our posterior, as well as an &lt;code&gt;Rhat&lt;/code&gt; value (which should be close to 1 for convergence). This provides both some diagnostic info, and the start our our inference from the posterior sample.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;trace_summ = pm.summary(trace)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## /usr/lib/python3.7/site-packages/pymc3/stats.py:982: FutureWarning: The join_axes-keyword is deprecated. Use .reindex or .reindex_like on the result to achieve the same functionality.
##   axis=1, join_axes=[dforg.index])&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reticulate)
library(knitr)
kable(py$trace_summ)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;sd&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mc_error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;hpd_2.5&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;hpd_97.5&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n_eff&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Rhat&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;changepoint&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2011.148295&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.1784383&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0490107&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2009.995179&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2014.808750&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;469.4743&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0012466&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;rate__0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.844826&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2490664&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0062827&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.380820&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.339726&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1534.3652&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9994486&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;rate__1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.201112&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9717701&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0288860&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.332041&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9.067021&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1163.6754&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9994642&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;The above summary shows the change point was likely in early 2011, and the rate before was just under 2 mass shootings per year to about 7 after. The high probability density intervals (Bayesian analogs of confidence intervals) show that there is a &lt;span class=&#34;math inline&#34;&gt;\(95\%\)&lt;/span&gt; probability the change point is between early 2010 to late 2012, early rate is between &lt;span class=&#34;math inline&#34;&gt;\(1.39\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(2.37\)&lt;/span&gt; and the late rate is between &lt;span class=&#34;math inline&#34;&gt;\(5.2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(8.86\)&lt;/span&gt;. The interpretation of these is more direct, and positive, than the frequentist confidence interval. PyMC3 also provides several ways to visualize the posteriors and I like the &lt;code&gt;plot_posterior&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;pm.plots.plot_posterior(trace)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## array([&amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f37322f2ed0&amp;gt;,
##        &amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f372ed7ac10&amp;gt;,
##        &amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f372e672510&amp;gt;],
##       dtype=object)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-08-23-mass-shoot_files/figure-html/unnamed-chunk-10-1.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;The skewness of the &lt;code&gt;changepoint&lt;/code&gt; posterior means the mean is lightly lower than the MAP value, but early to mid-2011 is most likely the point in time the rate change took place. Considering the convergence and clear difference of the rates (&lt;span class=&#34;math inline&#34;&gt;\(\sim 2\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\sim 7\)&lt;/span&gt;) it seems that a major change has taken place. Explaining why, and especially identifying underlying sociological causes of this change is important research that I’m sure people are investigating. Hopefully it doesn’t just rather dust in academic journals.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Pythonic SQL with SQLAlchemy</title>
      <link>/post/2019-05-18-pysql/</link>
      <pubDate>Sat, 18 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-05-18-pysql/</guid>
      <description>


&lt;p&gt;During my &lt;em&gt;Applied Databases&lt;/em&gt; course in Spring 2019, I gave my students a choice of which language to use to interact with SQL and relational databases. They had already learned core SQL and I only gave them 3 options: C++, R, and Python. The choices of R and python are natural given my data science interests and experience. Last year I just showed them R and got some complaints on evaluations (some people don’t think R is a “real language”). The option of C++ had 2 motivations: every student had 1 semester (many had 2) of the intro programming sequence in C++, and we were using SQLite which is written in C++. Fortunately, the majority picked Python and these are some examples we went through to start learning SQLAlchemy.&lt;/p&gt;
&lt;p&gt;Here’s the basic process of working with a database via &lt;code&gt;sqlalchemy&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create an engine via: &lt;code&gt;eng = create_engine(&#39;dialect+driver://db-location&#39;)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Create a connection via: &lt;code&gt;conn = eng.connection()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Create an object to hold metadata: &lt;code&gt;md = MetaData()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Create python objects for tables you’ll work with: &lt;code&gt;pyTable = Table(&#39;name&#39;,md, autoload=True, autoload_with=eng)&lt;/code&gt;&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Build queries - anything SQL can do, SQLAlchemy supports.&lt;/li&gt;
&lt;li&gt;execute queries: &lt;code&gt;proxy = conn.execute(query)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;fetch results: &lt;code&gt;results = proxy.fetchall()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;work with results (any python code you need)&lt;/li&gt;
&lt;li&gt;When finished, close the connection&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;creating-an-engine&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating an Engine&lt;/h2&gt;
&lt;p&gt;First we need an engine to handle communication with the database. This requires passing the SQL driver and dialect information as well as the location of the database. For SQLite, this is very simple but large RDBMS connections will require additional connection information.&lt;/p&gt;
&lt;p&gt;Once we create the engine, we’ll ask for the table names to see which tables are in the database (it should be empty in this case).&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import sqlalchemy as sa

eng = sa.create_engine(&amp;#39;sqlite:///../../static/files/countries.sqlite&amp;#39;)
eng.table_names()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## []&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;pandas-to-bulk-load-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pandas to Bulk Load Data&lt;/h2&gt;
&lt;p&gt;We could run SQL &lt;code&gt;INSERT&lt;/code&gt; commands, but &lt;code&gt;pandas&lt;/code&gt; can read common data files and export to SQL. Here we’ll put some city data into our database:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd
cityDF = pd.read_csv(&amp;#39;../../static/files/cities.csv&amp;#39;)
print(cityDF.head())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           name country_code  city_proper_pop  metroarea_pop  urbanarea_pop
## 0      Abidjan          CIV          4765000            NaN        4765000
## 1    Abu Dhabi          ARE          1145000            NaN        1145000
## 2        Abuja          NGA          1235880      6000000.0        1235880
## 3        Accra          GHA          2070463      4010054.0        2070463
## 4  Addis Ababa          ETH          3103673      4567857.0        3103673&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;cityDF.to_sql(&amp;#39;cities&amp;#39;, eng)

eng.table_names()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [&amp;#39;cities&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;pandas&lt;/code&gt; also provides ways to query a database as though it was a typical dataframe. If you already know &lt;code&gt;pandas&lt;/code&gt; and only need some basic queries then this is a natural path. Instead, we’re going to start using SQLAlchemy to write queries in a nice &lt;em&gt;pythonic&lt;/em&gt; way, but I’m not going to get into the full object-relational mapper that SQLAlchemy provides (but this is what to do if you want to store complex objects into a relational database).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sqlalchemy-queries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;SQLAlchemy Queries&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Warning about SQL injection&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;SQLAlchemy allows you to send raw query strings, &lt;strong&gt;but don’t do this&lt;/strong&gt;. If you’re doing interactive data analysis, raw query strings are quick and easy. If you’re building the query from user input, raw query strings are the easiest way to expose your database to SQL injection attacks.&lt;/p&gt;
&lt;p&gt;Now to the queries. We need a metadata object to hold table metadata. Then we have to have a table object to select from. Other SQL clauses are then methods on the select function:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;md = sa.MetaData()
cities = sa.Table(&amp;#39;cities&amp;#39;, md, autoload=True, autoload_with=eng)

stmt =  sa.sql.select([cities]).where(
  cities.columns.country_code==&amp;#39;USA&amp;#39;
  ).order_by(
    cities.columns.city_proper_pop.desc()
    )

results = eng.execute(stmt).fetchall()
print(results[0:10])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [(153, &amp;#39;New York City&amp;#39;, &amp;#39;USA&amp;#39;, 8550405, 20182305.0, 8550405), (124, &amp;#39;Los Angeles&amp;#39;, &amp;#39;USA&amp;#39;, 3884307, 15058000.0, 3884307), (45, &amp;#39;Chicago&amp;#39;, &amp;#39;USA&amp;#39;, 2695598, 9156000.0, 2695598), (87, &amp;#39;Houston&amp;#39;, &amp;#39;USA&amp;#39;, 2489558, 6490180.0, 2489558), (165, &amp;#39;Philadelphia&amp;#39;, &amp;#39;USA&amp;#39;, 1567872, 6069875.0, 1567872), (167, &amp;#39;Phoenix&amp;#39;, &amp;#39;USA&amp;#39;, 1563025, 4574531.0, 1563025), (186, &amp;#39;San Antonio&amp;#39;, &amp;#39;USA&amp;#39;, 1469845, 2454061.0, 1469845), (187, &amp;#39;San Diego&amp;#39;, &amp;#39;USA&amp;#39;, 1394928, 3095313.0, 1394928), (55, &amp;#39;Dallas&amp;#39;, &amp;#39;USA&amp;#39;, 1317929, 7233323.0, 1317929)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After building the query &lt;code&gt;stmt&lt;/code&gt; we obtain results by (1) executing the query on the engine and (2) fetching all the results. As you can see from the printed results, what’s returned is a list of database tuples.&lt;/p&gt;
&lt;div id=&#34;pretty-print-results&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Pretty Print Results&lt;/h3&gt;
&lt;p&gt;Since a list of tuples is a mess to read (but fairly easy to work with), here’s a function to display results in a more human-readable format.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# requires import pandas as pd
def query_results(query, engine, number):
    results = engine.execute(query).fetchmany(number)
    resDF = pd.DataFrame(results)
    resDF.columns = results[0].keys()
    print(resDF)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;joins&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Joins&lt;/h2&gt;
&lt;p&gt;You can’t do much in a real database without joins, so let’s do an example. We’ll load the JACS database - containing data on papers published in the Journal of the American Chemical Society. This database has 3 tables: &lt;code&gt;Papers&lt;/code&gt;, &lt;code&gt;Authors&lt;/code&gt; and &lt;code&gt;Paper_Authors&lt;/code&gt;. The last one is a join table since we can’t have a list of authors for each paper in a relational database.&lt;/p&gt;
&lt;p&gt;Here’s the initial set-up for the new database.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;#engine
JACSeng = sa.create_engine(&amp;#39;sqlite:///../../static/files/jacs.sqlite&amp;#39;)
# connection
conn = JACSeng.connect()
#metadata and tables
md = sa.MetaData()
authors = sa.Table(&amp;#39;Authors&amp;#39;, md, autoload=True, autoload_with=JACSeng)
papers = sa.Table(&amp;#39;Papers&amp;#39;, md, autoload=True, autoload_with=JACSeng)
pa = sa.Table(&amp;#39;Paper_Authors&amp;#39;, md, autoload=True, autoload_with=JACSeng)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’ll write a query to get the titles and authors names of all papers in volume 2 of issue 118 (pretty arbitrary, but a realistic problem). First we &lt;code&gt;select&lt;/code&gt; then add a &lt;code&gt;select_from&lt;/code&gt; that contains the &lt;code&gt;join&lt;/code&gt;(s). Finally, we use our function to print the results.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;query118v2 = sa.select([papers.columns.title, authors.columns.surname, authors.columns.forename])
query118v2 = query118v2.select_from(
  papers.join(
    pa, pa.columns.paperID==papers.columns.paperID
    ).join(
      authors, pa.columns.authorID==authors.columns.authorID
      )
    )
query118v2 = query118v2.where(sa.and_(papers.columns.issue==118, papers.columns.volume==2))

query_results(query118v2, conn, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                                title   surname  forename
## 0  Evidence for Concert in the Thermal Unimolecul...  Gajewski    Joseph
## 1  Evidence for Concert in the Thermal Unimolecul...     Olson      Leif
## 2  Evidence for Concert in the Thermal Unimolecul...  Willcott        M.
## 3  Orthogonal Ligation of Unprotected Peptide Seg...       Liu  Chuan-Fa
## 4  Orthogonal Ligation of Unprotected Peptide Seg...       Rao     Chang&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;end&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;End&lt;/h2&gt;
&lt;p&gt;We can use similar commands to run any SQL query we want, including DDL commands to create tables.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Python Web Scraping</title>
      <link>/post/2019-04-21-pyscrape/</link>
      <pubDate>Sun, 21 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-04-21-pyscrape/</guid>
      <description>


&lt;p&gt;I was putting some data together about previous catalogs for students for projects in my Applied Databases course and realized that I was missing something. I had course info (subject, number, title and url) for the last 4 catalog years at the College of Idaho, but I didn’t have course descriptions! What a great chance to do some simple web scraping in python.&lt;/p&gt;
&lt;div id=&#34;data-import-and-cleaning&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data Import and Cleaning&lt;/h1&gt;
&lt;p&gt;Since I have a &lt;code&gt;csv&lt;/code&gt; file for each catalog year with a link to each course, I just needed to read the urls, extract the description from the page, and save the results. So let’s start by loading the data I had:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd

path = &amp;#39;../../static/files/&amp;#39;
files = [&amp;#39;class-list-2015-2016.csv&amp;#39;,
          &amp;#39;class-list-2016-2017.csv&amp;#39;,
          &amp;#39;class-list-2017-2018.csv&amp;#39;,
          &amp;#39;class-list-2018-2019.csv&amp;#39;]

dfs = []
for file in files:
    df = pd.read_csv(path+file)
    df[&amp;#39;catalog&amp;#39;] = file.split(&amp;#39;-&amp;#39;)[2]
    dfs.append(df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This gives me a list of data frames with the course info for each catalog. Before combining them, there’s a problem to deal with. The 2017-2018 catalog was scraped when it was “current”, but now the 2018-2019 is current.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(dfs[2].url[0:4])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0    http://collegeofidaho.smartcatalogiq.com/en/cu...
## 1    http://collegeofidaho.smartcatalogiq.com/en/cu...
## 2    http://collegeofidaho.smartcatalogiq.com/en/cu...
## 3    http://collegeofidaho.smartcatalogiq.com/en/cu...
## Name: url, dtype: object&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(dfs[3].url[0:4])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0    http://collegeofidaho.smartcatalogiq.com/en/cu...
## 1    http://collegeofidaho.smartcatalogiq.com/en/cu...
## 2    http://collegeofidaho.smartcatalogiq.com/en/cu...
## 3    http://collegeofidaho.smartcatalogiq.com/en/cu...
## Name: url, dtype: object&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This means we have to change to the urls in the 2017-2018 catalog or we’ll just find a lot of broken links.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dfs[2][&amp;#39;url&amp;#39;] = dfs[2].url.apply(lambda x: x.replace(&amp;#39;current&amp;#39;, &amp;#39;2017-2018&amp;#39;))

catalogs = pd.concat(dfs, axis=0, ignore_index=True)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’re ready to go get the descriptions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scraping-descriptions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Scraping Descriptions&lt;/h1&gt;
&lt;p&gt;This is a pretty simple scraping problem: each &lt;code&gt;url&lt;/code&gt; in &lt;code&gt;catalogs&lt;/code&gt; leads to a page with 2 &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; tags, the second is the course description. I’ve been learning a bit about &lt;code&gt;scrapy&lt;/code&gt; which is overkill for this job so I’ll stick with &lt;code&gt;beautifulsoup4&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here’s the code to get one description before we try to grab 4 catalog years at once.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import urllib.request, urllib.error
from bs4 import BeautifulSoup

html = urllib.request.urlopen(catalogs.url[1])
soup = BeautifulSoup(html, &amp;#39;html&amp;#39;)
ps = soup.find_all(&amp;#39;p&amp;#39;)
print(ps[0].text)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2015-2016 Undergraduate Catalog &amp;gt; Courses &amp;gt; ACC - Accounting &amp;gt; 200 &amp;gt; ACC-222&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(ps[1].text)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## A study of the role of accounting information in
## decision-making emphasizing the use of accounting
## data for internal management decisions.  The
## course includes an introduction to cash flows,
## cost accounting, cost-volume-profit relationships
## and budgeting in business decisions.
## Prerequisite: ACC-221&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looks like we’re good to go - but looks can be deceiving. In the last two years, there have been a handful of courses removed after I scraped my original lists. For example MAT-150 (Applied Calculus) used to have a lab component. Due to switching to new catalog software this course re-appeared in the 2017-2018 catalog (when it was “current”) but this was identified and removed after I scraped. There were several other courses like this which means I have them in my list with a url, but the link won’t work. Fortunately, python has some pretty good error handling capabilities.&lt;/p&gt;
&lt;div id=&#34;getting-descriptions-and-handling-errors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting Descriptions and Handling Errors&lt;/h2&gt;
&lt;p&gt;Below is the function that I’ll hand each &lt;code&gt;url&lt;/code&gt; to and expect a description returned. If the &lt;code&gt;url&lt;/code&gt; is bad, the function should:
* print the bad url (so we know which course is involved) and
* return ‘NA’&lt;/p&gt;
&lt;p&gt;We’ll accomplish this be “try”ing to open the url and handling “except”ions that are thrown.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def get_desc(url):
    try:
        html = urllib.request.urlopen(url)
    except urllib.error.HTTPError as e:
        print(&amp;#39;HTTP Error: {}&amp;#39;.format(e.code))
        print(url)
        return(&amp;#39;NA&amp;#39;)
    except urllib.error.URLError as e:
        print(&amp;#39;URLError: {}&amp;#39;.format(e.code))
        print(url)
        return(&amp;#39;NA&amp;#39;)
    else:
        desc = BeautifulSoup(html, &amp;#39;html&amp;#39;).find_all(&amp;#39;p&amp;#39;)
        if(len(desc)&amp;gt;1):
            return(desc[1].text)
        else:
            return(desc[0].text)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we simply apply our function over the &lt;code&gt;url&lt;/code&gt; column of &lt;code&gt;catalogs&lt;/code&gt; (this is also when I wish python had easier parallelization). I’m also going to get rid of the courses “without” descriptions, remove hard-coded &lt;code&gt;\n&lt;/code&gt;s, and save the results.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;catalogs[&amp;#39;description&amp;#39;] = catalogs.url.apply(get_desc)

catalogs = catalogs[catalogs[&amp;#39;description&amp;#39;]!=&amp;#39;NA&amp;#39;]

#remove newlines in description
catalogs[&amp;#39;description&amp;#39;] = catalogs.description.astype(str)
catalogs[&amp;#39;description&amp;#39;] = catalogs.description.apply(lambda x: x.replace(&amp;#39;\n&amp;#39;, &amp;#39; &amp;#39;))

catalogs.to_csv(&amp;quot;class-desc-2015-2019.csv&amp;quot;, header=True, index=False)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I didn’t have the above code chunk execute when rendering the page due to the time it takes, but you can download &lt;a href=&#34;/files/class-desc-2015-2019.csv&#34;&gt;class-desc-2015-2019.csv&lt;/a&gt; to verify (or run the code yourself).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Mapping with an 800 Pound Gorilla</title>
      <link>/post/2019-03-18-matplotlib-map/</link>
      <pubDate>Mon, 18 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-03-18-matplotlib-map/</guid>
      <description>


&lt;p&gt;I’ve been focusing on &lt;code&gt;python&lt;/code&gt; recently to become a bi-lingual data scientist. Probably my least favorite thing about &lt;code&gt;python&lt;/code&gt; is its plotting libraries - there are too many options built on top of &lt;code&gt;matplotlib&lt;/code&gt; which pre-dates &lt;code&gt;pandas&lt;/code&gt; dataframes. This makes for some clunky code and blurry boundaries (both “is that a &lt;code&gt;seaborn&lt;/code&gt;, &lt;code&gt;pandas&lt;/code&gt;, or &lt;code&gt;matplotlib&lt;/code&gt; function?” and situations with 3 equally messy solutions but in very different ways). In my opinion, &lt;code&gt;ggplot2&lt;/code&gt;’s deep interplay with dataframes makes a lot more sense and &lt;code&gt;ggplot&lt;/code&gt;’s layers make it easy to change plot type (just switch the &lt;code&gt;geom_&lt;/code&gt;), add facets, and tweak aesthetics. I think this &lt;a href=&#34;https://dsaber.com/2016/10/02/a-dramatic-tour-through-pythons-data-visualization-landscape-including-ggplot-and-altair/&#34;&gt;post&lt;/a&gt; does a great job comparing the main &lt;code&gt;python&lt;/code&gt; plotting libraries and also illustrates why &lt;code&gt;matplotlib&lt;/code&gt; turns me off a bit - too many loops!&lt;/p&gt;
&lt;p&gt;Despite &lt;code&gt;matplotlib&lt;/code&gt;’s issues, it can make some really nice plots! Today I’ll be doing a little mapping so show off it’s prettier side.&lt;/p&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;The data I’ll be using is of the places I’ve lived - GPS coordinates of 9 cities in 7 states (for security reasons I’m omitting where I was born). I’m going to use &lt;code&gt;R&lt;/code&gt;’s &lt;code&gt;maps&lt;/code&gt; package to get most of the data (because it’s soooo easy).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(ggplot2)
library(maps)

cities &amp;lt;- filter(maps::us.cities, name %in% c(&amp;quot;Reno NV&amp;quot;, &amp;quot;Sacramento CA&amp;quot;, &amp;quot;Salt Lake City UT&amp;quot;, &amp;quot;Tacoma WA&amp;quot;, &amp;quot;Boise ID&amp;quot;, &amp;quot;Tulare CA&amp;quot;, &amp;quot;Harrisonburg VA&amp;quot;, &amp;quot;Charlotte NC&amp;quot;, &amp;quot;Seattle WA&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This dataframe was saved to a &lt;code&gt;.csv&lt;/code&gt; for later use. The rest will be in &lt;code&gt;python&lt;/code&gt; to draw a pretty map, overlay some points and connect the points with great circles.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;basemap&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Basemap&lt;/h2&gt;
&lt;p&gt;There are two main ways I know of to draw maps in &lt;code&gt;python&lt;/code&gt;: &lt;code&gt;matplotlib&lt;/code&gt;’s &lt;code&gt;basemap&lt;/code&gt; toolkit and &lt;code&gt;geopandas&lt;/code&gt;. As you might expect, &lt;code&gt;geopandas&lt;/code&gt; is generally a lot nicer to use, but can be limiting. I’m going to use &lt;code&gt;basemap&lt;/code&gt; since most of the data I need to plot is just to get the background map drawn and &lt;code&gt;basemap&lt;/code&gt; has that built in (no shapefiles or extra datasets).&lt;/p&gt;
&lt;p&gt;First, we’ll load our &lt;code&gt;python&lt;/code&gt; libraries and set-up the underlying map.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl
from mpl_toolkits.basemap import Basemap
m = Basemap(llcrnrlon=-130.,llcrnrlat=20.,urcrnrlon=-60. , urcrnrlat=50., rsphere=(6378137.00,6356752.3142),
            resolution=&amp;#39;l&amp;#39;,projection=&amp;#39;merc&amp;#39;,
            lat_0=35.,lon_0=-95.)
m.drawcoastlines()
m.drawstates()
m.drawcountries()
m.fillcontinents()
plt.title(&amp;quot;The US&amp;quot;)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-18-matplotlib-map_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Basemap&lt;/code&gt; command has many options, here I’ve set the lower-left corner and upper-right corner, the Mercator projection, low resolution, and a center of &lt;span class=&#34;math inline&#34;&gt;\(35^{\circ}N\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(95^{\circ}W\)&lt;/span&gt;. This will include the first 48 states of the US roughly centered. Then we add coastlines, states, countries, and fill the land. These last commands are what makes &lt;code&gt;Basemap&lt;/code&gt; nice - I don’t need shapefiles or other data to get all this detail onto the map.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-points&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adding Points&lt;/h2&gt;
&lt;p&gt;Now I can add points for the cities. First, I’ll load the dataframe of the gps data and build lists of the order in which I moved.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;city = pd.read_csv(&amp;#39;../../static/files/city-gps2.csv&amp;#39;)
print(city.head())
#lists of longitude and latitude in order.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               name     pop    lat    long
## 0         Boise ID  193628  43.61 -116.23
## 1     Charlotte NC  607111  35.20  -80.83
## 2  Harrisonburg VA   41992  38.44  -78.87
## 3          Reno NV  206626  39.54 -119.82
## 4    Sacramento CA  480392  38.57 -121.47&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;clon = [city.long[3], city.long[8], city.long[4], 
        city.long[7], city.long[5], city.long[1], 
        city.long[6], city.long[7], city.long[2], 
        city.long[0]]
clat = [city.lat[3], city.lat[8], city.lat[4], 
        city.lat[7], city.lat[5], city.lat[1], 
        city.lat[6], city.lat[7], city.lat[2], 
        city.lat[0]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we add these points (&lt;code&gt;(clon,clat)&lt;/code&gt; pairs) to the earlier basemap.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x,y = m(clon, clat)
m.scatter(x,y,marker=&amp;#39;*&amp;#39;, color=&amp;#39;r&amp;#39;, zorder=5) #the zorder is important
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-18-matplotlib-map_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;connecting-the-points&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Connecting the Points&lt;/h2&gt;
&lt;p&gt;Now we can connect the points. If I were really bored, I could trace out the actual paths driven to get from one place to another. I’m not, so I’m just going to use &lt;code&gt;Basemap&lt;/code&gt;’s &lt;code&gt;drawgreatcircle()&lt;/code&gt; method to connect points with a great circle. This is why having the lists of latitude and longitude was really done.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for i in range(0,len(clon)-1):
    m.drawgreatcircle(clon[i],clat[i],clon[i+1],clat[i+1],linewidth=4,color=&amp;#39;b&amp;#39;)
plt.title(&amp;quot;Places Lived with Great Circle Moves&amp;quot;)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-18-matplotlib-map_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not bad, but adding an &lt;code&gt;alpha&lt;/code&gt; setting to change transparency based on the order of moves helps. The earliest move will be at &lt;span class=&#34;math inline&#34;&gt;\(1/3\)&lt;/span&gt; (light, but visible) while the most recent will be &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, with a linear increment. I’m also going to close the old plot and put all the mapping code together here.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.close() 
m = Basemap(llcrnrlon=-130.,llcrnrlat=20.,urcrnrlon=-60. , urcrnrlat=50., rsphere=(6378137.00,6356752.3142),
            resolution=&amp;#39;l&amp;#39;,projection=&amp;#39;merc&amp;#39;,
            lat_0=35.,lon_0=-95.)
m.drawcoastlines()
m.drawstates()
m.drawcountries()
m.shadedrelief()
x,y = m(clon, clat)
m.scatter(x,y,marker=&amp;#39;*&amp;#39;, color=&amp;#39;r&amp;#39;, zorder=5)
for i in range(0,len(clon)-1):
    m.drawgreatcircle(clon[i],clat[i],clon[i+1],clat[i+1],linewidth=4,color=&amp;#39;b&amp;#39;, alpha=((1/3)+i*(2/27)))
plt.title(&amp;quot;Places Lived with Great Circle Moves&amp;quot;)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-18-matplotlib-map_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You may have noticed that I used &lt;code&gt;.shadedrelief()&lt;/code&gt; instead of &lt;code&gt;.fillcontinents()&lt;/code&gt;. &lt;code&gt;Basemap&lt;/code&gt; has several similar options to provide some real quality to maps. Here are the &lt;code&gt;.etopo()&lt;/code&gt; and &lt;code&gt;.bluemarble()&lt;/code&gt; options as well:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.close()
plt.subplot(1,2,1)
m2 = Basemap(llcrnrlon=-130.,llcrnrlat=20.,urcrnrlon=-60. , urcrnrlat=50., rsphere=(6378137.00,6356752.3142),
            resolution=&amp;#39;l&amp;#39;,projection=&amp;#39;merc&amp;#39;,
            lat_0=35.,lon_0=-95.)
m2.drawcoastlines()
m2.drawstates()
m2.drawcountries()
m2.bluemarble()
x,y = m2(clon, clat)
m2.scatter(x,y,marker=&amp;#39;*&amp;#39;, color=&amp;#39;r&amp;#39;, zorder=5)
for i in range(0,len(clon)-1):
    m2.drawgreatcircle(clon[i],clat[i],clon[i+1],clat[i+1],linewidth=4,color=&amp;#39;b&amp;#39;, alpha=(.3+i*(2/27)))
plt.subplot(1,2,2)
m3 = Basemap(llcrnrlon=-130.,llcrnrlat=20.,urcrnrlon=-60. , urcrnrlat=50., rsphere=(6378137.00,6356752.3142),
            resolution=&amp;#39;l&amp;#39;,projection=&amp;#39;merc&amp;#39;,
            lat_0=35.,lon_0=-95.)
m3.drawcoastlines()
m3.drawstates()
m3.drawcountries()
m3.etopo()
x,y = m3(clon, clat)
m3.scatter(x,y,marker=&amp;#39;*&amp;#39;, color=&amp;#39;r&amp;#39;, zorder=5)
for i in range(0,len(clon)-1):
    m3.drawgreatcircle(clon[i],clat[i],clon[i+1],clat[i+1],linewidth=4,color=&amp;#39;b&amp;#39;, alpha=(.3+i*(2/27)))
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-18-matplotlib-map_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s no &lt;code&gt;ggplot2&lt;/code&gt;, but very nice for an 800 pound gorilla&lt;a href=&#34;https://dsaber.com/2016/10/02/a-dramatic-tour-through-pythons-data-visualization-landscape-including-ggplot-and-altair/&#34;&gt;*&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Expectation-Maximization</title>
      <link>/post/2019-02-23-em-1/</link>
      <pubDate>Sat, 23 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-02-23-em-1/</guid>
      <description>


&lt;p&gt;As part of some clustering work and learning about hidden Markov models, I’ve been doing some reading about the EM algorithm and it’s applications. It’s a pretty neat algorithm (I love iterative algorithms like Newton’s method and the Euclidean algorithm) so I thought I’d illustrate how it works.&lt;/p&gt;
&lt;p&gt;I’ve also been doing a bit more python recently, so I thought I would do all this in python rather than R. However, this post is still done in RMarkdown using python code chunks! I know Jupyter notebooks have their fans, but as an authoring tool RMarkdown is plain text which makes it easier to create/edit documents and maintain them via tools like Git (unlike Jupyter notebooks).&lt;/p&gt;
&lt;div id=&#34;the-basic-idea&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Basic Idea&lt;/h1&gt;
&lt;p&gt;Like any iterative algorithm, the big picture behind the EM algorithm is to converge to values given some initial guess. For our purposes, we want to converge to the parameters of a Gaussian mixture model given observed data, a number of Gaussians being mixed, and guesses for the initial means and standard deviations of the mixture components. This is one of the major practical uses of the EM algorithm: determining unknown parameters of mixture model components.&lt;/p&gt;
&lt;p&gt;The algorithm repeats two key steps: &lt;em&gt;E&lt;/em&gt;xpectation and &lt;em&gt;M&lt;/em&gt;aximization. The expectation step calculates probabilities of each data-point being in a mixture component. The maximization step uses these probabilities to update (a) means and standard deviations of mixture components and (b) the proportion of data-points in each component. There’s not much here that’s special to the case of Gaussian mixture models (GMM’s), except where we use normal distributions and the parameters we compute in the maximization step.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The following functions are intended to be easy to understand as opposed to highly optimized with great error handling. If you’re estimating components of a Gaussian Mixture model in a production environment, Scikit-Learn has excellent clustering routines that will be far more robust and efficient than anything here.&lt;/p&gt;
&lt;div id=&#34;expectation-details&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Expectation Details&lt;/h2&gt;
&lt;p&gt;The expectation step computes the probability of a data point being in any one of the mixture components. If &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; is the data-point, &lt;span class=&#34;math inline&#34;&gt;\(\mu_c,~\sigma_c\)&lt;/span&gt; the mean and standard deviation for component &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(f_c\)&lt;/span&gt; the fraction of data-points in component &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;, then these probabilities are: &lt;span class=&#34;math display&#34;&gt;\[
p_{ic} = \frac{f_c*N(x_i | \mu_c, \sigma_c)}{\sum_k f_k*N(x_i | \mu_k, \sigma_k)}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This can be turned in the following python code (as well as some libraries we’ll need throughout the post):&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy
from scipy.stats import norm
def expectation_probabilities_gmm(data, mean_vect, sd_vect, fracPerComp):
    ret_probs = np.zeros([len(data),len(mean_vect)])
    for i in range(len(data)):
        for j in range(len(mean_vect)):
            ret_probs[i,j] = (fracPerComp[j]*norm.pdf(data[i],mean_vect[j],sd_vect[j]))
        ret_probs[i,:] = ret_probs[i,:]/np.sum(ret_probs[i,:])
    return(ret_probs)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will return a &lt;code&gt;numpy&lt;/code&gt; array with one row per data-point and one column per mixture component, where values are the probabilities. Clearly, we have to supply the data, and previous values for the means, standard deviations, and fraction of points per component. Initially, these will be our guesses, but subsequent iterations will get them from the maximization step.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;maximization-details&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Maximization Details&lt;/h2&gt;
&lt;p&gt;Once we have a new batch of probabilities, we need to update values for the mixture model parameters. First, we calculate a &lt;em&gt;component weight&lt;/em&gt; for each component which is the column sum of our probability array. We then use the component weight to compute weighted means, standard deviations, and fractions of points per component. &lt;span class=&#34;math display&#34;&gt;\[
w_c = \sum_i p_{ic},\\
\mu_c = \frac{\sum_i p_{ic}x_i}{w_c},\\
\sigma_c^2 = \frac{\sum_i (p_{ic}(x_i-\mu_c)^2)}{w_c}\\
f_c = \frac{w_c}{n}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here’s a python function that computes these and returns the &lt;span class=&#34;math inline&#34;&gt;\(\mu_c, \sigma_c, f_c\)&lt;/span&gt; tuple given the data and expectation probabilities:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def maximization_params_gmm(expect_probs, data):
    clust_weights = [np.sum(expect_probs[:,j]) for j in
        range(np.shape(expect_probs)[1])]
    new_means = np.divide([np.sum(expect_probs[:,j]*data[:]) for
        j in range(np.shape(expect_probs)[1])],clust_weights)
    new_sds = np.divide([np.sum(expect_probs[:,j]*(data[:]-new_means[j])*(data[:]-new_means[j])) for j in range(np.shape(expect_probs)[1]) ], clust_weights)
    new_sds = np.sqrt(new_sds)
    new_frac = np.divide(clust_weights,np.shape(expect_probs)[0])
    return new_means, new_sds, new_frac;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that this is for a mixture of univariate Gaussian distributions, in higher dimensions you would need covariance matrices to be computed rather than just standard deviations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stopping-criteria&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stopping Criteria&lt;/h2&gt;
&lt;p&gt;A key part of an iterative algorithm is when to stop, which is achieved in the EM algorithm by looking at the convergence of the log-likelihood. Although in this post we’ll just run a fixed number of iterations.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def log_likelihood(data, mean_vect, sd_vect, fracPerComp):
  ret_probs = np.zeros([len(data),len(mean_vect)])
  for i in range(len(data)):
      for j in range(len(mean_vect)):
          ret_probs[i,j] = (fracPerComp[j]*norm.pdf(data[i],mean_vect[j],sd_vect[j]))
      row_sums = np.sum(ret_probs[i,:])
  loglike = np.sum(np.log(row_sums))
  return(loglike)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;observed-mixture-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Observed Mixture Data&lt;/h1&gt;
&lt;p&gt;I don’t have a nice real-world GMM dataset, but we can easily manufacture some:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.random.seed(seed=123987) #reproducibility
vals = [np.random.normal(5, .5, 100), np.random.normal(3, 1, 100), np.random.normal(6.5,1.25,100)]
grps = [&amp;#39;A&amp;#39;,&amp;#39;B&amp;#39;,&amp;#39;C&amp;#39;]
#build dataframe
df = pd.concat([pd.DataFrame({&amp;quot;group&amp;quot;:grps[i], &amp;quot;value&amp;quot;:vals[i]}) for i in range(0,3)], axis=0, ignore_index=True)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Initially I had written this with 3 separate vectors of values and 3 data-frames that I then glued together. Then I remembered one of my favorite things about python: list comprehensions! Clearly I’ve made heavy use of them in the above functions as well.&lt;/p&gt;
&lt;p&gt;With the &lt;code&gt;group&lt;/code&gt; column in our dataframe, we can easily see the three components of the mixture:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;#grouped
plt.figure(figsize=(3,3))
plots = sns.FacetGrid(data=df, hue=&amp;quot;group&amp;quot;, legend_out=True)
plots.map(sns.distplot,&amp;#39;value&amp;#39;, kde_kws={&amp;quot;shade&amp;quot;:False},rug_kws={&amp;quot;alpha&amp;quot;:.3}, rug=True, kde=True, hist=False)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-23-EM-1_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;288&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In practice though, we would only be able to see the full distribution of our data:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.close() #get a blank plot rather than overlay on our last distplots
plt.figure(figsize=(3,3))
sns.distplot(df[&amp;#39;value&amp;#39;], rug=True, kde=True, hist=False)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-23-EM-1_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;288&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The “shoulders” on the curve given a hint that we may be dealing with an underlying mixture model of 3 components. But how “non-normal” is our data? We can run a Shapiro-Wilkes test and compare with the same number of points sampled from a single normal distribution to get an idea.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;##Shapiro-Wilkes test
print(scipy.stats.shapiro(df[&amp;#39;value&amp;#39;]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (0.9909396767616272, 0.0613214448094368)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second value in this tuple is the p-value associated to a null hypothesis that our data is from a single normal distribution. In this case we would fail to reject that our observed data is different from a sample from a single normal distribution at the customary &lt;span class=&#34;math inline&#34;&gt;\(0.05\)&lt;/span&gt; significance level. More evidence that p-values and significance tests can miss important details.&lt;/p&gt;
&lt;p&gt;We can also compare to simulated data sampled from a true normal distribution with the same mean and standard deviation as our observed data to see how close our data is to being normal.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;##compare with true normal sample
simDF = pd.DataFrame({&amp;quot;type&amp;quot;:&amp;quot;simulated&amp;quot;, &amp;quot;value&amp;quot;:np.random.normal(df[&amp;#39;value&amp;#39;].mean(), df[&amp;#39;value&amp;#39;].std(), 300)})
simDF = pd.concat([pd.DataFrame({&amp;quot;type&amp;quot;:&amp;quot;sample&amp;quot;, &amp;quot;value&amp;quot;:df[&amp;#39;value&amp;#39;]}), simDF], axis=0, ignore_index=True)
plt.figure(figsize=(3,3))
plotsim = sns.FacetGrid(data=simDF, hue=&amp;quot;type&amp;quot;)
plotsim.map(sns.distplot,&amp;#39;value&amp;#39;, kde_kws={&amp;quot;shade&amp;quot;:False},rug_kws={&amp;quot;alpha&amp;quot;:.3}, rug=True, kde=True, hist=False)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-23-EM-1_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;288&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;running-the-em&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Running The EM&lt;/h1&gt;
&lt;p&gt;Here’s a function to handle calling the E and M steps and holding the results of each. This will be handle to look at how the process converges.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def run_em(data, init_means, init_sds, init_frac, num_iterations):
    ret_means = np.zeros((num_iterations,len(init_means)))
    ret_sds = np.zeros((num_iterations,len(init_means)))
    ret_fracs = np.zeros((num_iterations,len(init_means)))
    ret_ll = np.zeros(num_iterations)
    ret_means[0,:] = init_means
    ret_sds[0,:] = init_sds
    ret_fracs[0,:] = init_frac
    for i in range(num_iterations-1):
        ret_ll[i]=log_likelihood(data, ret_means[i,:],ret_sds[i,:], ret_fracs[i,:])
        new_probs = expectation_probabilities_gmm(data, ret_means[i,:],ret_sds[i,:], ret_fracs[i,:])
        ret_means[i+1,:], ret_sds[i+1,:],ret_fracs[i+1,:] = maximization_params_gmm(new_probs, data)
    return ret_means, ret_sds, ret_fracs, ret_ll;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can make some guesses and run the EM algorithm. Based on the distribution of all the data, it looks like there are irregularities around 3, 5.5, and 7 - so those will be our means. We’ll assume variance of 1 and equal size for all groups.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;means = [3,5.5, 7]
sds = [1,1,1]
fracs = [1/3,1/3,1/3]
mu,sigma,frac,ll = run_em(df[&amp;#39;value&amp;#39;], means, sds, fracs, 50)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then easily use &lt;code&gt;matplotlib&lt;/code&gt; to see how these change over the iterations:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.figure(figsize=(5,5))
plt.subplot(2,2,1)
plt.plot(mu)
plt.title(&amp;quot;Means&amp;quot;)
plt.xlabel(&amp;quot;Iteration&amp;quot;)
plt.xticks([10*(i+1) for i in range(5)])
plt.subplot(2,2,2)
plt.plot(sigma)
plt.title(&amp;quot;Std. Deviations&amp;quot;)
plt.xlabel(&amp;quot;Iteration&amp;quot;)
plt.xticks([10*(i+1) for i in range(5)])
plt.subplot(2,2,3)
plt.plot(frac)
plt.title(&amp;quot;Fraction Per Cluster&amp;quot;)
plt.xlabel(&amp;quot;Iteration&amp;quot;)
plt.xticks([10*(i+1) for i in range(5)])
plt.subplot(2,2,4)
plt.plot(ll[0:48]) #last ll value hasn&amp;#39;t been calculated
plt.title(&amp;quot;Log-Likelihood&amp;quot;)
plt.xlabel(&amp;quot;Iteration&amp;quot;)
plt.xticks([10*(i+1) for i in range(5)])
plt.tight_layout()
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-23-EM-1_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This looks like we had convergence fairly quickly, except the fraction of points per cluster - but our y-scale is very narrow. In the end our estimates are:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Means:&amp;quot;, mu[49,:])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Means: [2.9767655  4.91279169 6.30925586]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Std. Dev.:&amp;quot;, sigma[49,:])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Std. Dev.: [0.83852695 0.45363153 1.15733853]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Fraction per Cluster:&amp;quot;, frac[49,:])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Fraction per Cluster: [0.28652637 0.32794345 0.38553017]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since our mixture components had means &lt;span class=&#34;math inline&#34;&gt;\(3, 5, 6.5\)&lt;/span&gt; and standard deviations &lt;span class=&#34;math inline&#34;&gt;\(1, 0.5, 1.25\)&lt;/span&gt; we did pretty good. The main difference is the points where the normal distributions overlap with “high” probability. This is where any clustering algorithm will break down. Despite how close our data was to a single normal distribution, the EM algorithm was able to divide the observations into 3 groups with means and standard deviations almost equal to the actual cluster parameters.&lt;/p&gt;
&lt;p&gt;I want to show some visuals of our mean and std. dev. estimates with the data and how the clusters evolve, but this post has gotten a little long. Look forward to part 2 soon…&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tidy Clouds</title>
      <link>/post/2019-01-11-wc/</link>
      <pubDate>Fri, 11 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-01-11-wc/</guid>
      <description>


&lt;p&gt;In my data visualization class I had the students get a book from &lt;a href=&#34;https://www.gutenberg.org&#34;&gt;Project Gutenberg&lt;/a&gt; using the &lt;code&gt;gutenbergr&lt;/code&gt; package and build a word cloud using &lt;code&gt;tidytext&lt;/code&gt; and &lt;code&gt;wordcloud&lt;/code&gt;. It’s much easier that the “old” corpus/text mapping approach, and when the students were sharing their clouds they started showing the cloud and having students try to guess the book. This made me think of using a Shiny runtime to make a little word cloud guessing game.&lt;/p&gt;
&lt;div id=&#34;building-clouds&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Building Clouds&lt;/h1&gt;
&lt;p&gt;First, here’s the code to grab a book and build a word cloud with line-by-line comments but the code is pretty straight forward (especially if you’ve build a word cloud before).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(tidyr)
library(ggplot2)

library(gutenbergr)
library(tidytext)
library(wordcloud)
library(RColorBrewer)

cloud_from_book &amp;lt;- function(gutenberg_id){
  #download book using gutenbergr
  book &amp;lt;- gutenberg_download(gutenberg_id)
  #turn line per row into word per row
  Words &amp;lt;- unnest_tokens(book, word, text)
  #remove stop_words using the standard english stop_words
  WordsReduced &amp;lt;- anti_join(Words, stop_words)
  #turn word list into frequency table
  WRCount &amp;lt;- WordsReduced %&amp;gt;% count(word) %&amp;gt;% ungroup()
  #build wordcloud
  wordcloud(WRCount$word, WRCount$n, random.order = FALSE, max.words = 75, colors = brewer.pal(8, &amp;quot;Dark2&amp;quot;))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s test it out on a whale of a tale:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cloud_from_book(2489)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-11-wc_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;shiny-clouds&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Shiny Clouds&lt;/h1&gt;
&lt;p&gt;To make this more interesting, we’ll turn this into an embedded shiny app. It will have a pretty simple design, two buttons: “New Book” and “Show Book Info”. Selecting “New Book” will update the wordcloud from a random gutenberg_id, while “Show Book Info” will reveal title and other book information.&lt;/p&gt;
&lt;p&gt;Here’s the &lt;code&gt;ui&lt;/code&gt; function which is pretty basic:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ui &amp;lt;- fluidPage(
   titlePanel(&amp;quot;Random Book Word Cloud&amp;quot;),
   sidebarLayout(
      sidebarPanel(
        actionButton(&amp;quot;new&amp;quot;, &amp;quot;New Book&amp;quot;),
        br(),
        actionButton(&amp;quot;reveal&amp;quot;, &amp;quot;Show Book Info&amp;quot;)
      ),
      
      mainPanel(
         plotOutput(&amp;quot;distPlot&amp;quot;),
         tableOutput(&amp;quot;BookInfo&amp;quot;))
   )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now the server function (assuming our &lt;code&gt;cloud_from_book&lt;/code&gt; function is already defined). Tis is a little more complicated because of the reactive values to control the table of book info:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;server &amp;lt;- function(input, output) {
  
   v&amp;lt;- reactiveValues(random_id=NULL, metaTab=NULL)
   
   observeEvent(input$new,{
     v$random_id&amp;lt;-sample(gutenberg_metadata$gutenberg_id, 1)
     v$metaTab &amp;lt;- NULL
   })
   observeEvent(input$reveal, {
     v$metaTab&amp;lt;-filter(gutenberg_metadata, gutenberg_id==v$random_id)[,2:3]
   })
   
   output$distPlot &amp;lt;- renderPlot({
     if(!is.null(v$random_id)){
       cloud_from_book(v$random_id)
     }
   })

   output$BookInfo &amp;lt;- renderTable({
     if(!is.null(v$metaTab)){v$metaTab}
   })
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;reactiveValues&lt;/code&gt; function produces of list-like structure (the documentation words, not mine…) for reactive variables. Both the &lt;code&gt;random_id&lt;/code&gt; and &lt;code&gt;metaTab&lt;/code&gt; are &lt;code&gt;NULL&lt;/code&gt;. When the &lt;code&gt;New Book&lt;/code&gt; button is pressed, &lt;code&gt;random_id&lt;/code&gt; is filled and &lt;code&gt;metaTab&lt;/code&gt; is set to &lt;code&gt;NULL&lt;/code&gt; (this resets the table of book info if it’s been shown). The &lt;code&gt;Show Book Info&lt;/code&gt; button (aka &lt;code&gt;input$reveal&lt;/code&gt;) doesn’t do anything to the &lt;code&gt;random_id&lt;/code&gt; but fills the &lt;code&gt;metaTab&lt;/code&gt; table. The &lt;code&gt;if&lt;/code&gt; statements in the &lt;code&gt;render*&lt;/code&gt; functions then control what is rendered and avoids error messages being printed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;embedded-app&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Embedded &lt;a href=&#34;https://jpreszler.shinyapps.io/Gutenberg-Clouds/&#34;&gt;App&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;If greyed out, scroll down and reconnect to server&lt;/em&gt; or the app is off because of my shinyapps.io limit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::include_app(&amp;quot;https://jpreszler.shinyapps.io/Gutenberg-Clouds/&amp;quot;, height = &amp;quot;600px&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;iframe src=&#34;https://jpreszler.shinyapps.io/Gutenberg-Clouds/?showcase=0&#34; width=&#34;672&#34; height=&#34;600px&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Sankey Diagram</title>
      <link>/post/2018-12-27-sankey/</link>
      <pubDate>Thu, 27 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-12-27-sankey/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/d3/d3.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/sankey/sankey.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/sankeyNetwork-binding/sankeyNetwork.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/pymjs/pym.v1.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/widgetframe-binding/widgetframe.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;Update 7/23/2019&lt;/strong&gt; Various package updates have created problems with showing more than one javascript plot on a post. I’ve added calls to &lt;code&gt;htlwidgets::onRender&lt;/code&gt; to get at least one plot displayed. I may revisit this, but the interaction between hugo, blogdown, and various javascript libraries (chorddiag, networkD3, D3, data tables, etc) is more than I’m able to dive into at the moment.&lt;/p&gt;
&lt;p&gt;This post is about a type of visualization the will hopefully help see how students “flow” through college. The data is an anonymized selection of Math and Math-Computer Science majors at the College of Idaho, and for simplicity we’ll only be using the math and computer science courses. Out goal is to produce the following &lt;code&gt;Sankey Diagram&lt;/code&gt;, which is really just a graph (in the discrete math sense - nodes connected by edges) where the edges are scaled by weight, in this case the number of students taking course A then B will be reflected in the width of the link between A and B. The &lt;code&gt;sankeyNetwork&lt;/code&gt; command also adjust the node layout to minimize edge crossing and have a general “left to right” aspect.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sn &amp;lt;- sankeyNetwork(Links = edgeDFTemp, Nodes = nodeDFTemp, Source = &amp;quot;source&amp;quot;, 
              Target = &amp;quot;target&amp;quot;, Value = &amp;quot;value&amp;quot;, NodeID = &amp;quot;id&amp;quot;, 
              nodeWidth = 20, fontSize = 8, units = &amp;quot;students&amp;quot;)

htmlwidgets::onRender(sn, &amp;#39;document.getElementsByTagName(&amp;quot;svg&amp;quot;)[0].setAttribute(&amp;quot;viewBox&amp;quot;, &amp;quot;&amp;quot;)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;sankeyNetwork html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;links&#34;:{&#34;source&#34;:[0,0,0,0,1,1,1,1,2,2,2,2,2,3,3,3,4,4,5,5,6,6,6,6,7,7,7,7,7,8,9,9,9,10,10,10,11,12,12,13,15,15,18,18,18,19,19,19,20,21,21,21,23,24,26,27,27,28,29,30,32,32,33,34,34,35,35,35,36,37,38,38,38,38,38,38,38,39,39,39,40,40,41,41,41,41,41,41,41,41,41,42,42,42,42,43,43,43,44,45,45,45,47,47,48,50,51,53,53,53,53,53,54,54,55,56,57,59,59,59,60,62,62,62,62,62,62,62,62,62,63,63,63,64,64,64,64,64,64,64,64,65,65,65,65,65,65,65,65,65,65,66,67,67,68,69,69,69,69,70,70,70,71,71,71,71,71,72,73,73,74,74,74,74,74,74,75,75,75,75,77,77,77,77,78,79,80,81,82,82,86,86,86,86,87,88,88,88],&#34;target&#34;:[9,42,53,64,7,43,47,62,15,44,60,65,77,63,72,75,50,70,85,88,14,39,54,59,18,65,71,74,77,19,7,62,68,8,24,69,23,7,68,23,21,24,15,20,60,16,61,84,21,11,13,75,81,22,28,1,39,39,40,44,31,39,39,1,42,2,7,43,44,74,1,34,35,42,53,64,78,2,30,36,3,44,1,9,14,46,51,53,54,59,64,7,47,62,65,15,60,77,63,1,39,42,37,65,72,88,62,1,9,42,59,73,2,62,74,75,19,2,7,62,21,3,18,37,55,65,71,74,77,79,49,66,72,1,9,12,14,46,59,73,83,3,10,15,20,49,56,60,71,72,74,72,8,69,18,19,72,75,80,58,85,88,8,57,69,75,82,70,2,62,8,48,57,63,69,86,5,23,70,87,10,67,71,74,1,3,84,85,4,86,5,50,70,87,81,17,76,85],&#34;value&#34;:[1,1,1,3,8,3,1,10,1,1,1,1,1,1,1,2,1,1,1,1,1,1,1,1,2,6,1,1,1,1,2,2,1,1,1,1,1,1,1,1,1,1,1,2,2,1,1,1,1,1,1,1,1,1,2,1,1,2,1,1,1,1,1,1,1,1,1,2,1,1,9,1,2,13,2,1,1,1,1,1,1,1,8,1,1,2,1,5,2,2,5,4,1,6,1,1,1,1,1,1,1,1,1,1,1,1,1,3,1,2,1,1,1,1,1,1,1,1,1,3,1,1,1,1,1,14,2,1,1,1,1,1,2,3,1,1,1,1,4,1,1,3,1,5,4,1,1,4,8,1,3,1,1,1,1,1,1,1,1,1,1,1,1,1,4,1,1,2,1,1,1,1,1,2,2,1,1,1,2,1,1,1,1,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1]},&#34;nodes&#34;:{&#34;name&#34;:[&#34;CSC-150_0FA&#34;,&#34;CSC-150_0SP&#34;,&#34;CSC-150_1FA&#34;,&#34;CSC-150_1SP&#34;,&#34;CSC-150_2SP&#34;,&#34;CSC-150_3FA&#34;,&#34;CSC-152_0FA&#34;,&#34;CSC-152_1FA&#34;,&#34;CSC-152_2FA&#34;,&#34;CSC-160_0SP&#34;,&#34;CSC-160_1SP&#34;,&#34;CSC-160_2SP&#34;,&#34;CSC-235_0SP&#34;,&#34;CSC-235_2SP&#34;,&#34;CSC-270_0SP&#34;,&#34;CSC-270_1SP&#34;,&#34;CSC-270_2SP&#34;,&#34;CSC-270_3SP&#34;,&#34;CSC-285_1W&#34;,&#34;CSC-285_2W&#34;,&#34;CSC-340_1SP&#34;,&#34;CSC-350_2FA&#34;,&#34;CSC-494_2FA&#34;,&#34;CSC-494_3FA&#34;,&#34;CSC-497_1SU&#34;,&#34;CSC-497_2FA&#34;,&#34;MAT-101_0FA&#34;,&#34;MAT-102_0FA&#34;,&#34;MAT-102_0W&#34;,&#34;MAT-102_0SP&#34;,&#34;MAT-102_1FA&#34;,&#34;MAT-111_0W&#34;,&#34;MAT-125_0FA&#34;,&#34;MAT-130_0FA&#34;,&#34;MAT-130_0W&#34;,&#34;MAT-130_0SP&#34;,&#34;MAT-130_1FA&#34;,&#34;MAT-130_1W&#34;,&#34;MAT-150_0FA&#34;,&#34;MAT-150_0SP&#34;,&#34;MAT-150_1FA&#34;,&#34;MAT-175_0FA&#34;,&#34;MAT-175_0SP&#34;,&#34;MAT-175_1FA&#34;,&#34;MAT-175_1SP&#34;,&#34;MAT-199_0FA&#34;,&#34;MAT-199_0SP&#34;,&#34;MAT-199_1FA&#34;,&#34;MAT-199_2FA&#34;,&#34;MAT-199_2SP&#34;,&#34;MAT-199_3FA&#34;,&#34;MAT-199T_0SP&#34;,&#34;MAT-212_0FA&#34;,&#34;MAT-212_0W&#34;,&#34;MAT-212_0SP&#34;,&#34;MAT-212_1W&#34;,&#34;MAT-212_1SP&#34;,&#34;MAT-212_2FA&#34;,&#34;MAT-212_3SP&#34;,&#34;MAT-252_0SP&#34;,&#34;MAT-252_1SP&#34;,&#34;MAT-252_2SP&#34;,&#34;MAT-275_1FA&#34;,&#34;MAT-275_2FA&#34;,&#34;MAT-28X_0W&#34;,&#34;MAT-28X_1W&#34;,&#34;MAT-28X_2W&#34;,&#34;MAT-294_1SP&#34;,&#34;MAT-311_1FA&#34;,&#34;MAT-311_2FA&#34;,&#34;MAT-311_3FA&#34;,&#34;MAT-352_1SP&#34;,&#34;MAT-352_2SP&#34;,&#34;MAT-361_0SP&#34;,&#34;MAT-361_1SP&#34;,&#34;MAT-361_2SP&#34;,&#34;MAT-361_3SP&#34;,&#34;MAT-370_1W&#34;,&#34;MAT-372_0W&#34;,&#34;MAT-372_1W&#34;,&#34;MAT-372_2W&#34;,&#34;MAT-372_3W&#34;,&#34;MAT-399T_2FA&#34;,&#34;MAT-431_0SP&#34;,&#34;MAT-431_2SP&#34;,&#34;MAT-431_3SP&#34;,&#34;MAT-451_2SP&#34;,&#34;MAT-494_3FA&#34;,&#34;MAT-494_3W&#34;],&#34;group&#34;:[&#34;CSC-150_0FA&#34;,&#34;CSC-150_0SP&#34;,&#34;CSC-150_1FA&#34;,&#34;CSC-150_1SP&#34;,&#34;CSC-150_2SP&#34;,&#34;CSC-150_3FA&#34;,&#34;CSC-152_0FA&#34;,&#34;CSC-152_1FA&#34;,&#34;CSC-152_2FA&#34;,&#34;CSC-160_0SP&#34;,&#34;CSC-160_1SP&#34;,&#34;CSC-160_2SP&#34;,&#34;CSC-235_0SP&#34;,&#34;CSC-235_2SP&#34;,&#34;CSC-270_0SP&#34;,&#34;CSC-270_1SP&#34;,&#34;CSC-270_2SP&#34;,&#34;CSC-270_3SP&#34;,&#34;CSC-285_1W&#34;,&#34;CSC-285_2W&#34;,&#34;CSC-340_1SP&#34;,&#34;CSC-350_2FA&#34;,&#34;CSC-494_2FA&#34;,&#34;CSC-494_3FA&#34;,&#34;CSC-497_1SU&#34;,&#34;CSC-497_2FA&#34;,&#34;MAT-101_0FA&#34;,&#34;MAT-102_0FA&#34;,&#34;MAT-102_0W&#34;,&#34;MAT-102_0SP&#34;,&#34;MAT-102_1FA&#34;,&#34;MAT-111_0W&#34;,&#34;MAT-125_0FA&#34;,&#34;MAT-130_0FA&#34;,&#34;MAT-130_0W&#34;,&#34;MAT-130_0SP&#34;,&#34;MAT-130_1FA&#34;,&#34;MAT-130_1W&#34;,&#34;MAT-150_0FA&#34;,&#34;MAT-150_0SP&#34;,&#34;MAT-150_1FA&#34;,&#34;MAT-175_0FA&#34;,&#34;MAT-175_0SP&#34;,&#34;MAT-175_1FA&#34;,&#34;MAT-175_1SP&#34;,&#34;MAT-199_0FA&#34;,&#34;MAT-199_0SP&#34;,&#34;MAT-199_1FA&#34;,&#34;MAT-199_2FA&#34;,&#34;MAT-199_2SP&#34;,&#34;MAT-199_3FA&#34;,&#34;MAT-199T_0SP&#34;,&#34;MAT-212_0FA&#34;,&#34;MAT-212_0W&#34;,&#34;MAT-212_0SP&#34;,&#34;MAT-212_1W&#34;,&#34;MAT-212_1SP&#34;,&#34;MAT-212_2FA&#34;,&#34;MAT-212_3SP&#34;,&#34;MAT-252_0SP&#34;,&#34;MAT-252_1SP&#34;,&#34;MAT-252_2SP&#34;,&#34;MAT-275_1FA&#34;,&#34;MAT-275_2FA&#34;,&#34;MAT-28X_0W&#34;,&#34;MAT-28X_1W&#34;,&#34;MAT-28X_2W&#34;,&#34;MAT-294_1SP&#34;,&#34;MAT-311_1FA&#34;,&#34;MAT-311_2FA&#34;,&#34;MAT-311_3FA&#34;,&#34;MAT-352_1SP&#34;,&#34;MAT-352_2SP&#34;,&#34;MAT-361_0SP&#34;,&#34;MAT-361_1SP&#34;,&#34;MAT-361_2SP&#34;,&#34;MAT-361_3SP&#34;,&#34;MAT-370_1W&#34;,&#34;MAT-372_0W&#34;,&#34;MAT-372_1W&#34;,&#34;MAT-372_2W&#34;,&#34;MAT-372_3W&#34;,&#34;MAT-399T_2FA&#34;,&#34;MAT-431_0SP&#34;,&#34;MAT-431_2SP&#34;,&#34;MAT-431_3SP&#34;,&#34;MAT-451_2SP&#34;,&#34;MAT-494_3FA&#34;,&#34;MAT-494_3W&#34;]},&#34;options&#34;:{&#34;NodeID&#34;:&#34;id&#34;,&#34;NodeGroup&#34;:&#34;id&#34;,&#34;LinkGroup&#34;:null,&#34;colourScale&#34;:&#34;d3.scaleOrdinal(d3.schemeCategory20);&#34;,&#34;fontSize&#34;:8,&#34;fontFamily&#34;:null,&#34;nodeWidth&#34;:20,&#34;nodePadding&#34;:10,&#34;units&#34;:&#34;students&#34;,&#34;margin&#34;:{&#34;top&#34;:null,&#34;right&#34;:null,&#34;bottom&#34;:null,&#34;left&#34;:null},&#34;iterations&#34;:32,&#34;sinksRight&#34;:true}},&#34;evals&#34;:[],&#34;jsHooks&#34;:{&#34;render&#34;:[{&#34;code&#34;:&#34;document.getElementsByTagName(\&#34;svg\&#34;)[0].setAttribute(\&#34;viewBox\&#34;, \&#34;\&#34;)&#34;,&#34;data&#34;:null}]}}&lt;/script&gt;
&lt;div id=&#34;the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Data&lt;/h2&gt;
&lt;p&gt;Following general practice, let’s first look at the data that we’ll be using. I’ve said this is some anonymized student data consisting of an &lt;code&gt;id&lt;/code&gt; (hashed, not actual student id’s), a course prefix and number &lt;code&gt;crs&lt;/code&gt;, and a standardized year value &lt;code&gt;std.year&lt;/code&gt;. This &lt;code&gt;std.year&lt;/code&gt; variable indicates when a student is taking the course during their “college career” with &lt;code&gt;0.0&lt;/code&gt; meaning first fall semester, &lt;code&gt;1.4&lt;/code&gt; meaning spring of their second year. The first digit is basically the number of years of college completed (&lt;span class=&#34;math inline&#34;&gt;\(0,1,2,\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(3\)&lt;/span&gt;) and the second codes the semester type (&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; is fall, &lt;span class=&#34;math inline&#34;&gt;\(2\)&lt;/span&gt; is winter, &lt;span class=&#34;math inline&#34;&gt;\(4\)&lt;/span&gt; is spring and &lt;span class=&#34;math inline&#34;&gt;\(6\)&lt;/span&gt; is summer). These are numeric so I could do arithmetic and standardize things. The point is to help us see students taking a course like &lt;code&gt;CSC-150&lt;/code&gt; (Intro to CS) in their freshman versus junior years, how they got there, and what they do next.&lt;/p&gt;
&lt;p&gt;Now that you understand the variables, here’s a glimpse of the data (and the packages I need for the post):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(ggplot2)
library(stringr)
library(tidyr)
library(DT)
library(networkD3)

students &amp;lt;- read.csv(&amp;quot;../../static/files/math-major-anon.csv&amp;quot;, header=TRUE)
students %&amp;gt;% head() %&amp;gt;% knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;crs&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.year&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;8b551f6ba68aef2c9da1bf682e44716c&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;8b551f6ba68aef2c9da1bf682e44716c&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MAT-252&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;8b551f6ba68aef2c9da1bf682e44716c&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MAT-175&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;5069338819f368fb8960772475069d02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;5069338819f368fb8960772475069d02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MAT-175&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;5069338819f368fb8960772475069d02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MAT-150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Let’s also explore the data a little. Here’s the distribution of &lt;code&gt;std.year&lt;/code&gt;’s:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(students, aes(x=as.factor(std.year)))+geom_bar()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-27-sankey_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Clearly, most students are early in college and few students take summer math or CS courses (we don’t offer them, except internships and occasionally intro. stats).&lt;/p&gt;
&lt;p&gt;Next, we’ll look at the courses by popularity (within our dataset):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(widgetframe)

students %&amp;gt;% group_by(crs) %&amp;gt;% 
  summarise(Count = n_distinct(std.year), 
            Num.Students = n_distinct(id)) %&amp;gt;% 
  arrange(desc(Num.Students), desc(Count)) %&amp;gt;% 
  datatable() %&amp;gt;%
  frameWidget(height = 550, width = &amp;quot;100%&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:100%;height:550px;&#34; class=&#34;widgetframe html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;url&#34;:&#34;/post/2018-12-27-sankey_files/figure-html//widgets/widget_widget1.html&#34;,&#34;options&#34;:{&#34;xdomain&#34;:&#34;*&#34;,&#34;allowfullscreen&#34;:false,&#34;lazyload&#34;:false}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Since these are math and math/cs majors, it’s not surprising that single variable calculus and intro cs are the two most common courses (they’re required of all students in this dataset). What may be more interesting is that students have completed single variable calculus (MAT-175) at 4 different points in their trajectory towards a math or math/cs major. This means that we will expect 4 nodes for MAT-175 in our sankey diagram (this may get messy).&lt;/p&gt;
&lt;p&gt;There is one thing we can do to make things a little cleaner. The MAT-28x intro to proofs courses should be combined since students usually only take MAT-280, MAT-281, MAT-282, etc. depending on what’s offered the year they need to take it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;students$crs &amp;lt;- str_replace(students$crs, &amp;quot;MAT-28[:digit:]{1}&amp;quot;, &amp;quot;MAT-28X&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;building-nodes-and-edges&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Building Nodes and Edges&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;sankeyNetwork&lt;/code&gt; command, like most &lt;code&gt;networkD3&lt;/code&gt; commands, needs to be given a graph (nodes and edges). Unfortunately, it’s not smart enough to build it from our &lt;code&gt;students&lt;/code&gt; data frame, we need to build it.&lt;/p&gt;
&lt;p&gt;The nodes are easier, so we’ll start there. We want an &lt;code&gt;id&lt;/code&gt; for each node that combines the &lt;code&gt;crs&lt;/code&gt; and &lt;code&gt;std.year&lt;/code&gt; variables, so I’ll keep those and make a new &lt;code&gt;id&lt;/code&gt; column (note this will lose all student info).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nodes &amp;lt;- group_by(students, crs, std.year) %&amp;gt;% 
  summarise(cnt = n()) %&amp;gt;% ungroup() %&amp;gt;%
  select(name=crs, std.year) %&amp;gt;% 
  unite(id, name, std.year, sep=&amp;quot;_&amp;quot;, remove=FALSE)

nodes %&amp;gt;% head() %&amp;gt;% knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.year&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150_0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150_0.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150_1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150_1.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150_2.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150_3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The &lt;code&gt;group_by&lt;/code&gt;, &lt;code&gt;summarise&lt;/code&gt;, and &lt;code&gt;ungroup&lt;/code&gt; sequence is just a way to collapse down to each distinct &lt;code&gt;crs&lt;/code&gt;, &lt;code&gt;std.year&lt;/code&gt; pair that occurs in the data. We now have 89 nodes that will appear in the diagram.&lt;/p&gt;
&lt;p&gt;Now for the edges. We’re going to create a data frame of “source” nodes and another of “target” nodes (remember we’re building a directed graph). A full join of the two will give all possible edges, so we’ll then remove those that aren’t needed. Most of the extra edges will be those connecting what a student did during fall of freshmen year to all subsequent courses, not just the immediate “next” course that we want plotted. These seem far easier to remove then to selectively build a much more careful join (only joining pairs with “adjacent” standard years).&lt;/p&gt;
&lt;p&gt;Here’s the code to build the first pass of an edge list data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#build source data
s.crs &amp;lt;- select(students, s.name=crs, s.std.year=std.year, id)
s.crs &amp;lt;- s.crs[!duplicated(s.crs),]
#build target data
t.crs &amp;lt;- select(students, t.name=crs, t.std.year=std.year, id)
t.crs &amp;lt;- t.crs[!duplicated(t.crs),]
#join and remove self-loops and backward edges
edgePerStudent &amp;lt;- full_join(s.crs, t.crs) %&amp;gt;% 
  filter(s.name!=t.name, s.std.year&amp;lt;t.std.year) %&amp;gt;%
  arrange(id, s.std.year, t.std.year)

edgePerStudent %&amp;gt;% head() %&amp;gt;% knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;s.name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;s.std.year&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;t.name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t.std.year&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;09d4fb320db773a123eb97e7260caba1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MAT-275&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;MAT-175&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;09d4fb320db773a123eb97e7260caba1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MAT-275&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;09d4fb320db773a123eb97e7260caba1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MAT-28X&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;MAT-175&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;09d4fb320db773a123eb97e7260caba1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MAT-28X&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;09d4fb320db773a123eb97e7260caba1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MAT-352&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;MAT-175&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;09d4fb320db773a123eb97e7260caba1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MAT-352&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As the name implies, we now have a data frame where each row is an edge from a distinct student. It’s important to note that I’ve arranged this in order for what comes next: id, then source standard year, and then target standard year. This data frame has 1112 rows, which is too many. We have extra edges formed by paths of what we want to keep and we need to collapse to have an edge with a weight equal to number of students taking that sequence of courses. The second part is easy (we could do a &lt;code&gt;group_by&lt;/code&gt; and &lt;code&gt;summarise&lt;/code&gt; now, but we would over count). The first part is possibly the ugliest R code I’ve written in a while, but it works and without messy lags and functional trickery is fairly straightforward. I initially tried to use a “run-length encoding”, but the resulting objects became to cumbersome (an &lt;code&gt;rle&lt;/code&gt; object isn’t a data frame or a list or anything else “nice”), so here’s a &lt;code&gt;for&lt;/code&gt; loop in R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#build edges
edgeDF &amp;lt;- edgePerStudent[1,] #got to start somewhere
for(i in 2:nrow(edgePerStudent)){
  if(edgeDF[nrow(edgeDF),]$id != edgePerStudent[i,]$id){
    #different id&amp;#39;s count
    edgeDF &amp;lt;- rbind.data.frame(edgeDF, edgePerStudent[i,])
  }
  else if(edgeDF[nrow(edgeDF),]$s.std.year == edgePerStudent[i,]$s.std.year &amp;amp; edgeDF[nrow(edgeDF),]$t.std.year == edgePerStudent[i,]$t.std.year){
    #same source and target as counting counts
    edgeDF &amp;lt;- rbind.data.frame(edgeDF, edgePerStudent[i,])
  }
  else if(edgeDF[nrow(edgeDF),]$t.std.year == edgePerStudent[i,]$s.std.year){
    #if last counted target is current source, it counts
    edgeDF &amp;lt;- rbind.data.frame(edgeDF, edgePerStudent[i,])
  }
  
}

#now count edge weights
edgeDF &amp;lt;- group_by(edgeDF, s.name, s.std.year, t.name,t.std.year) %&amp;gt;% 
  summarise(value=n_distinct(id))
edgeDF &amp;lt;- unite(edgeDF, s.id, s.name,s.std.year, sep=&amp;quot;_&amp;quot;, remove=FALSE) %&amp;gt;%
  unite(t.id, t.name,t.std.year, sep=&amp;quot;_&amp;quot;, remove = FALSE)

edgeDF %&amp;gt;% head() %&amp;gt;% knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;s.id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;s.name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;s.std.year&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;t.id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;t.name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t.std.year&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150_0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSC-160_0.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSC-160&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150_0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MAT-175_0.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MAT-175&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150_0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MAT-212_0.2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MAT-212&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150_0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MAT-28X_0.2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MAT-28X&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150_0.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSC-152_1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSC-152&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150_0.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSC-150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MAT-175_1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MAT-175&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now we have 198 edges, with the desired weights for the sankey diagram.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;making-the-diagram&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Making the Diagram&lt;/h2&gt;
&lt;p&gt;Now we just call &lt;code&gt;sankeyNetwork&lt;/code&gt; and we’re done right? No, because we have data in R that we have to send to JavaScript to do the plotting so there’s a little book-keeping left to do. First, we need numeric node id’s and we need those id’s in the edge data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#replace character s.id and t.id with numbers
nodes$node.id &amp;lt;- 1:nrow(nodes)
edgeDF &amp;lt;- inner_join(edgeDF, nodes, by=c(&amp;quot;s.id&amp;quot;=&amp;quot;id&amp;quot;))
edgeDF &amp;lt;- select(edgeDF, -s.id, source=node.id)
edgeDF &amp;lt;- inner_join(edgeDF, nodes, by=c(&amp;quot;t.id&amp;quot;=&amp;quot;id&amp;quot;))
edgeDF &amp;lt;- select(edgeDF, -t.id, target=node.id)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next is the big conflict between R and almost every other programming language: indexing. R starts counting at 1, but JavaScript starts at 0 (as does Python, C/C++, Java, …) so we’ll have to re-index our node id’s. I’ll also replace everything after the first digit of &lt;code&gt;std.year&lt;/code&gt; info in the node &lt;code&gt;id&lt;/code&gt; variable, with a string indicating the semester.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#switch to zero indexing for javascript
edgeDFTemp &amp;lt;- mutate(edgeDF, source=source-1, target = target-1)
nodeDFTemp &amp;lt;- mutate(nodes, nid = node.id-1, id = 
                       ifelse(str_detect(id, &amp;quot;\\.6&amp;quot;), 
                              str_replace(id, &amp;quot;\\.6&amp;quot;,&amp;quot;SU&amp;quot;),
                              ifelse(str_detect(id, &amp;quot;\\.4&amp;quot;),
                                     str_replace(id, &amp;quot;\\.4&amp;quot;, &amp;quot;SP&amp;quot;),
                                     ifelse(str_detect(id, &amp;quot;\\.2&amp;quot;),
                                            str_replace(id, &amp;quot;\\.2&amp;quot;, &amp;quot;W&amp;quot;),
                                            str_c(id, &amp;quot;FA&amp;quot;)))))

sn2 &amp;lt;- sankeyNetwork(Links = edgeDFTemp, Nodes = nodeDFTemp, Source = &amp;quot;source&amp;quot;, 
              Target = &amp;quot;target&amp;quot;, Value = &amp;quot;value&amp;quot;, NodeID = &amp;quot;id&amp;quot;, 
              nodeWidth = 20, fontSize = 8, units = &amp;quot;students&amp;quot;)
htmlwidgets::onRender(sn2, &amp;#39;document.getElementsByTagName(&amp;quot;svg&amp;quot;)[2].setAttribute(&amp;quot;viewBox&amp;quot;, &amp;quot;&amp;quot;)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-3&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;sankeyNetwork html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-3&#34;&gt;{&#34;x&#34;:{&#34;links&#34;:{&#34;source&#34;:[0,0,0,0,1,1,1,1,2,2,2,2,2,3,3,3,4,4,5,5,6,6,6,6,7,7,7,7,7,8,9,9,9,10,10,10,11,12,12,13,15,15,18,18,18,19,19,19,20,21,21,21,23,24,26,27,27,28,29,30,32,32,33,34,34,35,35,35,36,37,38,38,38,38,38,38,38,39,39,39,40,40,41,41,41,41,41,41,41,41,41,42,42,42,42,43,43,43,44,45,45,45,47,47,48,50,51,53,53,53,53,53,54,54,55,56,57,59,59,59,60,62,62,62,62,62,62,62,62,62,63,63,63,64,64,64,64,64,64,64,64,65,65,65,65,65,65,65,65,65,65,66,67,67,68,69,69,69,69,70,70,70,71,71,71,71,71,72,73,73,74,74,74,74,74,74,75,75,75,75,77,77,77,77,78,79,80,81,82,82,86,86,86,86,87,88,88,88],&#34;target&#34;:[9,42,53,64,7,43,47,62,15,44,60,65,77,63,72,75,50,70,85,88,14,39,54,59,18,65,71,74,77,19,7,62,68,8,24,69,23,7,68,23,21,24,15,20,60,16,61,84,21,11,13,75,81,22,28,1,39,39,40,44,31,39,39,1,42,2,7,43,44,74,1,34,35,42,53,64,78,2,30,36,3,44,1,9,14,46,51,53,54,59,64,7,47,62,65,15,60,77,63,1,39,42,37,65,72,88,62,1,9,42,59,73,2,62,74,75,19,2,7,62,21,3,18,37,55,65,71,74,77,79,49,66,72,1,9,12,14,46,59,73,83,3,10,15,20,49,56,60,71,72,74,72,8,69,18,19,72,75,80,58,85,88,8,57,69,75,82,70,2,62,8,48,57,63,69,86,5,23,70,87,10,67,71,74,1,3,84,85,4,86,5,50,70,87,81,17,76,85],&#34;value&#34;:[1,1,1,3,8,3,1,10,1,1,1,1,1,1,1,2,1,1,1,1,1,1,1,1,2,6,1,1,1,1,2,2,1,1,1,1,1,1,1,1,1,1,1,2,2,1,1,1,1,1,1,1,1,1,2,1,1,2,1,1,1,1,1,1,1,1,1,2,1,1,9,1,2,13,2,1,1,1,1,1,1,1,8,1,1,2,1,5,2,2,5,4,1,6,1,1,1,1,1,1,1,1,1,1,1,1,1,3,1,2,1,1,1,1,1,1,1,1,1,3,1,1,1,1,1,14,2,1,1,1,1,1,2,3,1,1,1,1,4,1,1,3,1,5,4,1,1,4,8,1,3,1,1,1,1,1,1,1,1,1,1,1,1,1,4,1,1,2,1,1,1,1,1,2,2,1,1,1,2,1,1,1,1,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1]},&#34;nodes&#34;:{&#34;name&#34;:[&#34;CSC-150_0FA&#34;,&#34;CSC-150_0SP&#34;,&#34;CSC-150_1FA&#34;,&#34;CSC-150_1SP&#34;,&#34;CSC-150_2SP&#34;,&#34;CSC-150_3FA&#34;,&#34;CSC-152_0FA&#34;,&#34;CSC-152_1FA&#34;,&#34;CSC-152_2FA&#34;,&#34;CSC-160_0SP&#34;,&#34;CSC-160_1SP&#34;,&#34;CSC-160_2SP&#34;,&#34;CSC-235_0SP&#34;,&#34;CSC-235_2SP&#34;,&#34;CSC-270_0SP&#34;,&#34;CSC-270_1SP&#34;,&#34;CSC-270_2SP&#34;,&#34;CSC-270_3SP&#34;,&#34;CSC-285_1W&#34;,&#34;CSC-285_2W&#34;,&#34;CSC-340_1SP&#34;,&#34;CSC-350_2FA&#34;,&#34;CSC-494_2FA&#34;,&#34;CSC-494_3FA&#34;,&#34;CSC-497_1SU&#34;,&#34;CSC-497_2FA&#34;,&#34;MAT-101_0FA&#34;,&#34;MAT-102_0FA&#34;,&#34;MAT-102_0W&#34;,&#34;MAT-102_0SP&#34;,&#34;MAT-102_1FA&#34;,&#34;MAT-111_0W&#34;,&#34;MAT-125_0FA&#34;,&#34;MAT-130_0FA&#34;,&#34;MAT-130_0W&#34;,&#34;MAT-130_0SP&#34;,&#34;MAT-130_1FA&#34;,&#34;MAT-130_1W&#34;,&#34;MAT-150_0FA&#34;,&#34;MAT-150_0SP&#34;,&#34;MAT-150_1FA&#34;,&#34;MAT-175_0FA&#34;,&#34;MAT-175_0SP&#34;,&#34;MAT-175_1FA&#34;,&#34;MAT-175_1SP&#34;,&#34;MAT-199_0FA&#34;,&#34;MAT-199_0SP&#34;,&#34;MAT-199_1FA&#34;,&#34;MAT-199_2FA&#34;,&#34;MAT-199_2SP&#34;,&#34;MAT-199_3FA&#34;,&#34;MAT-199T_0SP&#34;,&#34;MAT-212_0FA&#34;,&#34;MAT-212_0W&#34;,&#34;MAT-212_0SP&#34;,&#34;MAT-212_1W&#34;,&#34;MAT-212_1SP&#34;,&#34;MAT-212_2FA&#34;,&#34;MAT-212_3SP&#34;,&#34;MAT-252_0SP&#34;,&#34;MAT-252_1SP&#34;,&#34;MAT-252_2SP&#34;,&#34;MAT-275_1FA&#34;,&#34;MAT-275_2FA&#34;,&#34;MAT-28X_0W&#34;,&#34;MAT-28X_1W&#34;,&#34;MAT-28X_2W&#34;,&#34;MAT-294_1SP&#34;,&#34;MAT-311_1FA&#34;,&#34;MAT-311_2FA&#34;,&#34;MAT-311_3FA&#34;,&#34;MAT-352_1SP&#34;,&#34;MAT-352_2SP&#34;,&#34;MAT-361_0SP&#34;,&#34;MAT-361_1SP&#34;,&#34;MAT-361_2SP&#34;,&#34;MAT-361_3SP&#34;,&#34;MAT-370_1W&#34;,&#34;MAT-372_0W&#34;,&#34;MAT-372_1W&#34;,&#34;MAT-372_2W&#34;,&#34;MAT-372_3W&#34;,&#34;MAT-399T_2FA&#34;,&#34;MAT-431_0SP&#34;,&#34;MAT-431_2SP&#34;,&#34;MAT-431_3SP&#34;,&#34;MAT-451_2SP&#34;,&#34;MAT-494_3FA&#34;,&#34;MAT-494_3W&#34;],&#34;group&#34;:[&#34;CSC-150_0FA&#34;,&#34;CSC-150_0SP&#34;,&#34;CSC-150_1FA&#34;,&#34;CSC-150_1SP&#34;,&#34;CSC-150_2SP&#34;,&#34;CSC-150_3FA&#34;,&#34;CSC-152_0FA&#34;,&#34;CSC-152_1FA&#34;,&#34;CSC-152_2FA&#34;,&#34;CSC-160_0SP&#34;,&#34;CSC-160_1SP&#34;,&#34;CSC-160_2SP&#34;,&#34;CSC-235_0SP&#34;,&#34;CSC-235_2SP&#34;,&#34;CSC-270_0SP&#34;,&#34;CSC-270_1SP&#34;,&#34;CSC-270_2SP&#34;,&#34;CSC-270_3SP&#34;,&#34;CSC-285_1W&#34;,&#34;CSC-285_2W&#34;,&#34;CSC-340_1SP&#34;,&#34;CSC-350_2FA&#34;,&#34;CSC-494_2FA&#34;,&#34;CSC-494_3FA&#34;,&#34;CSC-497_1SU&#34;,&#34;CSC-497_2FA&#34;,&#34;MAT-101_0FA&#34;,&#34;MAT-102_0FA&#34;,&#34;MAT-102_0W&#34;,&#34;MAT-102_0SP&#34;,&#34;MAT-102_1FA&#34;,&#34;MAT-111_0W&#34;,&#34;MAT-125_0FA&#34;,&#34;MAT-130_0FA&#34;,&#34;MAT-130_0W&#34;,&#34;MAT-130_0SP&#34;,&#34;MAT-130_1FA&#34;,&#34;MAT-130_1W&#34;,&#34;MAT-150_0FA&#34;,&#34;MAT-150_0SP&#34;,&#34;MAT-150_1FA&#34;,&#34;MAT-175_0FA&#34;,&#34;MAT-175_0SP&#34;,&#34;MAT-175_1FA&#34;,&#34;MAT-175_1SP&#34;,&#34;MAT-199_0FA&#34;,&#34;MAT-199_0SP&#34;,&#34;MAT-199_1FA&#34;,&#34;MAT-199_2FA&#34;,&#34;MAT-199_2SP&#34;,&#34;MAT-199_3FA&#34;,&#34;MAT-199T_0SP&#34;,&#34;MAT-212_0FA&#34;,&#34;MAT-212_0W&#34;,&#34;MAT-212_0SP&#34;,&#34;MAT-212_1W&#34;,&#34;MAT-212_1SP&#34;,&#34;MAT-212_2FA&#34;,&#34;MAT-212_3SP&#34;,&#34;MAT-252_0SP&#34;,&#34;MAT-252_1SP&#34;,&#34;MAT-252_2SP&#34;,&#34;MAT-275_1FA&#34;,&#34;MAT-275_2FA&#34;,&#34;MAT-28X_0W&#34;,&#34;MAT-28X_1W&#34;,&#34;MAT-28X_2W&#34;,&#34;MAT-294_1SP&#34;,&#34;MAT-311_1FA&#34;,&#34;MAT-311_2FA&#34;,&#34;MAT-311_3FA&#34;,&#34;MAT-352_1SP&#34;,&#34;MAT-352_2SP&#34;,&#34;MAT-361_0SP&#34;,&#34;MAT-361_1SP&#34;,&#34;MAT-361_2SP&#34;,&#34;MAT-361_3SP&#34;,&#34;MAT-370_1W&#34;,&#34;MAT-372_0W&#34;,&#34;MAT-372_1W&#34;,&#34;MAT-372_2W&#34;,&#34;MAT-372_3W&#34;,&#34;MAT-399T_2FA&#34;,&#34;MAT-431_0SP&#34;,&#34;MAT-431_2SP&#34;,&#34;MAT-431_3SP&#34;,&#34;MAT-451_2SP&#34;,&#34;MAT-494_3FA&#34;,&#34;MAT-494_3W&#34;]},&#34;options&#34;:{&#34;NodeID&#34;:&#34;id&#34;,&#34;NodeGroup&#34;:&#34;id&#34;,&#34;LinkGroup&#34;:null,&#34;colourScale&#34;:&#34;d3.scaleOrdinal(d3.schemeCategory20);&#34;,&#34;fontSize&#34;:8,&#34;fontFamily&#34;:null,&#34;nodeWidth&#34;:20,&#34;nodePadding&#34;:10,&#34;units&#34;:&#34;students&#34;,&#34;margin&#34;:{&#34;top&#34;:null,&#34;right&#34;:null,&#34;bottom&#34;:null,&#34;left&#34;:null},&#34;iterations&#34;:32,&#34;sinksRight&#34;:true}},&#34;evals&#34;:[],&#34;jsHooks&#34;:{&#34;render&#34;:[{&#34;code&#34;:&#34;document.getElementsByTagName(\&#34;svg\&#34;)[2].setAttribute(\&#34;viewBox\&#34;, \&#34;\&#34;)&#34;,&#34;data&#34;:null}]}}&lt;/script&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>PCA Overview</title>
      <link>/post/2018-11-24-pca/</link>
      <pubDate>Sat, 24 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-11-24-pca/</guid>
      <description>


&lt;p&gt;This post is primarily to give the basic overview of principal components analysis (PCA) for dimensionality reduction and regression. I wanted to create it as a guide for my regression students who may find it useful for their projects. First, let’s note the two main times that you may want to use PCA - dimensionality reduction (reducing variables in a dataset) and removing colinearity issues. These are not exclusive problems, often you want to do both. However, depending on the data, PCA will ensure a lack of colinearity among the principal components but may not be able to use less variables in subsequent models.&lt;/p&gt;
&lt;div id=&#34;basic-idea&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Basic Idea&lt;/h2&gt;
&lt;p&gt;Before getting into real examples, let’s look at what PCA does in 2 dimensions. I’ll generate some highly correlated data and compute the principal components, and we’ll make it easy to predict the components. My data will be related by &lt;span class=&#34;math inline&#34;&gt;\(y=3*x+1+\epsilon\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is normally distributed random error. This means that the greatest variation in my data should be along the line &lt;span class=&#34;math inline&#34;&gt;\(y=3x+1\)&lt;/span&gt;, which should give the first principal component. The second (and final in the 2-D case) will be along the perpendicular line &lt;span class=&#34;math inline&#34;&gt;\(y=\frac{-1}{3}x+b\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr) #pipes and df manipulation
library(ggplot2) #graphing
library(patchwork) #graph layout

x &amp;lt;- seq(from=1, to=10, length.out = 100)
rn &amp;lt;- rnorm(100, mean=0, sd=1.25) #random error
y &amp;lt;- 3*x+1+rn 
df &amp;lt;- data.frame(x, y)

#compute principal components
pcdf &amp;lt;- prcomp(df)

#graph data and pc&amp;#39;s
data.graph &amp;lt;- ggplot(df, aes(x=x,y=y))+geom_point()+
  ggtitle(&amp;quot;Original Data&amp;quot;)
pc.graph &amp;lt;- ggplot(as.data.frame(pcdf$x), aes(x=PC1, y=PC2))+
  geom_point()+ggtitle(&amp;quot;Data in PC-space&amp;quot;)

data.graph+pc.graph&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-24-pca_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice how the correlation between &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; vanishes when looking at the data with axes aligned along the principal components - now &lt;code&gt;PC1&lt;/code&gt; and &lt;code&gt;PC2&lt;/code&gt; provide non-colinear data to us in regression. Furthermore, the &lt;code&gt;sdev&lt;/code&gt; element of &lt;code&gt;pcdf&lt;/code&gt; tells use how much of the standard deviation (and hence variance) is explained by each component:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pcdf$sdev^2/sum(pcdf$sdev^2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.997359167 0.002640833&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So &lt;code&gt;PC1&lt;/code&gt; accounts for almost all of the variance seen in the original data. This isn’t surprising given how the data was made, it is so highly correlated that the data is basically one-dimensional and PCA has found that. With higher dimensional data, a &lt;span class=&#34;math inline&#34;&gt;\(scree~plot\)&lt;/span&gt; is useful to see how additional components explain more variance:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data.frame(
  component = 1:length(pcdf$sdev), 
  explained.var.pct = pcdf$sdev^2/sum(pcdf$sdev^2)
  ), 
  aes(x=component, y=cumsum(explained.var.pct)))+
    geom_line()+ylab(&amp;quot;Total Percent Variance Explained&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-24-pca_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now, what about the relationship in our data (&lt;span class=&#34;math inline&#34;&gt;\(y=3x+1\)&lt;/span&gt;) and the principal components? The &lt;code&gt;rotation&lt;/code&gt; element of &lt;code&gt;pcdf&lt;/code&gt; gives us a matrix of eigenvectors that tells use how to turn a point in the original &lt;span class=&#34;math inline&#34;&gt;\(xy\)&lt;/span&gt;-plane into a point in the &lt;span class=&#34;math inline&#34;&gt;\(PC1PC2\)&lt;/span&gt;-plane. The second row of the rotation matrix divided by the first (&lt;span class=&#34;math inline&#34;&gt;\(y/x\)&lt;/span&gt;) gives use slopes of almost &lt;span class=&#34;math inline&#34;&gt;\(3\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\frac{-1}{3}\)&lt;/span&gt; (the difference is the random error I’ve added to the data). The principal components are just a new basis (in the linear algebra sense), each column is a unit vector and the columns are orthogonal to each-other, so in two-dimensions the slope determines a unit vector. In higher dimensions this gets more complicated, but the rotation matrix columns still give us the direction vector for the principal components. If you remember multivariate calculus, you can turn a direction vector into a line in higher dimensions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-real-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A Real Example&lt;/h2&gt;
&lt;p&gt;I’ll load data from &lt;a href=&#34;/files/sb3tnzv.csv&#34;&gt;sb3tnzv&lt;/a&gt;, which has data about the content of certain molecules in a certain species of sagebrush (this is related to a collaboration with a biochemist).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sb &amp;lt;- read.csv(&amp;quot;../../static/files/sb3tnzv.csv&amp;quot;, header=TRUE)
knitr::kable(head(sb))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;species&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;browsed&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;CYP1A.grouse.micr&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;CYP1A.human.micr&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB02&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB03&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB05&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB07&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB09&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB10&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB11&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB12&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB13&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB14&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB15&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB16&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB17&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB18&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB19&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB20&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB22&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB23&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB24&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB26&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB28&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB29&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB30&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB31&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB32&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB34&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB36&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB37&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB38&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB39&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB40&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB41&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB42&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB43&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB44&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB45&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB45s&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB46&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB47&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SB48&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;105&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3T&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NB&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;629.0214&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;565.9454&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1510.473&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;618.7710&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1389.2164&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;719.0859&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;370.6543&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10076.76&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;26818.27&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;55521.95&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7762.168&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7988.484&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;29838.72&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;70267.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;109853.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;60935.48&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5583.644&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4029.331&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;76519.95&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;134&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3T&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;418.1162&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;787.7897&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;649.8425&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1825.224&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;749.1660&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1629.1661&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;815.1082&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;495.5262&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13573.21&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1406.067&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;29028.81&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;68483.62&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9038.913&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9783.465&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;30501.25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10599.994&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7101.127&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8305.438&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4244.354&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2289.997&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13235.83&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3617.598&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5641.736&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5766.007&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;67198.16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;127896.20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;71841.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;66638.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17905.29&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4631.608&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7263.780&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;86716.27&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;154&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3T&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NB&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;137.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;64.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;403.1469&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;654.3164&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;565.2982&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1808.972&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;650.8421&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1560.4817&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1266.7593&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12916.54&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;37969.62&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;66222.91&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8011.746&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8014.458&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25759.11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6783.765&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6627.330&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6373.151&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4767.672&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3078.736&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15643.33&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4111.693&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5938.325&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16812.998&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;59342.97&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;93693.29&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;69981.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;64628.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18887.17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5405.461&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4248.908&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;67234.19&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;182&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3T&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NB&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;123.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;62.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;558.8956&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1044.3494&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2590.590&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1161.7732&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1744.6504&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;902.3607&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10598.65&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1635.713&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;32982.45&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;67699.61&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8178.428&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6533.004&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;27824.57&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12031.102&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6346.032&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9447.529&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5135.961&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2169.135&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6862.923&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6174.927&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;23795.510&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14001.83&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20861.51&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;131440.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;147611.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20330.75&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4810.313&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8282.686&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;80970.20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;222&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3T&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NB&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;161.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;74.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;254.2615&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;431.4813&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;445.9520&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1078.684&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;447.0190&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;895.7186&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;594.2125&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16864.11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;23013.96&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;47683.07&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5734.898&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7296.023&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12866.58&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3783.557&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4826.480&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5690.679&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3626.880&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11667.33&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3201.437&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5310.534&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8928.760&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;21563.69&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;44012.07&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20978.52&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;34732.02&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7606.205&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11376.954&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25991.20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;238&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3T&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;132.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;72.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;668.8263&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;854.496&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;332.0903&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;652.6357&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;99.0683&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;22382.95&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25243.57&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;68834.52&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9652.101&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6982.520&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;42749.88&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4809.935&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6726.340&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4758.589&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2101.150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3842.604&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4641.517&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15705.387&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;51669.16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;90192.88&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;63978.74&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;88519.23&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;24585.34&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5925.326&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3597.326&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;88216.69&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Each of the &lt;code&gt;SB&lt;/code&gt; variables basically tells you how much of the molecule is in the sample and each number corresponds to a different molecule. Hopefully PCA will help reduce the number of variables. Let’s perform the PC computation and look at a scree plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sbpc &amp;lt;- prcomp(sb[,6:45], center = TRUE, scale. = TRUE)

ggplot(data.frame(
  component = 1:length(sbpc$sdev), 
  explained.var.pct = sbpc$sdev^2/sum(sbpc$sdev^2)
  ), 
  aes(x=component, y=cumsum(explained.var.pct)))+
    geom_line()+ylab(&amp;quot;Total Percent Variance Explained&amp;quot;)+
    ggtitle(&amp;quot;Scree Plot of Sagebrush Data&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-24-pca_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This shows that we can explain most of the variation in our data with far fewer variables than all the &lt;code&gt;SB&lt;/code&gt;’s. It’s worth noting that I’ve removed all variables with zero variance already and am scaling and centering the data prior to performing the PCA computation - this is needed whenever different variables have vastly different scales.&lt;/p&gt;
&lt;p&gt;We can now go about building models using the principal components instead of the original &lt;code&gt;SB&lt;/code&gt; variables and we don’t have to worry about colinearity. Furthermore, the order of our components is in order of decreasing variance explained so we would build models using the PC’s in order (i.e. a model without PC1 but with other PC’s would be strange). The &lt;code&gt;SB&lt;/code&gt; variables lack this aspect, but are more interpretable.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Reticulated Mixture Models</title>
      <link>/post/2018-11-10-reticulate-mm/</link>
      <pubDate>Sat, 10 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-11-10-reticulate-mm/</guid>
      <description>


&lt;p&gt;Clearly, there’s no such thing as a “reticulated mixture model” but if you create one I’ll gladly take credit for the name. Instead this post is a demonstration of using mixture models for clustering and the interplay of R and Python via RStudio’s &lt;code&gt;reticulate&lt;/code&gt; package.&lt;/p&gt;
&lt;div id=&#34;mixture-model-basics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Mixture Model Basics&lt;/h1&gt;
&lt;p&gt;The idea behind mixture models is that you have data containing information from two (or more) subgroups and you want to uncover structure of the subgroups. A classic example is you have a bunch of people’s height data and you would like to figure out which are likely to be from men and which are from women. If the data set is labeled with &lt;code&gt;gender&lt;/code&gt; the problem is trivial, but if it’s not then it seems reasonable to think we’re looking at data sampled from 2 different normal distributions and we would like to use our data to get an idea what those distributions are. Of course, there’s no reason why we need to limit to only 2 groups or normal distributions, but we will here so we don’t overcomplicate the process.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;our-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Our Data&lt;/h1&gt;
&lt;p&gt;So show the mixture model process, I’m going to manufacture some data out of two bi-variate normal distributions, and I them to have different covariance matrices.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mvtnorm) #gets rmvnorm function

#function to make random covariance matrices
randCov &amp;lt;- function(n=2, k=1){
  mat &amp;lt;- matrix(runif(n^2)*k, ncol=n)
  return(t(mat)%*%mat) #make mat symmetric and return
}

cv1 &amp;lt;- randCov(2,2.5)
cv2 &amp;lt;- randCov(2,1.25)
A &amp;lt;- rmvnorm(100, mean=c(20,75), sigma = cv1)
B &amp;lt;- rmvnorm(100, mean=c(18,69), sigma = cv2)
df &amp;lt;- rbind.data.frame(as.data.frame(A), as.data.frame(B))
df$V3 &amp;lt;- c(rep(&amp;quot;A&amp;quot;,100),rep(&amp;quot;B&amp;quot;,100))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here I’ve labeled the data so we can check how our mixture model performed. Let’s look at our data with and with-out using the labels:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(patchwork)
gNoLab &amp;lt;- ggplot(df, aes(x=V1, y=V2))+geom_point()+ggtitle(&amp;quot;No Labels&amp;quot;)
gLab &amp;lt;- ggplot(df, aes(x=V1, y=V2, col=V3))+geom_point()+ggtitle(&amp;quot;True Labels&amp;quot;)

gNoLab+gLab&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-10-reticulate-mm_files/figure-html/dataViz-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now our goal will be to recover the labels is we start with the data in the left graph.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;passing-data-to-python&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Passing Data to Python&lt;/h1&gt;
&lt;p&gt;R has the functionality to build a gaussian mixture model, but I’ve been working with python some and want to use &lt;code&gt;reticulate&lt;/code&gt;’s ability to pass data and results between R and python. First, let’s get R ready:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reticulate)
use_python(&amp;quot;/usr/bin/python&amp;quot;) # I&amp;#39;m using python 3.7.1 in Arch linux&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now in a python code chunk, we can access R objects.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
import pandas as pd
print(r.df.head())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           V1         V2 V3
## 0  22.593247  75.423297  A
## 1  20.012685  74.874178  A
## 2  20.074491  75.357672  A
## 3  20.541347  75.030955  A
## 4  24.746648  76.296463  A&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;mixture-model-in-python&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Mixture Model in Python&lt;/h1&gt;
&lt;p&gt;Now that we can get our data from R into python, we’ll use SciKit Learn to build a Gaussian Mixture model. We’ll need to give two parameters, the number of components we think the mixture has and a parameter about how the covariances may vary. We also have to copy the dataframe from R into a pandas dataframe so we can add a new column.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.mixture import GaussianMixture
gmm = GaussianMixture(n_components=2, covariance_type=&amp;#39;full&amp;#39;)
pydf = r.df
pydf[&amp;#39;gml&amp;#39;]=gmm.fit_predict(pydf[[&amp;#39;V1&amp;#39;,&amp;#39;V2&amp;#39;]])
print(pydf.head())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           V1         V2 V3  gml
## 0  22.593247  75.423297  A    1
## 1  20.012685  74.874178  A    1
## 2  20.074491  75.357672  A    1
## 3  20.541347  75.030955  A    1
## 4  24.746648  76.296463  A    1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;check-results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Check Results&lt;/h1&gt;
&lt;p&gt;We can take advantage of &lt;code&gt;ggplot2&lt;/code&gt; to visualize the mixture model labels now. I’ll reproduce the graph above, but now the left side will be colored by the labels from the mixture model while the right is still colored with the true labels.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;py$pydf$gml &amp;lt;- ifelse(py$pydf$gml==0, &amp;quot;A&amp;quot;,&amp;quot;B&amp;quot;)
gMMLab &amp;lt;- ggplot(py$pydf, aes(x=V1, y=V2, col=gml))+geom_point()+ggtitle(&amp;quot;Labeled by GMM&amp;quot;)

gMMLab+gLab&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-10-reticulate-mm_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That looks pretty successful! Obviously, the more mixed the data is, the hard it is for the mixture model to correctly identify the boundary. Also, if we have the wrong number of mixture components, the model labels will muddle the components. Sklearn provides a &lt;code&gt;BayesianGaussianMixture&lt;/code&gt; that can identify less than the provided number of components. Perhaps that can be a post in the near future.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Ridges of Normality</title>
      <link>/post/2018-10-01-norm-ridge/</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-10-01-norm-ridge/</guid>
      <description>


&lt;p&gt;One of the classic assumptions of the linear regression models is that, conditioned on the explanatory variables, the response variable should be normally distributed. While teaching this the other day, I had a flash of insight into how to visualize this - ridge-line plots!&lt;/p&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;I’ve been using Matloff’s &lt;em&gt;Statistical Regression and Classification&lt;/em&gt; book, which uses the &lt;code&gt;mlb&lt;/code&gt; dataset from his &lt;code&gt;freqparcoord&lt;/code&gt; package. This has data on heights, weights, ages, positions, and teams of over 1000 major league baseball players. We’ll focus on just height and weight for now. Let’s load the packages we’ll need and the data and look at a simple scatter plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(freqparcoord)
library(dplyr)
library(purrr)
library(ggplot2)
library(ggridges)
data(mlb)

ggplot(mlb, aes(x=Height, y=Weight))+
  geom_point(alpha=.2)+
  ggtitle(&amp;quot;Heights and Weights of MLB Players&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-01-norm-ridge_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Because height is only measured to the inch, the data is naturally “grouped” which helps see the conditioning we’ll need.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;first-try-at-normality&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;First Try at Normality&lt;/h2&gt;
&lt;p&gt;The first thing I tried in class was to use a side-by-side boxplot that we had constructed earlier in the semester. This uses a familiar visualization to see the distribution of weights for each height - but even symmetric boxplots don’t ensure normality. Here’s the graph:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(mlb, aes(x=Height, y=Weight, group=Height))+
  geom_boxplot()+
  ggtitle(&amp;quot;Boxplots of Weights for each Height of MLB Player&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-01-norm-ridge_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;second-try-at-normality&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Second Try at Normality&lt;/h2&gt;
&lt;p&gt;While we were discussing the normality (or lack of it) in the boxplots, I thought about using a ridge-line plot so I brought this up in class:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(mlb, aes(y=Height, x=Weight, group=Height))+
  geom_density_ridges()+
  ggtitle(&amp;quot;Ridgeline Plot of MLB Players Weights for each Height&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-01-norm-ridge_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This has the advantage of showing the sample distribution. Few if any of my students had seen such a plot, but seemed to understand what it was showing. I remarked that these seemed pretty normal, for real sample data, right at the end of class and at least one student looked concerned about that.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-with-a-normal-sample&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparing with a Normal Sample&lt;/h2&gt;
&lt;p&gt;To show how normal the data is, I decided to generate sample normal data (via &lt;code&gt;rnorm&lt;/code&gt;) with the same mean and standard deviation of weights for each height. First, let’s build the dataframe of conditional means and standard deviations (homoscedasticity is another issue):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mlbNormP &amp;lt;- group_by(mlb, Height) %&amp;gt;% 
  summarise(mean=mean(Weight), sd = sd(Weight), n=n()) %&amp;gt;%
  filter(n&amp;gt;4)

knitr::kable(head(mlbNormP))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;Height&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;sd&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;68&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;173.8571&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;22.08641&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;69&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;179.9474&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15.32055&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;70&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;183.0980&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13.54143&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;51&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;71&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;190.3596&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16.43461&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;89&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;72&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;192.5600&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17.56349&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;150&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;73&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;196.7716&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16.41249&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;162&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;I’ve filtered out heights with less than 4 players, mainly for aesthetic purposes. Now for each row of this dataframe, we want to generate a random sample of normally distributed data. This is where &lt;code&gt;purrr::pmap_dfr&lt;/code&gt; comes in - which will &lt;code&gt;map&lt;/code&gt; (tidyverse version of &lt;code&gt;apply&lt;/code&gt;) a function onto a list of input vectors in parallel and bind the results into a dataframe along rows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mlbNorm&amp;lt;-pmap_dfr(
  list(
    x=mlbNormP$Height, y=mlbNormP$mean, 
    z=mlbNormP$sd, w = mlbNormP$n),
  function(x,y,z,w){
    data.frame(Ht=rep(x,100), WtNorm=rnorm(100, y, z))
    }) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here I’m not using the size of our sample of each height, instead using 100 for each height. Now let’s plot both ridge-lines, and color based on the observed (blue) or generated (red) data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot()+
  geom_density_ridges(data=mlbNorm, 
                      aes(y=Ht, x=WtNorm, group=Ht), fill=&amp;quot;red&amp;quot;, alpha=.3) + 
  geom_density_ridges(data=mlb, 
                      aes(x=Weight, y=Height, group=Height), 
                      fill=&amp;quot;blue&amp;quot;, alpha=.3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-01-norm-ridge_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Alternatively, we can take advantage of the size of each sample of observed data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mlbNorm2&amp;lt;-pmap_dfr(
  list(
    x=mlbNormP$Height, y=mlbNormP$mean, 
    z=mlbNormP$sd, w = mlbNormP$n),
  function(x,y,z,w){
    data.frame(Ht=rep(x,w), WtNorm=rnorm(w, y, z))
    }) 

ggplot()+
  geom_density_ridges(data=mlbNorm2, 
                      aes(y=Ht, x=WtNorm, group=Ht), fill=&amp;quot;red&amp;quot;, alpha=.3) + 
  geom_density_ridges(data=mlb, 
                      aes(x=Weight, y=Height, group=Height), 
                      fill=&amp;quot;blue&amp;quot;, alpha=.3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-01-norm-ridge_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In either case, the generated random normal data is very similar to the actual data in our dataset. This also seems to provide a nice, general method to visualize if this assumption of our linear model is violated by the data.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>What a Tangled Web We Weave...</title>
      <link>/post/2018-08-21-chords/</link>
      <pubDate>Tue, 21 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-08-21-chords/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/d3/d3.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/d3-tip/index.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/chorddiag/chorddiag.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/chorddiag/chorddiag.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/chorddiag-binding/chorddiag.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;Update 7/23/2019&lt;/strong&gt; Various package updates have created problems with showing more than one javascript plot on a post. I’ve added calls to &lt;code&gt;htlwidgets::onRender&lt;/code&gt; to get at least one plot displayed. I may revisit this, but the interaction between hugo, blogdown, and various javascript libraries (chorddiag, networkD3, D3, data tables, etc) is more than I’m able to dive into at the moment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cd &amp;lt;- chorddiag(
  xtabs(~MAJOR+minor, data = mmhl[mmhl$Grad.Year == 2017,],
        drop.unused.levels = TRUE),
  showTicks = FALSE, groupColors = many_colors, type = &amp;quot;bipartite&amp;quot;
)

htmlwidgets::onRender(cd,&amp;#39;document.getElementsByTagName(&amp;quot;svg&amp;quot;)[0].setAttribute(&amp;quot;viewBox&amp;quot;, &amp;quot;&amp;quot;)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;chorddiag html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;matrix&#34;:[[0,0,0,0,0,0,0,0,2,2,0,1,0,0,2,0,0,0,0,0,5,0,1,1,4,1,3,1],[0,0,0,0,0,0,0,0,1,0,0,1,0,1,0,1,0,0,0,0,0,0,0,0,1,0,1,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1],[0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0],[0,0,0,0,0,0,0,0,1,2,0,0,1,2,0,4,0,0,1,0,0,1,0,1,3,0,1,0],[0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0],[2,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,1,0,0,1,1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[2,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,1,0,0,0,0,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[5,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[4,1,1,0,1,1,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[3,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]],&#34;options&#34;:{&#34;type&#34;:&#34;bipartite&#34;,&#34;width&#34;:null,&#34;height&#34;:null,&#34;margin&#34;:100,&#34;showGroupnames&#34;:true,&#34;groupNames&#34;:[&#34;BUS&#34;,&#34;CHE&#34;,&#34;HEA&#34;,&#34;HIS&#34;,&#34;MAT&#34;,&#34;MATCS&#34;,&#34;MATPH&#34;,&#34;THE&#34;,&#34;ais&#34;,&#34;artde&#34;,&#34;bus&#34;,&#34;criju&#34;,&#34;crw&#34;,&#34;csc&#34;,&#34;csd&#34;,&#34;dueng&#34;,&#34;ed10&#34;,&#34;fre&#34;,&#34;his&#34;,&#34;hpmed&#34;,&#34;matap&#34;,&#34;phi&#34;,&#34;phsci&#34;,&#34;poec&#34;,&#34;psy&#34;,&#34;relst&#34;,&#34;spa&#34;,&#34;vstdy&#34;],&#34;groupColors&#34;:[&#34;#7FC97F&#34;,&#34;#BEAED4&#34;,&#34;#FDC086&#34;,&#34;#FFFF99&#34;,&#34;#386CB0&#34;,&#34;#F0027F&#34;,&#34;#BF5B17&#34;,&#34;#666666&#34;,&#34;#1B9E77&#34;,&#34;#D95F02&#34;,&#34;#7570B3&#34;,&#34;#E7298A&#34;,&#34;#66A61E&#34;,&#34;#E6AB02&#34;,&#34;#A6761D&#34;,&#34;#666666&#34;,&#34;#A6CEE3&#34;,&#34;#1F78B4&#34;,&#34;#B2DF8A&#34;,&#34;#33A02C&#34;,&#34;#FB9A99&#34;,&#34;#E31A1C&#34;,&#34;#FDBF6F&#34;,&#34;#FF7F00&#34;,&#34;#CAB2D6&#34;,&#34;#6A3D9A&#34;,&#34;#FFFF99&#34;,&#34;#B15928&#34;,&#34;#FBB4AE&#34;,&#34;#B3CDE3&#34;,&#34;#CCEBC5&#34;,&#34;#DECBE4&#34;,&#34;#FED9A6&#34;,&#34;#FFFFCC&#34;,&#34;#E5D8BD&#34;,&#34;#FDDAEC&#34;,&#34;#F2F2F2&#34;,&#34;#B3E2CD&#34;,&#34;#FDCDAC&#34;,&#34;#CBD5E8&#34;,&#34;#F4CAE4&#34;,&#34;#E6F5C9&#34;,&#34;#FFF2AE&#34;,&#34;#F1E2CC&#34;,&#34;#CCCCCC&#34;,&#34;#E41A1C&#34;,&#34;#377EB8&#34;,&#34;#4DAF4A&#34;,&#34;#984EA3&#34;,&#34;#FF7F00&#34;,&#34;#FFFF33&#34;,&#34;#A65628&#34;,&#34;#F781BF&#34;,&#34;#999999&#34;,&#34;#66C2A5&#34;,&#34;#FC8D62&#34;,&#34;#8DA0CB&#34;,&#34;#E78AC3&#34;,&#34;#A6D854&#34;,&#34;#FFD92F&#34;,&#34;#E5C494&#34;,&#34;#B3B3B3&#34;,&#34;#8DD3C7&#34;,&#34;#FFFFB3&#34;,&#34;#BEBADA&#34;,&#34;#FB8072&#34;,&#34;#80B1D3&#34;,&#34;#FDB462&#34;,&#34;#B3DE69&#34;,&#34;#FCCDE5&#34;,&#34;#D9D9D9&#34;,&#34;#BC80BD&#34;,&#34;#CCEBC5&#34;,&#34;#FFED6F&#34;],&#34;groupThickness&#34;:0.1,&#34;groupPadding&#34;:0.0349065850398866,&#34;groupnamePadding&#34;:[30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30],&#34;groupnameFontsize&#34;:18,&#34;groupedgeColor&#34;:null,&#34;chordedgeColor&#34;:&#34;#808080&#34;,&#34;categoryNames&#34;:[&#34;MAJOR&#34;,&#34;minor&#34;],&#34;categorynamePadding&#34;:100,&#34;categorynameFontsize&#34;:28,&#34;showTicks&#34;:false,&#34;tickInterval&#34;:0.1,&#34;ticklabelFontsize&#34;:10,&#34;fadeLevel&#34;:0.1,&#34;showTooltips&#34;:true,&#34;showZeroTooltips&#34;:true,&#34;tooltipNames&#34;:[&#34;BUS&#34;,&#34;CHE&#34;,&#34;HEA&#34;,&#34;HIS&#34;,&#34;MAT&#34;,&#34;MATCS&#34;,&#34;MATPH&#34;,&#34;THE&#34;,&#34;ais&#34;,&#34;artde&#34;,&#34;bus&#34;,&#34;criju&#34;,&#34;crw&#34;,&#34;csc&#34;,&#34;csd&#34;,&#34;dueng&#34;,&#34;ed10&#34;,&#34;fre&#34;,&#34;his&#34;,&#34;hpmed&#34;,&#34;matap&#34;,&#34;phi&#34;,&#34;phsci&#34;,&#34;poec&#34;,&#34;psy&#34;,&#34;relst&#34;,&#34;spa&#34;,&#34;vstdy&#34;],&#34;tooltipFontsize&#34;:12,&#34;tooltipUnit&#34;:&#34;&#34;,&#34;tooltipGroupConnector&#34;:&#34; &amp;#x25B6; &#34;,&#34;precision&#34;:&#34;null&#34;,&#34;clickAction&#34;:null,&#34;clickGroupAction&#34;:null}},&#34;evals&#34;:[],&#34;jsHooks&#34;:{&#34;render&#34;:[{&#34;code&#34;:&#34;document.getElementsByTagName(\&#34;svg\&#34;)[0].setAttribute(\&#34;viewBox\&#34;, \&#34;\&#34;)&#34;,&#34;data&#34;:null}]}}&lt;/script&gt;
&lt;p&gt;The point of this post is to illustrate how to make the above “chord diagram” using the &lt;code&gt;chorddiag&lt;/code&gt; package which leverages D3.js from R. The above visual shows students major/minor combinations who graduated in 2017 from the College of Idaho with at least one minor connected to my department: MAPS. Lower case abbreviations are minors, upper case are majors. Be sure to hover over chords, sectors, or labels to get the full D3.js effect. Also each major can be up to triple counted since students typically earn a major and 3 minors and we’re only plotting the pairings.&lt;/p&gt;
&lt;div id=&#34;getting-ready&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting Ready&lt;/h2&gt;
&lt;p&gt;To do this, we’ll need several common packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr) #pipes, mutate, select
library(tidyr) #gather
library(stringr) #string processing
library(RColorBrewer) #Colors!!&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll also need the excellent &lt;a href=&#34;https://github.com/mattflor/chorddiag&#34;&gt;chorddiag&lt;/a&gt; package, available via github:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#install if needed
devtools::install_github(&amp;quot;mattflor/chorddiag&amp;quot;)
#definitely load
library(chorddiag)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of course, we’ll also need some data, I’ll be using a data set of “anonymized” records of students major/minors degree info from the College of Idaho, which I had in another project: &lt;a href=&#34;https://github.com/jpreszler/peak-neo4j/blob/master/misc/maps-minor-historical-cleaned.csv&#34;&gt;data&lt;/a&gt;. I’ve read it in with &lt;code&gt;read.csv()&lt;/code&gt; and dropped 2 columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(mmh) %&amp;gt;% knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;Grad.Year&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Degree&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Major1&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Minor1&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Minor2&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Minor3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2012&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;BA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;BUS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MAT&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2012&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;BA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PSY&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PHY&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CRW&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSC&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2011&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;BA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MUS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;EDS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MAT&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2011&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;BA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;ACCT&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MAT&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2011&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;BS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MAT&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CHE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PHY&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2013&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;BS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MATPH&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSC&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Since students at C of I get a major and 3 minors (in place of a set general education core), we’ll have to reshape this and clean it up a bit.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reshape-and-first-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reshape and First Plot&lt;/h2&gt;
&lt;p&gt;We want to just look at major/minor pairs, so we’re going to gather the minors together, change case and trim spaces:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mmhl &amp;lt;- gather(mmh, key=mNum, value = minor, 
               -c(Grad.Year, Major1, Degree)) %&amp;gt;% 
  select(-mNum) %&amp;gt;% 
  mutate(minor = str_to_lower(str_trim(minor, side = &amp;quot;both&amp;quot;))) %&amp;gt;%
  rename(MAJOR = Major1)

head(mmhl) %&amp;gt;% knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;Grad.Year&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Degree&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;MAJOR&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;minor&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2012&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;BA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;BUS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;mat&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2012&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;BA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PSY&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;phy&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2011&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;BA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MUS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;eds&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2011&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;BA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;ACCT&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;mat&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2011&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;BS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MAT&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;che&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2013&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;BS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MATPH&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;csc&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Let’s try a plot to see how it looks. Chord diagrams need a matrix or contingency table though, so we make that and then plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;majMinTab &amp;lt;- xtabs(~MAJOR+minor, data = mmhl[mmhl$Grad.Year==2017,],
                   drop.unused.levels = TRUE)
chorddiag(majMinTab, type=&amp;quot;bipartite&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;chorddiag html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;matrix&#34;:[[0,0,0,0,0,0,0,0,2,2,0,1,0,0,2,0,0,0,0,0,5,0,1,1,4,1,3,1],[0,0,0,0,0,0,0,0,1,0,0,1,0,1,0,1,0,0,0,0,0,0,0,0,1,0,1,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1],[0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0],[0,0,0,0,0,0,0,0,1,2,0,0,1,2,0,4,0,0,1,0,0,1,0,1,3,0,1,0],[0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0],[2,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,1,0,0,1,1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[2,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,1,0,0,0,0,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[5,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[4,1,1,0,1,1,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[3,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]],&#34;options&#34;:{&#34;type&#34;:&#34;bipartite&#34;,&#34;width&#34;:null,&#34;height&#34;:null,&#34;margin&#34;:100,&#34;showGroupnames&#34;:true,&#34;groupNames&#34;:[&#34;BUS&#34;,&#34;CHE&#34;,&#34;HEA&#34;,&#34;HIS&#34;,&#34;MAT&#34;,&#34;MATCS&#34;,&#34;MATPH&#34;,&#34;THE&#34;,&#34;ais&#34;,&#34;artde&#34;,&#34;bus&#34;,&#34;criju&#34;,&#34;crw&#34;,&#34;csc&#34;,&#34;csd&#34;,&#34;dueng&#34;,&#34;ed10&#34;,&#34;fre&#34;,&#34;his&#34;,&#34;hpmed&#34;,&#34;matap&#34;,&#34;phi&#34;,&#34;phsci&#34;,&#34;poec&#34;,&#34;psy&#34;,&#34;relst&#34;,&#34;spa&#34;,&#34;vstdy&#34;],&#34;groupColors&#34;:[&#34;#FFFFFF&#34;,&#34;#F0F0F0&#34;,&#34;#D9D9D9&#34;,&#34;#BDBDBD&#34;,&#34;#969696&#34;,&#34;#737373&#34;,&#34;#525252&#34;,&#34;#252525&#34;,&#34;#1B9E77&#34;,&#34;#D95F02&#34;,&#34;#7570B3&#34;,&#34;#E7298A&#34;,&#34;#66A61E&#34;,&#34;#E6AB02&#34;,&#34;#A6761D&#34;,&#34;#666666&#34;],&#34;groupThickness&#34;:0.1,&#34;groupPadding&#34;:0.0349065850398866,&#34;groupnamePadding&#34;:[30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30],&#34;groupnameFontsize&#34;:18,&#34;groupedgeColor&#34;:null,&#34;chordedgeColor&#34;:&#34;#808080&#34;,&#34;categoryNames&#34;:[&#34;MAJOR&#34;,&#34;minor&#34;],&#34;categorynamePadding&#34;:100,&#34;categorynameFontsize&#34;:28,&#34;showTicks&#34;:true,&#34;tickInterval&#34;:0.1,&#34;ticklabelFontsize&#34;:10,&#34;fadeLevel&#34;:0.1,&#34;showTooltips&#34;:true,&#34;showZeroTooltips&#34;:true,&#34;tooltipNames&#34;:[&#34;BUS&#34;,&#34;CHE&#34;,&#34;HEA&#34;,&#34;HIS&#34;,&#34;MAT&#34;,&#34;MATCS&#34;,&#34;MATPH&#34;,&#34;THE&#34;,&#34;ais&#34;,&#34;artde&#34;,&#34;bus&#34;,&#34;criju&#34;,&#34;crw&#34;,&#34;csc&#34;,&#34;csd&#34;,&#34;dueng&#34;,&#34;ed10&#34;,&#34;fre&#34;,&#34;his&#34;,&#34;hpmed&#34;,&#34;matap&#34;,&#34;phi&#34;,&#34;phsci&#34;,&#34;poec&#34;,&#34;psy&#34;,&#34;relst&#34;,&#34;spa&#34;,&#34;vstdy&#34;],&#34;tooltipFontsize&#34;:12,&#34;tooltipUnit&#34;:&#34;&#34;,&#34;tooltipGroupConnector&#34;:&#34; &amp;#x25B6; &#34;,&#34;precision&#34;:&#34;null&#34;,&#34;clickAction&#34;:null,&#34;clickGroupAction&#34;:null}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;The &lt;code&gt;type=&#34;bipartite&#34;&lt;/code&gt; allows us to pass a non-square matrix to &lt;code&gt;chorddiag&lt;/code&gt; since we only need links between two sets of groups (majors and minors). This isn’t bad, but the default color options only allow about 16 groups (8 from the &lt;code&gt;dark2&lt;/code&gt; palette and 8 form the &lt;code&gt;grey2&lt;/code&gt; palette). Also the tick marks seem unnecessary.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;improving-colors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Improving Colors&lt;/h2&gt;
&lt;p&gt;Removing the tick marks is easy (set option &lt;code&gt;showTicks=FALSE&lt;/code&gt;), but coming up with lots of colors is a bit harder. The table has dimensions 8, 20, so we would 28 colors, and this isn’t even all possible combinations that C of I students have obtained!&lt;/p&gt;
&lt;p&gt;To get a large list of relatively distinct colors, we can do this by squishing the “qualitative” colors palettes from color brewer together. This gives 74 colors with only a little repetition:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qual_cols = brewer.pal.info[brewer.pal.info$category == &amp;#39;qual&amp;#39;,]
many_colors = unlist(mapply(brewer.pal, qual_cols$maxcolors, rownames(qual_cols)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now &lt;code&gt;many_colors&lt;/code&gt; has a large color palette for us to make our plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;chorddiag(
  xtabs(~MAJOR+minor, data = mmhl[mmhl$Grad.Year == 2017,],
        drop.unused.levels = TRUE),
  showTicks = FALSE, groupColors = many_colors, type = &amp;quot;bipartite&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-3&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;chorddiag html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-3&#34;&gt;{&#34;x&#34;:{&#34;matrix&#34;:[[0,0,0,0,0,0,0,0,2,2,0,1,0,0,2,0,0,0,0,0,5,0,1,1,4,1,3,1],[0,0,0,0,0,0,0,0,1,0,0,1,0,1,0,1,0,0,0,0,0,0,0,0,1,0,1,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1],[0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0],[0,0,0,0,0,0,0,0,1,2,0,0,1,2,0,4,0,0,1,0,0,1,0,1,3,0,1,0],[0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0],[2,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,1,0,0,1,1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[2,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,1,0,0,0,0,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[5,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[4,1,1,0,1,1,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[3,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]],&#34;options&#34;:{&#34;type&#34;:&#34;bipartite&#34;,&#34;width&#34;:null,&#34;height&#34;:null,&#34;margin&#34;:100,&#34;showGroupnames&#34;:true,&#34;groupNames&#34;:[&#34;BUS&#34;,&#34;CHE&#34;,&#34;HEA&#34;,&#34;HIS&#34;,&#34;MAT&#34;,&#34;MATCS&#34;,&#34;MATPH&#34;,&#34;THE&#34;,&#34;ais&#34;,&#34;artde&#34;,&#34;bus&#34;,&#34;criju&#34;,&#34;crw&#34;,&#34;csc&#34;,&#34;csd&#34;,&#34;dueng&#34;,&#34;ed10&#34;,&#34;fre&#34;,&#34;his&#34;,&#34;hpmed&#34;,&#34;matap&#34;,&#34;phi&#34;,&#34;phsci&#34;,&#34;poec&#34;,&#34;psy&#34;,&#34;relst&#34;,&#34;spa&#34;,&#34;vstdy&#34;],&#34;groupColors&#34;:[&#34;#7FC97F&#34;,&#34;#BEAED4&#34;,&#34;#FDC086&#34;,&#34;#FFFF99&#34;,&#34;#386CB0&#34;,&#34;#F0027F&#34;,&#34;#BF5B17&#34;,&#34;#666666&#34;,&#34;#1B9E77&#34;,&#34;#D95F02&#34;,&#34;#7570B3&#34;,&#34;#E7298A&#34;,&#34;#66A61E&#34;,&#34;#E6AB02&#34;,&#34;#A6761D&#34;,&#34;#666666&#34;,&#34;#A6CEE3&#34;,&#34;#1F78B4&#34;,&#34;#B2DF8A&#34;,&#34;#33A02C&#34;,&#34;#FB9A99&#34;,&#34;#E31A1C&#34;,&#34;#FDBF6F&#34;,&#34;#FF7F00&#34;,&#34;#CAB2D6&#34;,&#34;#6A3D9A&#34;,&#34;#FFFF99&#34;,&#34;#B15928&#34;,&#34;#FBB4AE&#34;,&#34;#B3CDE3&#34;,&#34;#CCEBC5&#34;,&#34;#DECBE4&#34;,&#34;#FED9A6&#34;,&#34;#FFFFCC&#34;,&#34;#E5D8BD&#34;,&#34;#FDDAEC&#34;,&#34;#F2F2F2&#34;,&#34;#B3E2CD&#34;,&#34;#FDCDAC&#34;,&#34;#CBD5E8&#34;,&#34;#F4CAE4&#34;,&#34;#E6F5C9&#34;,&#34;#FFF2AE&#34;,&#34;#F1E2CC&#34;,&#34;#CCCCCC&#34;,&#34;#E41A1C&#34;,&#34;#377EB8&#34;,&#34;#4DAF4A&#34;,&#34;#984EA3&#34;,&#34;#FF7F00&#34;,&#34;#FFFF33&#34;,&#34;#A65628&#34;,&#34;#F781BF&#34;,&#34;#999999&#34;,&#34;#66C2A5&#34;,&#34;#FC8D62&#34;,&#34;#8DA0CB&#34;,&#34;#E78AC3&#34;,&#34;#A6D854&#34;,&#34;#FFD92F&#34;,&#34;#E5C494&#34;,&#34;#B3B3B3&#34;,&#34;#8DD3C7&#34;,&#34;#FFFFB3&#34;,&#34;#BEBADA&#34;,&#34;#FB8072&#34;,&#34;#80B1D3&#34;,&#34;#FDB462&#34;,&#34;#B3DE69&#34;,&#34;#FCCDE5&#34;,&#34;#D9D9D9&#34;,&#34;#BC80BD&#34;,&#34;#CCEBC5&#34;,&#34;#FFED6F&#34;],&#34;groupThickness&#34;:0.1,&#34;groupPadding&#34;:0.0349065850398866,&#34;groupnamePadding&#34;:[30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30],&#34;groupnameFontsize&#34;:18,&#34;groupedgeColor&#34;:null,&#34;chordedgeColor&#34;:&#34;#808080&#34;,&#34;categoryNames&#34;:[&#34;MAJOR&#34;,&#34;minor&#34;],&#34;categorynamePadding&#34;:100,&#34;categorynameFontsize&#34;:28,&#34;showTicks&#34;:false,&#34;tickInterval&#34;:0.1,&#34;ticklabelFontsize&#34;:10,&#34;fadeLevel&#34;:0.1,&#34;showTooltips&#34;:true,&#34;showZeroTooltips&#34;:true,&#34;tooltipNames&#34;:[&#34;BUS&#34;,&#34;CHE&#34;,&#34;HEA&#34;,&#34;HIS&#34;,&#34;MAT&#34;,&#34;MATCS&#34;,&#34;MATPH&#34;,&#34;THE&#34;,&#34;ais&#34;,&#34;artde&#34;,&#34;bus&#34;,&#34;criju&#34;,&#34;crw&#34;,&#34;csc&#34;,&#34;csd&#34;,&#34;dueng&#34;,&#34;ed10&#34;,&#34;fre&#34;,&#34;his&#34;,&#34;hpmed&#34;,&#34;matap&#34;,&#34;phi&#34;,&#34;phsci&#34;,&#34;poec&#34;,&#34;psy&#34;,&#34;relst&#34;,&#34;spa&#34;,&#34;vstdy&#34;],&#34;tooltipFontsize&#34;:12,&#34;tooltipUnit&#34;:&#34;&#34;,&#34;tooltipGroupConnector&#34;:&#34; &amp;#x25B6; &#34;,&#34;precision&#34;:&#34;null&#34;,&#34;clickAction&#34;:null,&#34;clickGroupAction&#34;:null}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;I think it’s interesting to see the change over time, here’s the same plot using 2016 data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;chorddiag(
  xtabs(~MAJOR+minor, data = mmhl[mmhl$Grad.Year == 2016,],
        drop.unused.levels = TRUE),
  showTicks = FALSE, groupColors = many_colors, type = &amp;quot;bipartite&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-4&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;chorddiag html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-4&#34;&gt;{&#34;x&#34;:{&#34;matrix&#34;:[[0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0],[0,0,0,0,0,0,0,0,0,0,2,1,0,0,0,0,3,0,0,1,0,1,0,3,0,0,0,0,0,4,0,2,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,5,0,2,1,0,1,0,0,0,1,0,1,0,1,5,1,0,0,1],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,1,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0],[1,0,2,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,3,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,3,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[1,0,4,0,0,0,5,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]],&#34;options&#34;:{&#34;type&#34;:&#34;bipartite&#34;,&#34;width&#34;:null,&#34;height&#34;:null,&#34;margin&#34;:100,&#34;showGroupnames&#34;:true,&#34;groupNames&#34;:[&#34;ACCT&#34;,&#34;ATH&#34;,&#34;BUS&#34;,&#34;HIS&#34;,&#34;IPE&#34;,&#34;MATCS&#34;,&#34;MATPH&#34;,&#34;MUS&#34;,&#34;PSY&#34;,&#34;THE&#34;,&#34;artde&#34;,&#34;ath&#34;,&#34;brits&#34;,&#34;bus&#34;,&#34;chean&#34;,&#34;csc&#34;,&#34;csd&#34;,&#34;dueng&#34;,&#34;elit&#34;,&#34;enhum&#34;,&#34;fre&#34;,&#34;his&#34;,&#34;mat&#34;,&#34;matap&#34;,&#34;mus&#34;,&#34;muspe&#34;,&#34;phi&#34;,&#34;phsci&#34;,&#34;poec&#34;,&#34;psy&#34;,&#34;relst&#34;,&#34;spa&#34;,&#34;sperh&#34;,&#34;vstdy&#34;],&#34;groupColors&#34;:[&#34;#7FC97F&#34;,&#34;#BEAED4&#34;,&#34;#FDC086&#34;,&#34;#FFFF99&#34;,&#34;#386CB0&#34;,&#34;#F0027F&#34;,&#34;#BF5B17&#34;,&#34;#666666&#34;,&#34;#1B9E77&#34;,&#34;#D95F02&#34;,&#34;#7570B3&#34;,&#34;#E7298A&#34;,&#34;#66A61E&#34;,&#34;#E6AB02&#34;,&#34;#A6761D&#34;,&#34;#666666&#34;,&#34;#A6CEE3&#34;,&#34;#1F78B4&#34;,&#34;#B2DF8A&#34;,&#34;#33A02C&#34;,&#34;#FB9A99&#34;,&#34;#E31A1C&#34;,&#34;#FDBF6F&#34;,&#34;#FF7F00&#34;,&#34;#CAB2D6&#34;,&#34;#6A3D9A&#34;,&#34;#FFFF99&#34;,&#34;#B15928&#34;,&#34;#FBB4AE&#34;,&#34;#B3CDE3&#34;,&#34;#CCEBC5&#34;,&#34;#DECBE4&#34;,&#34;#FED9A6&#34;,&#34;#FFFFCC&#34;,&#34;#E5D8BD&#34;,&#34;#FDDAEC&#34;,&#34;#F2F2F2&#34;,&#34;#B3E2CD&#34;,&#34;#FDCDAC&#34;,&#34;#CBD5E8&#34;,&#34;#F4CAE4&#34;,&#34;#E6F5C9&#34;,&#34;#FFF2AE&#34;,&#34;#F1E2CC&#34;,&#34;#CCCCCC&#34;,&#34;#E41A1C&#34;,&#34;#377EB8&#34;,&#34;#4DAF4A&#34;,&#34;#984EA3&#34;,&#34;#FF7F00&#34;,&#34;#FFFF33&#34;,&#34;#A65628&#34;,&#34;#F781BF&#34;,&#34;#999999&#34;,&#34;#66C2A5&#34;,&#34;#FC8D62&#34;,&#34;#8DA0CB&#34;,&#34;#E78AC3&#34;,&#34;#A6D854&#34;,&#34;#FFD92F&#34;,&#34;#E5C494&#34;,&#34;#B3B3B3&#34;,&#34;#8DD3C7&#34;,&#34;#FFFFB3&#34;,&#34;#BEBADA&#34;,&#34;#FB8072&#34;,&#34;#80B1D3&#34;,&#34;#FDB462&#34;,&#34;#B3DE69&#34;,&#34;#FCCDE5&#34;,&#34;#D9D9D9&#34;,&#34;#BC80BD&#34;,&#34;#CCEBC5&#34;,&#34;#FFED6F&#34;],&#34;groupThickness&#34;:0.1,&#34;groupPadding&#34;:0.0349065850398866,&#34;groupnamePadding&#34;:[30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30],&#34;groupnameFontsize&#34;:18,&#34;groupedgeColor&#34;:null,&#34;chordedgeColor&#34;:&#34;#808080&#34;,&#34;categoryNames&#34;:[&#34;MAJOR&#34;,&#34;minor&#34;],&#34;categorynamePadding&#34;:100,&#34;categorynameFontsize&#34;:28,&#34;showTicks&#34;:false,&#34;tickInterval&#34;:0.1,&#34;ticklabelFontsize&#34;:10,&#34;fadeLevel&#34;:0.1,&#34;showTooltips&#34;:true,&#34;showZeroTooltips&#34;:true,&#34;tooltipNames&#34;:[&#34;ACCT&#34;,&#34;ATH&#34;,&#34;BUS&#34;,&#34;HIS&#34;,&#34;IPE&#34;,&#34;MATCS&#34;,&#34;MATPH&#34;,&#34;MUS&#34;,&#34;PSY&#34;,&#34;THE&#34;,&#34;artde&#34;,&#34;ath&#34;,&#34;brits&#34;,&#34;bus&#34;,&#34;chean&#34;,&#34;csc&#34;,&#34;csd&#34;,&#34;dueng&#34;,&#34;elit&#34;,&#34;enhum&#34;,&#34;fre&#34;,&#34;his&#34;,&#34;mat&#34;,&#34;matap&#34;,&#34;mus&#34;,&#34;muspe&#34;,&#34;phi&#34;,&#34;phsci&#34;,&#34;poec&#34;,&#34;psy&#34;,&#34;relst&#34;,&#34;spa&#34;,&#34;sperh&#34;,&#34;vstdy&#34;],&#34;tooltipFontsize&#34;:12,&#34;tooltipUnit&#34;:&#34;&#34;,&#34;tooltipGroupConnector&#34;:&#34; &amp;#x25B6; &#34;,&#34;precision&#34;:&#34;null&#34;,&#34;clickAction&#34;:null,&#34;clickGroupAction&#34;:null}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Perhaps we could animate over the years… But I should finish my second year evaluation first.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Lesser Known Verbs: top_n</title>
      <link>/post/2018-07-30-top_n/</link>
      <pubDate>Mon, 30 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-07-30-top_n/</guid>
      <description>


&lt;p&gt;I’ve been using R since 2006. That predates RStudio and the &lt;code&gt;tidyverse&lt;/code&gt;. I remember the struggle of keeping track of the variants of &lt;code&gt;apply&lt;/code&gt; and often fiddling with them to get code to work.&lt;/p&gt;
&lt;p&gt;Then came &lt;code&gt;plyr&lt;/code&gt; and the &lt;code&gt;dplyr&lt;/code&gt; and my life has never been the same. The major verbs of &lt;code&gt;dplyr&lt;/code&gt; include &lt;code&gt;select&lt;/code&gt;, &lt;code&gt;filter&lt;/code&gt;, &lt;code&gt;mutate&lt;/code&gt;, &lt;code&gt;group_by&lt;/code&gt;, &lt;code&gt;summarise&lt;/code&gt;, and &lt;code&gt;arrange&lt;/code&gt;; and if you are doing data analysis in R then you should be fluent in them. These are far from an exhaustive list of &lt;code&gt;dplyr&lt;/code&gt;’s verbs though, and I recently discovered one that solves a problem I’ve encountered a few times: &lt;code&gt;top_n&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;the-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Problem&lt;/h2&gt;
&lt;p&gt;Recently I was analyzing some student data on an exam that had students from multiple section and their performance on on the exam. Since students could take the course multiple times, I only wanted to include the row with the best score. Clearly, grouping by student was needed, and &lt;code&gt;summarise&lt;/code&gt; can get the max of one column but it won’t leave the other columns unchanged (it would apply and aggreation function to them). Instead &lt;code&gt;top_n&lt;/code&gt; saves the day.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An Example&lt;/h2&gt;
&lt;p&gt;To avoid voilating FERPA, I’ll manufacture some data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;student_id &amp;lt;- c(1:6, seq(1,10, by=2))
section &amp;lt;- sample(c(&amp;quot;A&amp;quot;,&amp;quot;B&amp;quot;,&amp;quot;C&amp;quot;), length(student_id), replace=TRUE)
score &amp;lt;- round(rnorm(11, 25, 5), 2)
question1 &amp;lt;- round(runif(1,0,5),0)
question2 &amp;lt;- round(runif(1,0,5),0)
question3 &amp;lt;- round(runif(1,0,5),0)
question4 &amp;lt;- round(runif(1,0,5),0)
question5 &amp;lt;- round(runif(1,0,5),0)

dat &amp;lt;- data.frame(student_id, section, score, question1, question2, question3, question4, question5)

kable(dat)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;student_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;section&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;score&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;question1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;question2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;question3&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;question4&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;question5&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28.17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;29.44&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;21.84&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;30.19&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;35.93&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;23.20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;27.40&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;35.43&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;30.70&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;22.07&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;23.03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So, we have a small dataset with repeated &lt;code&gt;student_id&lt;/code&gt; and we want to have no repeated students and keep the row (including question data) for the maximum score of any repeated student.&lt;/p&gt;
&lt;p&gt;Here’s the &lt;code&gt;dplyr&lt;/code&gt; one-liner:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;% group_by(student_id) %&amp;gt;%
  top_n(1, score) %&amp;gt;% kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;student_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;section&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;score&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;question1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;question2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;question3&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;question4&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;question5&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28.17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;29.44&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;30.19&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;35.93&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;23.20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;35.43&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;22.07&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;23.03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;As mentioned in &lt;code&gt;top_n&lt;/code&gt; documentation, it’s just a wrapper for &lt;code&gt;filter&lt;/code&gt; and &lt;code&gt;min_rank&lt;/code&gt;, so like most (all?) of &lt;code&gt;dplyr&lt;/code&gt; it’s possible to avoid, but once you know about it why would you?&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Catalog Evolution</title>
      <link>/post/2018-07-28-cat-evolve/</link>
      <pubDate>Sat, 28 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-07-28-cat-evolve/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/pymjs/pym.v1.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/widgetframe-binding/widgetframe.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Recently I’ve posted about the College of Idaho’s &lt;a href=&#34;/post/2018-05-05-counting-class.html&#34;&gt;2017-2018&lt;/a&gt; and &lt;a href=&#34;/post/2018-07-24-recount-class.html&#34;&gt;2018-2019&lt;/a&gt; course distribution. The second post showed how easy it was to reproduce everything, which was good because a colleague recently asked about the total number of courses in 2016-2017 for a funded grant related to curriculum review. These total numbers of courses of courses made me wonder about how the catalog has evolved over the last few years? Which subjects have seen the most change?&lt;/p&gt;
&lt;div id=&#34;getting-the-classes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting the Classes&lt;/h2&gt;
&lt;p&gt;I’m not going to reproduce all the scrapping code, refer to the previous posts linked above. The array indices changed for the second post, as well as the extension on the base url need slight adjustments. I’ve done that (R scripts are actually &lt;a href=&#34;https://github.com/jpreszler/peak-neo4j/scrape/&#34;&gt;here&lt;/a&gt;) and saved the class lists, so we just need to load the csv’s and tack on a catalog year for latter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cl15 &amp;lt;- read.csv(&amp;quot;../../static/files/class-list-2015-2016.csv&amp;quot;, 
                 header=TRUE) %&amp;gt;% 
  mutate(catYear = 2015) %&amp;gt;% select(-url)
cl16 &amp;lt;- read.csv(&amp;quot;../../static/files/class-list-2016-2017.csv&amp;quot;,
                 header=TRUE) %&amp;gt;% 
  mutate(catYear = 2016) %&amp;gt;% select(-url)
cl17 &amp;lt;- read.csv(&amp;quot;../../static/files/class-list-2017-2018.csv&amp;quot;, 
                 header=TRUE) %&amp;gt;% 
  mutate(catYear = 2017) %&amp;gt;% select(-url)
cl18 &amp;lt;- read.csv(&amp;quot;../../static/files/class-list-2018-2019.csv&amp;quot;, 
                 header=TRUE) %&amp;gt;% 
  mutate(catYear = 2018) %&amp;gt;% select(-url)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s glue these together and do some basic counting:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cl &amp;lt;- rbind.data.frame(cl15,cl16,cl17,cl18) 
cl %&amp;gt;% group_by(catYear) %&amp;gt;% summarise(classes = n(), subjects = n_distinct(sub)) %&amp;gt;% kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;catYear&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;classes&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;subjects&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2015&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;976&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;42&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2016&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;43&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2017&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1034&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;43&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2018&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1026&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;44&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So the number of subjects has barely changed, but there was significant growth in the course count for two consecutive years, and then recent increase was lost - possibly due to a perception of a growing, unwieldy curriculum. Let’s look into some of the changes that took place.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-first-rise&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The First Rise&lt;/h2&gt;
&lt;p&gt;From 2015 to 2016 the number of subjects increased by one. The difference is: ECN. So did economics courses account for the additional 37? To check, we’ll have some fun with &lt;code&gt;dplyr&lt;/code&gt; anti-joins.&lt;/p&gt;
&lt;div id=&#34;deletions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Deletions&lt;/h3&gt;
&lt;p&gt;First, let’s look into courses in the 2015-2016 catalog that are &lt;strong&gt;not in&lt;/strong&gt; the 2016-2017 catalog. I’m going to join along the subject and number, so a change in the name won’t show up (before writing this, I did include name, and there were over 200 courses in the following table!).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dropped15 &amp;lt;-anti_join(cl15, cl16, by=c(&amp;quot;sub&amp;quot;,&amp;quot;number&amp;quot;)) 
dropped15 %&amp;gt;% 
  datatable(rownames = FALSE, filter = &amp;quot;top&amp;quot;, 
            options = list(pageLength=5)) %&amp;gt;%
  frameWidget(height = 550, width = &amp;quot;100%&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:100%;height:550px;&#34; class=&#34;widgetframe html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;url&#34;:&#34;/post/2018-07-28-cat-evolve_files/figure-html//widgets/widget_unnamed-chunk-3.html&#34;,&#34;options&#34;:{&#34;xdomain&#34;:&#34;*&#34;,&#34;allowfullscreen&#34;:false,&#34;lazyload&#34;:false}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Considering the increase in courses, that’s a lot that got dropped. Let’s group things on the subject level:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dropped15 %&amp;gt;% group_by(sub) %&amp;gt;% 
  summarise(course.count = n()) %&amp;gt;%
  arrange(desc(course.count)) %&amp;gt;% 
  kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;sub&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;course.count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;POE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;MUS&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;HIS&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ATH&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ART&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PSY&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;CSC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;MAT&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;BIO&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;HHP&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;IND&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SPE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;THE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;With the addition of economics, it’s not surprising that POE (POlitics and Economics) dropped a lot of courses.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;additions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Additions&lt;/h3&gt;
&lt;p&gt;Now we’ll look at what was added.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;added16 &amp;lt;- anti_join(cl16, cl15, by=c(&amp;quot;sub&amp;quot;,&amp;quot;number&amp;quot;)) 
added16 %&amp;gt;% datatable(rownames = FALSE, filter = &amp;quot;top&amp;quot;, 
                      options = list(pageLength=5)) %&amp;gt;%
  frameWidget(height = 550, width = &amp;quot;100%&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:100%;height:550px;&#34; class=&#34;widgetframe html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;url&#34;:&#34;/post/2018-07-28-cat-evolve_files/figure-html//widgets/widget_unnamed-chunk-5.html&#34;,&#34;options&#34;:{&#34;xdomain&#34;:&#34;*&#34;,&#34;allowfullscreen&#34;:false,&#34;lazyload&#34;:false}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;added16 %&amp;gt;% group_by(sub) %&amp;gt;% 
  summarise(course.count = n()) %&amp;gt;%
  arrange(desc(course.count)) %&amp;gt;% 
  kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;sub&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;course.count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;HIS&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;MUS&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;POE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PSY&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SPE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ENV&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;THE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;BIO&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;GEO&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;HHPA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;IND&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ART&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;BUS&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;CHE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ECN&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;HHP&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;HSC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;JOURN&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PHY&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;CSC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ENG&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FRE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SOC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SPA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So the increase is from (a) almost twice as many subjects adding courses as removing them and (b) additions being greater than deletions on a subject level. History, Music, and POE had the most additions AND deletions, and additions outnumber deletions.&lt;/p&gt;
&lt;p&gt;Recall that we had a new subject: Economics. Notice that this only accounted for 2 new courses so POE dropped 10 and POE+ECN added 10. I find it fitting that “economics” has a zero-sum in this instance.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-second-rise&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Second Rise&lt;/h2&gt;
&lt;p&gt;We’re going to get a bit repetitive in terms of code, but I expect the results to be a bit different.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dropped16 &amp;lt;-anti_join(cl16, cl17, by=c(&amp;quot;sub&amp;quot;,&amp;quot;number&amp;quot;)) 
dropped16 %&amp;gt;% 
  datatable(rownames = FALSE, filter = &amp;quot;top&amp;quot;, 
            options = list(pageLength=5)) %&amp;gt;%
  frameWidget(height = 550, width = &amp;quot;100%&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-3&#34; style=&#34;width:100%;height:550px;&#34; class=&#34;widgetframe html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-3&#34;&gt;{&#34;x&#34;:{&#34;url&#34;:&#34;/post/2018-07-28-cat-evolve_files/figure-html//widgets/widget_unnamed-chunk-6.html&#34;,&#34;options&#34;:{&#34;xdomain&#34;:&#34;*&#34;,&#34;allowfullscreen&#34;:false,&#34;lazyload&#34;:false}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Only 18 courses dropped. This means there wasn’t as much movement in the curriculum, but which subjects were most active?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dropped16 %&amp;gt;% group_by(sub) %&amp;gt;% 
  summarise(course.count = n()) %&amp;gt;%
  arrange(desc(course.count)) %&amp;gt;% 
  kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;sub&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;course.count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;HHPA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;MUS&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SPA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ECN&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;POE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Notice Music is joint top of the list, is this related to faculty turn-over (a major driver of curricula change at small colleges) or do they just like tinkering with the catalog? Did they add as many as the previous year though?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;added17 &amp;lt;- anti_join(cl17, cl16, by=c(&amp;quot;sub&amp;quot;,&amp;quot;number&amp;quot;)) 
added17 %&amp;gt;% datatable(rownames = FALSE, filter = &amp;quot;top&amp;quot;, 
                      options = list(pageLength=5)) %&amp;gt;%
  frameWidget(height = 550, width = &amp;quot;100%&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-4&#34; style=&#34;width:100%;height:550px;&#34; class=&#34;widgetframe html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-4&#34;&gt;{&#34;x&#34;:{&#34;url&#34;:&#34;/post/2018-07-28-cat-evolve_files/figure-html//widgets/widget_unnamed-chunk-8.html&#34;,&#34;options&#34;:{&#34;xdomain&#34;:&#34;*&#34;,&#34;allowfullscreen&#34;:false,&#34;lazyload&#34;:false}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;added17 %&amp;gt;% group_by(sub) %&amp;gt;% 
  summarise(course.count = n()) %&amp;gt;%
  arrange(desc(course.count)) %&amp;gt;% 
  kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;sub&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;course.count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SPA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;BUS&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;CHE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;MUS&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SOC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;HHP&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;HIS&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;IND&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;LAS&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;POE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ART&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ENV&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PSY&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So again, more subjects added courses than removed but the addition counts by subject are generally much lower.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-decline&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Decline&lt;/h2&gt;
&lt;p&gt;Now we can look into the recent decline.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dropped17 &amp;lt;-anti_join(cl17, cl18, by=c(&amp;quot;sub&amp;quot;,&amp;quot;number&amp;quot;)) 
dropped17 %&amp;gt;% 
  datatable(rownames = FALSE, filter = &amp;quot;top&amp;quot;, 
            options = list(pageLength=5)) %&amp;gt;%
  frameWidget(height = 550, width = &amp;quot;100%&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-5&#34; style=&#34;width:100%;height:550px;&#34; class=&#34;widgetframe html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-5&#34;&gt;{&#34;x&#34;:{&#34;url&#34;:&#34;/post/2018-07-28-cat-evolve_files/figure-html//widgets/widget_unnamed-chunk-9.html&#34;,&#34;options&#34;:{&#34;xdomain&#34;:&#34;*&#34;,&#34;allowfullscreen&#34;:false,&#34;lazyload&#34;:false}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;For a net decline, the dropped number had to be big, and it’s basically the last two drops combined.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dropped17 %&amp;gt;% group_by(sub) %&amp;gt;% 
  summarise(course.count = n()) %&amp;gt;%
  arrange(desc(course.count)) %&amp;gt;% 
  kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;sub&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;course.count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ART&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;33&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PSY&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;HIS&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;MAT&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;CHE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;MUS&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SPA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;BUS&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;CSC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;HHP&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;THE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Music almost held steady for once. The two subjects in double digits all underwent major curricular changes: PSY restructured things based in part on new medical school requirements, and ART overhauled their program to both shift what types of faculty taught intro courses and to provide course offerings that will help majors go to graduate school. The point is that there’s no surprise (to anyone who was at faculty meetings that last year) that these programs would be at the top of this list.&lt;/p&gt;
&lt;p&gt;But we didn’t lose 65 courses, so what was added?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;added18 &amp;lt;- anti_join(cl18, cl17, by=c(&amp;quot;sub&amp;quot;,&amp;quot;number&amp;quot;)) 
added18 %&amp;gt;% datatable(rownames = FALSE, filter = &amp;quot;top&amp;quot;, 
                      options = list(pageLength=5)) %&amp;gt;%
  frameWidget(height = 550, width = &amp;quot;100%&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-6&#34; style=&#34;width:100%;height:550px;&#34; class=&#34;widgetframe html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-6&#34;&gt;{&#34;x&#34;:{&#34;url&#34;:&#34;/post/2018-07-28-cat-evolve_files/figure-html//widgets/widget_unnamed-chunk-11.html&#34;,&#34;options&#34;:{&#34;xdomain&#34;:&#34;*&#34;,&#34;allowfullscreen&#34;:false,&#34;lazyload&#34;:false}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;added18 %&amp;gt;% group_by(sub) %&amp;gt;% 
  summarise(course.count = n()) %&amp;gt;%
  arrange(desc(course.count)) %&amp;gt;% 
  kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;sub&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;course.count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ART&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;22&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ARH&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;CHE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;HIS&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;MUS&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PSY&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ACC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;BUS&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;THE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;CSC&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ECN&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ENG&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;IND&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;MAT&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Art’s curriculum revision meant adding courses and a new subject (Art History), but with a small net lose. Other programs were very small additions, and I’ll admit I accounted for 2 (CSC and MAT).&lt;/p&gt;
&lt;p&gt;I’ll also note that MFL changed their department code to WLC, but not the prefix of courses so they aren’t showing up.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The counts and lists of courses added and deleted are one thing, but there’s a deeper story here which I don’t know and this data alone can’t really tell. A college’s curriculum is driven by the faculty, sometimes encouraged or discouraged by the administration (deans and presidents especially). Over this time, the College of Idaho has had 2 deans and at least 4 presidents (that I’ve been around for). Some have encouraged growth and expansion, but more recently contraction, of the curriculum. Combining this with the natural turn-over in faculty, shifting specialties, and evolving priorities of what current students need is the real story here. While I would love to tell this story (like &lt;a href=&#34;https://youtu.be/usdJgEwMinM&#34;&gt;Hans Gosling’s Gap Minder talk&lt;/a&gt;), it would require a depth of institutional knowledge I don’t yet possess and would make this post far too long!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
