<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jason I. Preszler on Jason I. Preszler</title>
    <link>/</link>
    <description>Recent content in Jason I. Preszler on Jason I. Preszler</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Jason Preszler</copyright>
    <lastBuildDate>Thu, 28 Dec 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>SQL in RMarkdown!</title>
      <link>/post/2018-04-01-sql-rmd/</link>
      <pubDate>Sun, 01 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-04-01-sql-rmd/</guid>
      <description>&lt;p&gt;This semester I’m teaching &lt;em&gt;Applied Databases&lt;/em&gt; for the first time and have been struggling with some notes and handouts for students; as well as simple, easy to use database interfaces that work well across platforms. I love RMarkdown, and today realized that knitr has an SQL code engine!&lt;/p&gt;
&lt;div id=&#34;basic-syntax&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Basic Syntax&lt;/h2&gt;
&lt;p&gt;I often give handouts on SQL statements as we learn about them, so I need a nice way to show commands. To do this, we’ll set up a dummy database connection.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(DBI)
db &amp;lt;- dbConnect(RSQLite::SQLite(), dbname = &amp;quot;:memory:&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This creates an empty, in memory, SQLite database. Clearly you need both the R packages DBI and RSQLite installed. Now we can have an SQL code chunk (must give ‘connection=db’, and ‘eval=FALSE’) to demonstrate commands like:&lt;/p&gt;
&lt;pre class=&#34;sql&#34;&gt;&lt;code&gt;SELECT var1, COUNT(DISTINCT(var2)) FROM tab1 GROUP BY var1;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;real-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Real Example&lt;/h2&gt;
&lt;p&gt;Or, we can grab a real SQLite database (such as &lt;a href=&#34;/files/sf-salary.sqlite&#34;&gt;sf-salary&lt;/a&gt; from Kaggle) and do some querying.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sf_salary &amp;lt;- dbConnect(RSQLite::SQLite(),
                    dbname=&amp;quot;../../static/files/sf-salary.sqlite&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Obviously, within R we can use the dbGetQuery() and dbSendQuery() commands from DBI, or if you’re more comfortable with SQL, just use an SQL code chunk:&lt;/p&gt;
&lt;pre class=&#34;sql&#34;&gt;&lt;code&gt;SELECT JobTitle, AVG(BasePay) FROM Salaries 
  GROUP BY JobTitle ORDER BY AVG(BasePay) DESC LIMIT 8;&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;knitsql-table&#34;&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 1: &lt;/span&gt;8 records&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;JobTitle&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AVG(BasePay)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Chief of Police&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;309767.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Chief, Fire Department&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;304232.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Gen Mgr, Public Trnsp Dept&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;297769.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;CHIEF OF DEPARTMENT, (FIRE DEPARTMENT)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;285262.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Dep Dir for Investments, Ret&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;276153.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Mayor&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;275852.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Adm, SFGH Medical Center&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;265218.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;EXECUTIVE CONTRACT EMPLOYEE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;264452.5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-real-reason-for-excitement&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Real Reason For Excitement&lt;/h2&gt;
&lt;p&gt;Beyond a love of doing everything in R, I’m really excited because we’ve been using SQLiteStudio as a nice GUI for SQLite databases. However, my students with Macs have to use an old version due to an installation bug. This old version makes it impossible for them to use some SQL such as the DISTINCT command. Obviously this is a problem, but perhaps RMarkdown is the answer!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Running Tacoma: Maps</title>
      <link>/post/2018-3-11-run-tacoma1/</link>
      <pubDate>Sun, 11 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-3-11-run-tacoma1/</guid>
      <description>&lt;p&gt;When I lived in Tacoma, I was running quite a bit. Since I moved away my training has become much more irregular, but I thought it would be interesting to take the Tacoma data from my current Garmin Forerunner 220 a take a look.&lt;/p&gt;
&lt;div id=&#34;data-prep&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data Prep&lt;/h1&gt;
&lt;p&gt;The Garmin stores data in .fit format, but gpsbabel can translate to a nicely structured GPX file, which is what I’ll start with here. The XML package in R has some nice features to easily parse xml files (GPX is GPS data in a special XML schema). First, I have a function that turns a single run into a dataframe, then I can glue the dataframes together.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(XML)
library(lubridate)

getRunDF &amp;lt;- function(filename) {
  pfile &amp;lt;- htmlTreeParse(filename,
                      error = function (...) {}, useInternalNodes = T)
# Get all elevations, times and coordinates via the respective xpath
  elevations &amp;lt;- as.numeric(xpathSApply(pfile, path = &amp;quot;//trkpt/ele&amp;quot;, xmlValue))
  times &amp;lt;- xpathSApply(pfile, path = &amp;quot;//trkpt/time&amp;quot;, xmlValue)
  coords &amp;lt;- xpathSApply(pfile, path = &amp;quot;//trkpt&amp;quot;, xmlAttrs)
  speeds &amp;lt;- xpathSApply(pfile, path = &amp;quot;//trkpt/speed&amp;quot;, xmlValue)
#convert speed from meters/sec to minutes/mile and clean
  speeds &amp;lt;- 26.8224/as.numeric(speeds)
  speeds[1] &amp;lt;- 0 #first speed is 0 m/s
  speeds &amp;lt;- ifelse(speeds&amp;gt;12, mean(speeds),speeds)
  speeds &amp;lt;- ifelse(speeds&amp;lt;5.5, mean(speeds),speeds)
#convert elevation to feet from meters
  elevations &amp;lt;- elevations*3.28084
# Extract latitude and longitude from the coordinates
  lats &amp;lt;- as.numeric(coords[&amp;quot;lat&amp;quot;,])
  lons &amp;lt;- as.numeric(coords[&amp;quot;lon&amp;quot;,])
# Put everything in a dataframe and get rid of old variables
  geodf &amp;lt;- data.frame(lat = lats, lon = lons, elev = elevations, time = times, pace=speeds)
rm(list=c(&amp;quot;elevations&amp;quot;, &amp;quot;lats&amp;quot;, &amp;quot;lons&amp;quot;, &amp;quot;pfile&amp;quot;, &amp;quot;times&amp;quot;, &amp;quot;coords&amp;quot;, &amp;quot;speeds&amp;quot;))
geodf$time &amp;lt;- as.POSIXct(strptime(geodf$time, format = &amp;quot;%Y-%m-%dT%H:%M:%OS&amp;quot;))
geodf$elapsed.time &amp;lt;- difftime(geodf$time,geodf$time[1])/60
geodf$distance &amp;lt;- geodf$elapsed.time/geodf$pace
geodf$elev.offset &amp;lt;- geodf$elev - mean(geodf$elev[1:10])
geodf$elev.lag &amp;lt;- geodf$elev - lag(geodf$elev)
#geodf$pace.offset &amp;lt;- geodf$pace - lag(geodf$pace)
#geodf$total.elev.change &amp;lt;- cumsum(abs(geodf$elev.change))

return(geodf)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;running-it-together&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Running it Together&lt;/h1&gt;
&lt;p&gt;I have some index files with notes (shoe worn, location features like bridges or hills, race or not, etc.), I use that to grab the runs from Tacoma and combine each run into a single dataframe. Some of the GPX files were pulled from Garmin Connect, which I tried using briefly (it doesn’t play well with Linux, and I’d rather do my own analysis).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(tidyr)
library(ggplot2)
library(knitr)

indx &amp;lt;- read.table(&amp;quot;/home/jpreszler/garmin-220/GPX/index-data.org&amp;quot;, strip.white = TRUE, sep = &amp;quot;|&amp;quot;, header=TRUE) %&amp;gt;% select(File, Location) %&amp;gt;% filter(Location == &amp;quot;Tacoma&amp;quot;)
indxGC &amp;lt;- read.table(&amp;quot;/home/jpreszler/garmin-220/GPX/gc-index.org&amp;quot;, strip.white = TRUE, sep=&amp;quot;|&amp;quot;, header=TRUE) %&amp;gt;% select(File, Location) %&amp;gt;% filter(Location==&amp;quot;Tacoma&amp;quot;)

for(i in 1:length(indx$File)){
  run &amp;lt;- getRunDF(paste(&amp;quot;/home/jpreszler/garmin-220/GPX/&amp;quot;,indx$File[i],&amp;quot;.gpx&amp;quot;, sep=&amp;quot;&amp;quot;))
  ifelse(i==1,runs &amp;lt;- run, runs &amp;lt;- rbind.data.frame(runs,run))
}
for(i in 1:length(indxGC$File)){
  run &amp;lt;- getRunDF(paste(&amp;quot;/home/jpreszler/garmin-220/GPX/from-gc/&amp;quot;,indxGC$File[i],sep=&amp;quot;&amp;quot;))
  runs &amp;lt;- rbind.data.frame(runs,run)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This has gathered data for 52 runs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;maps&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Maps&lt;/h1&gt;
&lt;p&gt;Originally, I used OpenStreetMap to overlay the run data onto a map, but I’m not getting errors and ggmap seems to work much better. Since I’m combining lots of runs with overlapping coordincates, it’s important to set alpha fairly low unless you want a massive blob of red. First, I’ll plot the run coordinates without a map.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot the tracks without any map
ggplot(runs, aes(x=lon, y=lat))+geom_point(alpha=0.05, col=&amp;quot;red&amp;quot;)+xlab(&amp;quot;Longitude&amp;quot;)+ylab(&amp;quot;Latitude&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-3-11-run-tacoma1_files/figure-html/mapplots1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we’ll take the same map an overlay it on top of a satellite image from Google Maps via ggmap.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggmap)
map2 &amp;lt;- get_map(location = c(left=min(runs$lon), right = max(runs$lon), bottom = min(runs$lat), top = max(runs$lat)), maptype = &amp;quot;satellite&amp;quot;, zoom=12)
ggmap(map2)+geom_point(data=runs, aes(x=lon,y=lat),alpha=0.05, col=&amp;quot;red&amp;quot;, size=1)+xlab(&amp;quot;Longitude&amp;quot;)+ylab(&amp;quot;Latitude&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-3-11-run-tacoma1_files/figure-html/mapplots2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Clearly, I preferred to run towards the water rather than down into scenic South Tacoma.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Visualizing Classifier Peformance</title>
      <link>/talk/maps-312018/</link>
      <pubDate>Thu, 01 Mar 2018 08:24:06 -0700</pubDate>
      
      <guid>/talk/maps-312018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Idaho ACS Mapping</title>
      <link>/post/2018-01-27-acs-map/</link>
      <pubDate>Sat, 27 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-27-acs-map/</guid>
      <description>&lt;p&gt;Recently some diversity stats have been circulated around the College of Idaho, and as new Idahoan I wondered about the general diversity (or lack thereof) in Idaho. I remembered seeing &lt;a href=&#34;http://www.kevjohnson.org/making-maps-in-r/&#34;&gt;this post&lt;/a&gt; a while back about mapping in R, so I went to work.&lt;/p&gt;
&lt;div id=&#34;shapefiles&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Shapefiles&lt;/h1&gt;
&lt;p&gt;First, we need shapefiles for both the Idaho country boundaries and census tracts, which will give finer detail for data. These can be downloaded from the [US Census Bureau] (&lt;a href=&#34;https://www.census.gov/geo/maps-data/data/tiger-cart-boundary.html&#34; class=&#34;uri&#34;&gt;https://www.census.gov/geo/maps-data/data/tiger-cart-boundary.html&lt;/a&gt;). Under the “State-Based” files, get census tracts and country subdivisions, these will give you zip files for the state you’re interested in: &lt;a href=&#34;/files/shape/cb_2016_16_tract_500k.zip&#34;&gt;census shape zip&lt;/a&gt; and &lt;a href=&#34;/files/shape/cb_2016_cousub_500k.zip&#34;&gt;census country&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The zip files need to be extracted, and then we can load the data into R and prep it for ggplot. The &lt;em&gt;fortify&lt;/em&gt; commands produce a dataframe from the shapefile data the we can use with GGPlot2.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#packages needed:
library(ggplot2)
library(dplyr)
library(rgdal)
library(ggmap)
library(scales)

#load and prep shapefiles
tract &amp;lt;- readOGR(dsn=&amp;quot;../../static/files/shape/&amp;quot;, layer=&amp;quot;cb_2016_16_tract_500k&amp;quot;, verbose = FALSE)
tract &amp;lt;- fortify(tract, region = &amp;quot;GEOID&amp;quot;)

county &amp;lt;- readOGR(dsn=&amp;quot;../../static/files/shape/&amp;quot;, layer=&amp;quot;cb_2016_16_cousub_500k&amp;quot;, verbose = FALSE)
county &amp;lt;- fortify(county, region = &amp;quot;COUNTYFP&amp;quot;) #using GEOID gives census tract division&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’ve got our shapefile data, we need some demographic data to produce more than just empty maps.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;american-community-survey&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;American Community Survey&lt;/h1&gt;
&lt;p&gt;A great source of demographic data is the American Community Survey from the US Census Bureau. I already had the county and &lt;a href=&#34;/files/acs2015_census_tract_data.csv&#34;&gt;census tract ACS 2015 data&lt;/a&gt; from &lt;a href=&#34;http://www.kaggle.com&#34;&gt;Kaggle&lt;/a&gt;. When loading the data, change the &lt;em&gt;CensusTract&lt;/em&gt; variable to a character because of the encoding of the Kaggle version, otherwise there will be &lt;em&gt;NA&lt;/em&gt; for all of Idaho’s census tracts!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr)
IDacs2015 &amp;lt;- read_csv(&amp;quot;../../static/files/acs2015_census_tract_data.csv&amp;quot;, 
                      col_types = cols(CensusTract = col_character())) %&amp;gt;% 
  filter(State == &amp;quot;Idaho&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;mapping-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Mapping the Data&lt;/h1&gt;
&lt;p&gt;Now that we have geographic and demographic data loaded, we can combine it to produce some maps. There’s a fairly large amount of data in the ACS, for this post I’m going to focus on a few items: gender, racial diversity, and income.&lt;/p&gt;
&lt;div id=&#34;idaho-gender-map&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Idaho Gender Map&lt;/h2&gt;
&lt;p&gt;Let’s build a map of Idaho with each census tract colored according to the percent of it’s population that is men (sorry LGBTQ fans, the ACS gender data is old school binary men/women).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#remove extra data, 
IDacsM &amp;lt;- transmute(IDacs2015, id=CensusTract, pctMen = Men/TotalPop) 
MplotData &amp;lt;- left_join(tract, IDacsM)

ggplot() + 
  geom_polygon(data = MplotData, aes(x = long, y = lat, group = id,
        fill = pctMen)) +
    geom_polygon(data = county, aes(x = long, y = lat, group = id),
        fill = NA, color = &amp;quot;black&amp;quot;, size = 0.25) +
    coord_map() + scale_fill_distiller(palette = &amp;quot;Blues&amp;quot;, breaks = pretty_breaks(n = 10)) +
    guides(fill = guide_legend(reverse = TRUE)) + theme_nothing(legend = TRUE) + ggtitle(&amp;quot;Percent of Population is Male&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-27-ACS-map_files/figure-html/acsM-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice that white chuck in south Ada county (where the capitol Boise is located). Is this a data anonmaly or is something else going on? Let’s investigate a bit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;whiteSpot &amp;lt;-MplotData[MplotData$pctMen &amp;gt; .70,]
wsMap &amp;lt;- get_map(location = c(left = min(whiteSpot$long),bottom = min(whiteSpot$lat), right = max(whiteSpot$long), top = max(whiteSpot$lat)), maptype = &amp;quot;satellite&amp;quot;, zoom=11)
ggmap(wsMap)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-27-ACS-map_files/figure-html/allMen-1.png&#34; width=&#34;672&#34; /&gt; The empty expense is part of the &lt;em&gt;Morley Nelson Snake River Birds of Prey National Conservation Area&lt;/em&gt; which doesn’t have any people living in it. Let’s zoom in on the top right area of the census tract, longitude -116.2 and latitude 43.5.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wsMapZoom &amp;lt;- get_map(location = c(lon = -116.23, lat = 43.48), maptype = &amp;quot;hybrid&amp;quot;, zoom = 13)
ggmap(wsMapZoom)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-27-ACS-map_files/figure-html/allMenZoom-1.png&#34; width=&#34;672&#34; /&gt; The buildings at &lt;span class=&#34;math inline&#34;&gt;\((-116.225, 43.48)\)&lt;/span&gt; are several correctional facilities and the smaller one on the other side of Pleasant Valley road is the state Women’s correctional facility.&lt;/p&gt;
&lt;p&gt;The white chunck isn’t a data anomally. The includsion of a wildlife refuge and a mostly rural area make the census tract large. The presence of several men’s prisons dramatically skew the population.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;racial-diversity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Racial Diversity&lt;/h2&gt;
&lt;p&gt;Let’s build a similar map of the percent of the population that is white.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#remove extra data, 
IDacsR &amp;lt;- transmute(IDacs2015, id=CensusTract, pctWhite = White/100) 
RplotData &amp;lt;- left_join(tract, IDacsR)

ggplot() + geom_polygon(data = RplotData, aes(x = long, y = lat, group = id,
        fill = pctWhite)) +
    geom_polygon(data = county, aes(x = long, y = lat, group = id),
        fill = NA, color = &amp;quot;black&amp;quot;, size = 0.25) +
    coord_map() + scale_fill_distiller(palette = &amp;quot;Reds&amp;quot;, breaks = pretty_breaks(n = 10)) +
    guides(fill = guide_legend(reverse = TRUE)) + theme_nothing(legend = TRUE) + ggtitle(&amp;quot;Percent of Population is White&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-27-ACS-map_files/figure-html/acsR-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The bright red chuck on the border of Bingham and Bannock counties, with Pocatello on the southern edge, is the Fort Hall Indian Reservation. Not much else is surprising, Idaho is mostly white.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;per-capita-income&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Per Capita Income&lt;/h2&gt;
&lt;p&gt;Lastly, let’s take a look at the per capita income.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#remove extra data, 
IDacsI &amp;lt;- transmute(IDacs2015, id=CensusTract, IncomePerCap = IncomePerCap) 
IplotData &amp;lt;- left_join(tract, IDacsI)

ggplot() + geom_polygon(data = IplotData, aes(x = long, y = lat, group = id,
        fill = IncomePerCap)) +
    geom_polygon(data = county, aes(x = long, y = lat, group = id),
        fill = NA, color = &amp;quot;black&amp;quot;, size = 0.25) +
    coord_map() + scale_fill_distiller(palette = &amp;quot;Greens&amp;quot;, breaks = pretty_breaks(n = 10), direction = 1) +
    guides(fill = guide_legend(reverse = TRUE)) + theme_nothing(legend = TRUE) + ggtitle(&amp;quot;Per Capita Income&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-27-ACS-map_files/figure-html/acsI-1.png&#34; width=&#34;672&#34; /&gt; Notice the large dark green in the center of the state, that’s Sun Valley. There’s also a dark area in part of Boise.&lt;/p&gt;
&lt;p&gt;This is just a glimpse of what’s in the ACS, I encourage you to play around some.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Stock Random Walks</title>
      <link>/post/2018-01-15-stock-rw/</link>
      <pubDate>Mon, 15 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-15-stock-rw/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Recently a student in another course came to my office looking for someone “who could explain the Monte Carlo simulation” to her. I was caught a bit off-guard since (a) it was 10 minutes before my geometry class and (b) there is no single Monte Carlo simulation.&lt;/p&gt;
&lt;p&gt;After a brief discussion, I found out she wanted to predict stock prices using Monte Carlo simulation, but she thought that the Monte Carlo simulation provided the prediction - she couldn’t say how the actual predictions were being made which is the crucial part.&lt;/p&gt;
&lt;div id=&#34;aside-on-monte-carlo&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Aside on Monte Carlo&lt;/h2&gt;
&lt;p&gt;If you are familiar with Monte Carlo simulations, skip this, but if not it may be worth reading.&lt;/p&gt;
&lt;p&gt;A Monte Carlo simulation is a process of using the outcomes of a random process to better understand the probability distribution of the process. The method of creating the outcomes if dependent on the situation (although it should utilize some type of random sampling).&lt;/p&gt;
&lt;p&gt;In my Computer Science classes, I have students use a Monte Carlo simulation to calculate &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; (I usually do this Intro Stats too). This involves choosing &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; values between &lt;span class=&#34;math inline&#34;&gt;\(-1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; (uniform distribution) and seeing how many &lt;span class=&#34;math inline&#34;&gt;\((x,y)\)&lt;/span&gt; pairs are inside the unit circle. For a sufficiently large number of points, the ratio of the number inside to the total should be the same as the ratio of area of the unit circle to the area of the surrounding square (where all possible points lie).&lt;/p&gt;
&lt;p&gt;In Bayesian modelling, Markov Chain Monte Carlo simulations are run to get a sufficient understanding of the posterior probability distribution. This distribution is usually multivariate and except in particular circumstances doesn’t have a nice analytic definition.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;random-walks&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Random Walks&lt;/h1&gt;
&lt;p&gt;One way that we could use a Monte Carlo simulation to predict stock prices is to use a random walk to generate the predicted stock prices. There are many ways we could do this, some using lots of economics sophistication, but we’ll focus on the simpliest case to make the general process clear.&lt;/p&gt;
&lt;p&gt;A random walk is a random process that describes movement from a starting point over a number of steps through a space. For stocks, if we use the current price as the starting point then selecting normally distributed random numbers with mean &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, then cumulatively sum the random numbers and add to the base price, we form a random walk. More complex models could add (a) trends, (b) seasonality, (c) other distribution structures or combinations of the above.&lt;/p&gt;
&lt;p&gt;We’ll do the simple case &lt;span class=&#34;math display&#34;&gt;\[price~at~step~t = base~price + \sum_{k=1}^{t} \mathrm{rnorm}(n,\mu=0, \sigma=?)\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the length of the forecast and we’ll use stock data from Johnson and Johnson (NYSE:JNJ).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;jnj-prediction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;JNJ Prediction&lt;/h1&gt;
&lt;div id=&#34;the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Data&lt;/h2&gt;
&lt;p&gt;I downloaded weekly &lt;a href=&#34;/post/jnj-week.csv&#34;&gt;data&lt;/a&gt; for Johnson and Johnson from Yahoo finance. First, we’ll get rid of a couple coloumns and reduce the date range to 2017 and the start of 2018.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr)
jnj_all &amp;lt;- read_csv(&amp;quot;../../static/files/jnj-week.csv&amp;quot;, 
    col_types = cols(Date = col_date(format = &amp;quot;%Y-%m-%d&amp;quot;)))
library(dplyr)

#Get 2017 (and early 2018) data
jnj17 &amp;lt;- jnj_all %&amp;gt;% select(Date, Close, High, Low) %&amp;gt;%
  filter(Date&amp;gt; as.Date(&amp;quot;2017-01-01&amp;quot;)) %&amp;gt;% arrange(Date)

#plot
library(ggplot2)
ggplot(jnj17, aes(x=Date,y=Close)) + geom_line() + 
  ggtitle(&amp;quot;JNJ Stock Price since 1/1/2017&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-15-stock-rw_files/figure-html/jnjData-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;single-random-walk&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Single Random Walk&lt;/h2&gt;
&lt;p&gt;First, we’ll build a single random walk. A Monte Carlo simulation will need lots of random walks, but if we can do one, lots should be easy.&lt;/p&gt;
&lt;p&gt;Do simplify things, I’m going to add an “index” variable instead of working explicitly with dates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;jnj17$idx &amp;lt;- 1:length(jnj17$Close)
jnj17$type &amp;lt;- &amp;quot;Actual&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s make a random walk to predict the next 25 weeks of stock closing values. We’ll assume that the prices should have normally distributed changes around the most recent price and that the standard deviation will be half the average of the weekly ranges over the last year(ish). This last bit is pretty arbitrary, we could use a standard deviation &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, or something else justified by economics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n&amp;lt;- length(jnj17$Close)
rw &amp;lt;- jnj17$Close[n]+cumsum(rnorm(25, mean = 0, sd = 0.5*mean(jnj17$High - jnj17$Low)))

#build new data.frame
rwData &amp;lt;- data.frame(idx=(n+1):(n+25), Close=rw, type=rep(&amp;quot;RW&amp;quot;,25))

#table
library(knitr)
kable(rwData)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;idx&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Close&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;56&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;144.5784&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;57&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;146.2035&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;58&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;144.7451&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;59&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;142.6243&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;60&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;141.8303&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;61&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;142.1619&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;62&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;140.6649&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;63&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;141.3005&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;64&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;142.0169&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;65&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;142.9144&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;66&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;144.5575&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;67&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;145.5519&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;68&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;148.1797&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;69&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;146.6338&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;70&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;147.9975&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;71&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;147.0684&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;72&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;147.9050&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;73&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;146.7997&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;74&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;145.7820&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;75&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;145.8861&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;76&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;147.7343&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;77&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;146.6477&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;78&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149.7615&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;79&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149.4796&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;80&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;146.7478&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;RW&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#plot
rbind.data.frame(select(jnj17, idx,Close,type), rwData) %&amp;gt;%
  ggplot(aes(x=idx,y=Close, col=type))+geom_line()+
  ggtitle(&amp;quot;JNJ Actual and Predicted Price&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-15-stock-rw_files/figure-html/firstRW-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is likely a bad prediction at any given index. The hope is that lots of similarly constructed predictions will give insight into the probability distribution of the future JNJ stock prices. This means we’ll need lots of random walks.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-random-walks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple Random Walks&lt;/h2&gt;
&lt;p&gt;We just need to replicate what we did previously for an arbitrary number of times. To automate this, we’ll make a function to give a data frame with our random walk data, this will work with any similarly structured data (other stock data from Yahoo finance).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;randWalk &amp;lt;- function(typeName, len, obsData){
    n&amp;lt;- length(obsData$Close)
    rw &amp;lt;- obsData$Close[n]+cumsum(rnorm(len, mean = 0, sd = 0.5*mean(obsData$High - obsData$Low)))

    #build new data.frame
    rwData &amp;lt;- data.frame(idx=(n+1):(n+len), Close=rw, type = rep(typeName,len))
    return(rwData)
}

#doing 7 random walks because of the colorblind palette
rwList &amp;lt;- lapply(1:7, function(x) {randWalk(paste(&amp;quot;RW&amp;quot;,x,sep=&amp;quot;&amp;quot;),25,jnj17)})

rwDF &amp;lt;- as.data.frame(bind_rows(rwList))
jnjPred &amp;lt;- rbind.data.frame(select(jnj17,idx,Close,type), rwDF)

#store colorblind palette
cbbPalette &amp;lt;- c(&amp;quot;#000000&amp;quot;, &amp;quot;#E69F00&amp;quot;, &amp;quot;#56B4E9&amp;quot;, &amp;quot;#009E73&amp;quot;, &amp;quot;#F0E442&amp;quot;, &amp;quot;#0072B2&amp;quot;, &amp;quot;#D55E00&amp;quot;, &amp;quot;#CC79A7&amp;quot;)

ggplot(jnjPred, aes(x=idx,y=Close,col=type)) + 
  geom_line() + ggtitle(&amp;quot;JNJ Predictions with Multiple Random Walks&amp;quot;) + 
  scale_color_manual(values=cbbPalette)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-15-stock-rw_files/figure-html/MCRW-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The collection of random walks are a random sample of all JNJ stock price predictions for the next 25 weeks. Because of how we build our predictions, we clearly see oscilation about the most recent actual close. By using a more informative prediction process, we may see more informative predictions but this would just alter our randWalk function. We can use this to clean up the graph a bit, we can plot the mean of the random walks and their range at each index.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rwDFreduced &amp;lt;- group_by(rwDF, idx) %&amp;gt;% 
  summarise(meanPred=mean(Close), high = max(Close), low=min(Close)) %&amp;gt;% 
  mutate(Close = meanPred, type=&amp;quot;Prediction&amp;quot;)

ggplot(jnj17, aes(x=idx,y=Close,col=type)) + geom_line() +
  geom_ribbon(data=rwDFreduced, aes(x=idx,ymin=low,ymax=high), fill=&amp;quot;grey70&amp;quot;, inherit.aes = FALSE) + 
  geom_line(data=rwDFreduced, aes(x=idx,y=Close, col=type)) + 
  ggtitle(&amp;quot;JNJ 7 Random Walks Prediction Ribbon&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-15-stock-rw_files/figure-html/combRW-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Due to the lack of any economic theory, I wouldn’t put much weight in this prediction but it would be easy to incorporate that into the random walk and the Monte Carlo simulation won’t change. Additionally, each time this code is re-run, the above ribbon can change noticeably.&lt;/p&gt;
&lt;p&gt;With the ribbon, there’s no need to limit ourselves to 7 random walks. Let’s do more for a real Monte Carlo simulation (and maybe a better, or at least more stable, prediction).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rwList &amp;lt;- lapply(1:100, function(x) {randWalk(paste(&amp;quot;RW&amp;quot;,x,sep=&amp;quot;&amp;quot;),25,jnj17)})

rwDF &amp;lt;- as.data.frame(bind_rows(rwList))
rwDFreduced &amp;lt;- group_by(rwDF, idx) %&amp;gt;% 
  summarise(meanPred=mean(Close), high = max(Close), low=min(Close)) %&amp;gt;% 
  mutate(Close = meanPred, type=&amp;quot;Prediction&amp;quot;)

ggplot(jnj17, aes(x=idx,y=Close,col=type)) + geom_line() +
  geom_ribbon(data=rwDFreduced, aes(x=idx,ymin=low,ymax=high), fill=&amp;quot;grey70&amp;quot;, inherit.aes = FALSE) + 
  geom_line(data=rwDFreduced, aes(x=idx,y=Close, col=type)) + 
  ggtitle(&amp;quot;JNJ 100 Random Walks Prediction Ribbon&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-15-stock-rw_files/figure-html/bigrw-1.png&#34; width=&#34;672&#34; /&gt; With so many random walks, it’s no surprise the prediction line (the mean of the random walks) is nearly flat, this is the Central Limit Theorem in action.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>GGPlot and Geometric Transformations II: Inversions</title>
      <link>/post/2018-01-07-inversions/</link>
      <pubDate>Sun, 07 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-07-inversions/</guid>
      <description>&lt;p&gt;This is the second part of two posts about using ggplot to visualize geometric transformations in the complex plane.&lt;/p&gt;
&lt;div id=&#34;inversions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Inversions&lt;/h1&gt;
&lt;p&gt;For this post we’ll focus on inversions, having already covered rotations, translations, and reflections. An inversion can be thought of as a reflection across a circle, the the inside of the circle gets flipped to fill the plane outside the circle and the outside is flipped into the circle. This is a more complicated transformation, both to visualize and to perform mathematically, but is essential to geometry.&lt;/p&gt;
&lt;div id=&#34;the-math&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Math&lt;/h2&gt;
&lt;p&gt;Since an inversion involves a circle, we’ll need a center &lt;span class=&#34;math inline&#34;&gt;\(z_0 = x_0+iy_0\)&lt;/span&gt; and a radius &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;. Any point &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; that’s within &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(z_0\)&lt;/span&gt; must be moved to a point further than &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(z_0\)&lt;/span&gt;, and should lie on a line connected &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z_0\)&lt;/span&gt;. This is accomplished via the formula: &lt;span class=&#34;math display&#34;&gt;\[
inv(z) = \frac{r^2}{\overline{z-z_0}}+z_0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since dividing by a complex number is not really done, we can turn this formula into: &lt;span class=&#34;math display&#34;&gt;\[
inv(z) = \frac{r^2(z-z_0)}{|z-z_0|^2}+z_0
\]&lt;/span&gt; This may not look simpler, but it will be easier to write code for since it just multiples &lt;span class=&#34;math inline&#34;&gt;\(z-z_0\)&lt;/span&gt; by the square of the ratio of the radius to the distance from &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(z_0\)&lt;/span&gt;, then translates by &lt;span class=&#34;math inline&#34;&gt;\(z_0\)&lt;/span&gt;. Since the ratio of distances is real, this means we’re just translating &lt;span class=&#34;math inline&#34;&gt;\(z_0\)&lt;/span&gt; to the origin, dilating by the square of distance ratio, then translating back to &lt;span class=&#34;math inline&#34;&gt;\(z_0\)&lt;/span&gt;. Do we really even need new code? Not really.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The code&lt;/h2&gt;
&lt;p&gt;We use the translation and rotation functions from the previous post:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;translateR &amp;lt;- function(x,y,tx,ty){return (x+tx)}
translateI &amp;lt;- function(x,y,tx,ty){return (y+ty)}
rotateR &amp;lt;- function(x,y,rx,ry){return (x*rx-y*ry)}
rotateI &amp;lt;- function(x,y,rx,ry){return (x*ry+y*rx)}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our inversion function will apply these, and work well when handed a data.frame of points.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inversion &amp;lt;- function(x,y,x0,y0,r){
  #set-up a data.frame to be returned
  pt &amp;lt;- data.frame(x=x, y=y)
  #translate z_0 to the origin
  pt$x &amp;lt;-translateR(x,y,-x0,-y0)
  pt$y &amp;lt;-translateI(x,y,-x0,-y0)
  #dilate
  distRatio &amp;lt;- r^2/((x-x0)^2+(y-y0)^2)
  pt[,1:2] &amp;lt;- c(rotateR(pt$x,pt$y,distRatio,0),
          rotateI(pt$x,pt$y,distRatio,0))
  #translate back
  pt[,1:2] &amp;lt;- c(translateR(pt$x,pt$y,x0,y0),
          translateI(pt$x,pt$y,x0,y0))
  return(pt)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;graphs-of-inversion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Graphs of Inversion&lt;/h1&gt;
&lt;p&gt;Let’s start by re-using our rectangle from previously and doing an inversion across the circle centered at the origin with radius 2.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rectangle &amp;lt;- data.frame(x = c(rep(seq(from=0, to=2, by=.1),2), rep(0,11),rep(2,11)),
                  y=c(rep(0,21),rep(1,21),rep(seq(0,1,by=.1),2)))
rectangle$code &amp;lt;- &amp;quot;Original&amp;quot;

invRect &amp;lt;- data.frame(x=rectangle$x, y=rectangle$y)

invRect[,1:2]&amp;lt;- inversion(rectangle$x, rectangle$y, 0,0,2)
invRect$code &amp;lt;- &amp;quot;Inverted&amp;quot;

library(ggplot2)
library(dplyr, quietly = TRUE)

rbind.data.frame(rectangle, invRect) %&amp;gt;% 
  ggplot(aes(x=x,y=y, col=code)) + geom_point(size=1) + xlim(c(-2,12)) + ylim(c(-2,12))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-inversions_files/figure-html/makeSquare-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice how lines through the center of the circle remain lines. The lower right corner, which is on the circle, if fixed, but the center of the circle is sent to the outer reaches of the plane. The upper right corner, which was outside the circle, gets flipped inside the circle.&lt;/p&gt;
&lt;p&gt;Let’s do another example, this time we’ll use a circle centered at &lt;span class=&#34;math inline&#34;&gt;\((-1,-1)\)&lt;/span&gt; with radius 4.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;invRect &amp;lt;- data.frame(x=rectangle$x, y=rectangle$y)

invRect[,1:2]&amp;lt;- inversion(rectangle$x, rectangle$y, -1,-1,4)
invRect$code &amp;lt;- &amp;quot;Inverted&amp;quot;

rbind.data.frame(rectangle, invRect) %&amp;gt;% 
  ggplot(aes(x=x,y=y, col=code)) + geom_point(size=1) + xlim(c(-2,12)) + ylim(c(-2,12))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-inversions_files/figure-html/secInv-1.png&#34; width=&#34;672&#34; /&gt; What if we take a rectangle outside of the circle and invert?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;invRect &amp;lt;- data.frame(x=rectangle$x, y=rectangle$y)

invRect[,1:2]&amp;lt;- inversion(rectangle$x, rectangle$y, 5,0,2)
invRect$code &amp;lt;- &amp;quot;Inverted&amp;quot;

rbind.data.frame(rectangle, invRect) %&amp;gt;% 
  ggplot(aes(x=x,y=y, col=code)) + geom_point(size=1) + xlim(c(-2,7)) + ylim(c(-2,3))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-inversions_files/figure-html/thirdInv-1.png&#34; width=&#34;672&#34; /&gt; Inversions aren’t a simple to predict as rotations and translations. I encourage you to experiment a bit: try other shapes, lines, or just collections of points. Do inversions preserve anything geometrically meaningful?&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>GGPlot and Geometric Transformations</title>
      <link>/post/2018-01-06-ggplot-and-geometric-transformations/</link>
      <pubDate>Sat, 06 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-06-ggplot-and-geometric-transformations/</guid>
      <description>&lt;p&gt;I’m currently teaching a Geometry course, and wished there was an easy way to illustrate geometric transformations for my students. I’m sure they’ll agree I’m not a great artist.&lt;/p&gt;
&lt;p&gt;Since R is my preferred way to draw any picture, I thought “Let’s use GGPlot to show transformations!”&lt;/p&gt;
&lt;p&gt;For those not versed in geometry, we would like to easily visualize translations (shifts along a vector), rotations, and dilations of points (or collections of points) in the complex plane. Reflections can be achieved via a combination of rotation and translation. Another important transformation, inversion, will be done in the next post.&lt;/p&gt;
&lt;div id=&#34;functions-for-the-transformations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Functions for the transformations&lt;/h1&gt;
&lt;p&gt;I’m going to write separate functions for the real and imaginary parts of the result of each transformation, this is to make things easy to produce dataframes to send to ggplot. There are more efficient ways to do this, but I want all functions to have a consistent input/output structure so applying transformations and graphing the results is easy.&lt;/p&gt;
&lt;div id=&#34;translations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Translations&lt;/h2&gt;
&lt;p&gt;Mathematically, a translation in the complex plane is a function that adds a fixed number to the input. The inputs tx and ty are the real and imaginary parts of the point we’re translating by.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;translateR &amp;lt;- function(x,y,tx,ty){return (x+tx)}
translateI &amp;lt;- function(x,y,tx,ty){return (y+ty)}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;rotations-and-dilations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rotations and Dilations&lt;/h2&gt;
&lt;p&gt;Mathematically, multiplying by a complex number rotates (about the origin) by it’s argument and dilates by it’s modulus. Similarly to above, rx and ry are the real and imaginary parts of the number we multiply by to achieve the rotation/dilation (ry=0 will be just a dilation).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rotateR &amp;lt;- function(x,y,rx,ry){return (x*rx-y*ry)}
rotateI &amp;lt;- function(x,y,rx,ry){return (x*ry+y*rx)}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Rotations about a point other then the origin can be accomplished by applying a translation to the origin, rotation about the origin, and then translating back.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;illustrating-the-transformations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Illustrating the transformations&lt;/h1&gt;
&lt;p&gt;To clearly show a transformation, let’s start with a collection on points in the complex plane:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rectangle &amp;lt;- data.frame(x = c(rep(seq(from=0, to=2, by=.1),2), rep(0,11),rep(2,11)),
                  y=c(rep(0,21),rep(1,21),rep(seq(0,1,by=.1),2)))
rectangle$code &amp;lt;- &amp;quot;Original&amp;quot;

library(ggplot2)
ggplot(rectangle, aes(x=x,y=y))+geom_point(size=1)+xlim(c(-5,5))+ylim(c(-5,5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-06-ggplot-and-geometric-transformations_files/figure-html/makeSquare-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now let’s try a translation by &lt;span class=&#34;math inline&#34;&gt;\(-1-2i\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trRectangle &amp;lt;- data.frame(x = translateR(rectangle$x, rectangle$y, -1,-2), 
                          y=translateI(rectangle$x, rectangle$y, -1,-2))
trRectangle$code &amp;lt;- &amp;quot;Translate&amp;quot;

library(dplyr)

rbind.data.frame(rectangle, trRectangle) %&amp;gt;%
  ggplot(aes(x=x,y=y,col=code))+geom_point(size=1)+
  xlim(c(-5,5))+ylim(c(-5,5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-06-ggplot-and-geometric-transformations_files/figure-html/applyTrans-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And a rotation by &lt;span class=&#34;math inline&#34;&gt;\(\frac{2+3i}{\sqrt{13}}\)&lt;/span&gt; (why pick nice numbers?):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rotRectangle &amp;lt;- data.frame(x = rotateR(rectangle$x, rectangle$y, 2/sqrt(13),3/sqrt(13)), 
                           y=rotateI(rectangle$x, rectangle$y, 2/sqrt(13),3/sqrt(13)))
rotRectangle$code &amp;lt;- &amp;quot;Rotation&amp;quot;

library(dplyr)

rbind.data.frame(rectangle, rotRectangle) %&amp;gt;%
  ggplot(aes(x=x,y=y,col=code))+geom_point(size=1)+
  xlim(c(-5,5))+ylim(c(-5,5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-06-ggplot-and-geometric-transformations_files/figure-html/applyRot-1.png&#34; width=&#34;672&#34; /&gt; We can also perform more complicated transformations. Let’s rotate about the original rectangle’s top right corner &lt;span class=&#34;math inline&#34;&gt;\(2+i\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(\frac{\pi}{4}\)&lt;/span&gt;, and double the size. This means we first move &lt;span class=&#34;math inline&#34;&gt;\((2,1)\)&lt;/span&gt; to the origin, rotate/dilate, then move back.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#translate to origin
glRect &amp;lt;- data.frame(x=translateR(rectangle$x, rectangle$y, -2,-1), 
                     y=translateI(rectangle$x, rectangle$y, -2,-1))


#rotate and dilate by 2e^{i*pi/4} = sqrt(2)+isqrt(2)
#
#We&amp;#39;re doing this in one line since both the real and imaginary
#rotations change and use the original coordinates. Two lines 
#would produce very different transformation.
glRect[,1:2] &amp;lt;- c(rotateR(glRect$x, glRect$y, sqrt(2), sqrt(2)) ,
                  rotateI(glRect$x, glRect$y, sqrt(2),sqrt(2)))

#Now translate back 
glRect$x &amp;lt;- translateR(glRect$x, glRect$y, 2,1)
glRect$y &amp;lt;- translateI(glRect$x, glRect$y, 2,1)

#add code for graph
glRect$code &amp;lt;-&amp;quot;Transformed&amp;quot;

#and graph
rbind.data.frame(rectangle, glRect) %&amp;gt;%
  ggplot(aes(x=x,y=y,col=code))+geom_point(size=1)+
  xlim(c(-5,5))+ylim(c(-5,5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-06-ggplot-and-geometric-transformations_files/figure-html/rotTrans-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The combination of rotation and translation can also produce a reflection about any line.&lt;/p&gt;
&lt;p&gt;Next up, inversions…&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Thoughts on Severe Class Imbalance </title>
      <link>/post/2018-01-01/</link>
      <pubDate>Tue, 02 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-01/</guid>
      <description>&lt;p&gt;Besides lots of family time and the creation of this blog/website, this is what I’ve been thinking about over the winter break.&lt;/p&gt;
&lt;div id=&#34;background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;As part of my research in &lt;em&gt;emergent reducibility&lt;/em&gt;, I’ve had to face a binary classification situation with severe class imbalance. Among brute-force searches, it seems that there’s roughly 1 case of emergent reducibility (what I’m looking for) for every 1 million irreducible cubic polynomials. It is known that there are infinitely many cubic polynomials with emergent reducibility.&lt;/p&gt;
&lt;p&gt;One standard way of dealing with class imbalance is to artificially increase the incidence of positive cases in the training data, but I’ve seen very little about how to decide how much to adjust the ratio of the two classes - that’s what this post is about.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;training-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Training Data&lt;/h1&gt;
&lt;p&gt;To examine the relationship of class imbalance on several classifiers, I build 21 training sets each with the same 52 cases of emergent reducibility and between 500 and 2500 (by 100 increments) polynomials without emergent reducibility. Each training set was used to train a variety of logristic regression, random forest, naive Bayes, and k-nearest neighbor models via caret.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;confusion-matrices&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Confusion Matrices&lt;/h1&gt;
&lt;p&gt;Once the models were trained, they were all tested against the same data set with 23 cases of emergent reducibility (no overlap with training data) and 8000 cases without emergent reducibility. For each model and training set combination, a confusion “matrix” was build, this is in the file &lt;a href=&#34;/post/confMats.csv&#34;&gt;confMats.csv&lt;/a&gt;. Let’s read that into R and add another variable, &lt;em&gt;mdlType&lt;/em&gt; that’s either &lt;em&gt;logistic&lt;/em&gt;, &lt;em&gt;RF&lt;/em&gt;, or &lt;em&gt;other&lt;/em&gt;. This is to facet some graphs later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;confMats &amp;lt;- read.csv(&amp;quot;../../static/post/confMats.csv&amp;quot;, header=TRUE)

logLocations &amp;lt;- grep(&amp;quot;lr&amp;quot;, confMats$mdl)
rfLocations &amp;lt;- grep(&amp;quot;rf&amp;quot;, confMats$mdl)

confMats$mdlType &amp;lt;- vector(mode=&amp;quot;character&amp;quot;, length=length(confMats$mdl))

confMats[logLocations,]$mdlType &amp;lt;- &amp;quot;Logistic&amp;quot;
confMats[rfLocations,]$mdlType &amp;lt;- &amp;quot;RF&amp;quot;
confMats[!(1:length(confMats$mdl) %in% c(logLocations,rfLocations)),]$mdlType&amp;lt;-&amp;quot;Other&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;roc-plots&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;ROC Plots&lt;/h1&gt;
&lt;p&gt;Now we’ll plot our confusion matrices in ROC space, each point is a model and training set combo. I’ve facetted by model type for readability.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)

#11 distinct colors, courtesy of colorbrewer2.org
cb11&amp;lt;-c(&amp;#39;#a6cee3&amp;#39;,&amp;#39;#1f78b4&amp;#39;,&amp;#39;#b2df8a&amp;#39;,&amp;#39;#33a02c&amp;#39;,&amp;#39;#fb9a99&amp;#39;,&amp;#39;#e31a1c&amp;#39;,&amp;#39;#fdbf6f&amp;#39;,&amp;#39;#ff7f00&amp;#39;,&amp;#39;#cab2d6&amp;#39;,&amp;#39;#6a3d9a&amp;#39;,&amp;#39;#ffff99&amp;#39;)
ggplot(confMats,aes(x=FP/(FP+TN),y=TP/(TP+FN),col=mdl))+geom_point()+facet_wrap(~mdlType)+scale_color_manual(values=cb11)+ggtitle(&amp;quot;ROC Plots of Models and Class Imbalance &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-01_files/figure-html/scatter-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The model &lt;em&gt;max&lt;/em&gt; seems to find the most, but this simply marks a polynomial as having emergent reducibility if any other model says it does. This indicates some models find cases that others miss (I have some nice heatmaps showing this also, for another day). The logistic regression models have much more irregular variation than I was expecting.&lt;/p&gt;
&lt;p&gt;To see how varying the number of non-emergent reducibile polynomials impacts performance, I’ll throw in some animation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gganimate)

pathPlot &amp;lt;- ggplot(confMats,aes(x=FP/(FP+TN),y=TP/(TP+FN),col=mdl,frame=ner))+geom_path(aes(cumulative=TRUE, group=mdl))+facet_wrap(~mdlType)+scale_color_manual(values=cb11)+ggtitle(&amp;quot;Animated ROC Paths&amp;quot;)

gganimate(pathPlot, &amp;quot;../../static/post/pathPlot.gif&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’m saving the gif and then displaying it outside the code chunk. This is because animated graphs seem to be turned pink inside code chunks.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/pathPlot.gif&#34; alt=&#34;pathPlot.gif&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;pathPlot.gif&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The random forest and knn models seem pretty stable as the number of non-emergent reducible case changes. Looking at the number of true positives we see a gradual decline as &lt;em&gt;ner&lt;/em&gt; increases:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(knitr)
nerRF.tab &amp;lt;- xtabs(TP~ner+mdl, data=confMats[confMats$mdl %in% c(&amp;quot;rfs&amp;quot;,&amp;quot;rfp&amp;quot;,&amp;quot;rfpp&amp;quot;,&amp;quot;rfsq&amp;quot;,&amp;quot;knn&amp;quot;),], drop.unused.levels = TRUE)
kable(nerRF.tab)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;knn&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;rfp&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;rfpp&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;rfs&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;rfsq&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;21&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;600&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;21&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;700&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;800&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;900&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1100&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1200&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1300&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1400&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1500&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1600&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1700&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1800&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1900&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2100&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2200&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2300&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2400&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2500&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The logistic regression models show the odd variation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;TPnerLR.tab &amp;lt;- xtabs(TP~ner+mdl, data=confMats[confMats$mdlType == &amp;quot;Logistic&amp;quot;,], drop.unused.levels = TRUE)
kable(TPnerLR.tab)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;lrp&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;lrs&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;lrsq&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;600&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;700&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;800&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;900&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1100&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1200&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1300&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1400&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1500&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1600&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1700&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1800&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;1900&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2100&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2200&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2300&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2400&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2500&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The variation across elements of the confusion matrices is perhaps best seen in the following plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyr)
library(dplyr)

gather(confMats, key=Type, value=Count, -c(ner, mdl, mdlType)) %&amp;gt;% ggplot(aes(x=ner, y=Count, col=mdl))+geom_line()+facet_wrap(~Type, scales = &amp;quot;free_y&amp;quot;)+ggtitle(&amp;quot;Confusion Matrix Visual as Training Class Imbalance Changes&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-01_files/figure-html/CMplot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Clearly, there’s something in the &lt;em&gt;ner&lt;/em&gt; 1500,1700,1800, and 2300 training sets that really helps logistic models but not other model types. This is something to look into.&lt;/p&gt;
&lt;p&gt;However, I’m still left wondering &lt;em&gt;What is the best ratio of classes in a training set?&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Emergent Reducibility</title>
      <link>/project/emergent-reducibility/</link>
      <pubDate>Thu, 28 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/emergent-reducibility/</guid>
      <description>&lt;p&gt;Project details&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Statistics: Side B</title>
      <link>/talk/stat-b/</link>
      <pubDate>Sun, 22 Oct 2017 00:00:00 -0600</pubDate>
      
      <guid>/talk/stat-b/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Infinite Family of Cubics with Emergent Reducibility at Depth 1</title>
      <link>/publication/infinite-cubics/</link>
      <pubDate>Wed, 01 Mar 2017 00:00:00 -0700</pubDate>
      
      <guid>/publication/infinite-cubics/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Infinite Family of Cubic Polynomials with Depth 1 Emergent Reducibility</title>
      <link>/talk/jmm-2015/</link>
      <pubDate>Sun, 11 Jan 2015 00:00:00 -0700</pubDate>
      
      <guid>/talk/jmm-2015/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
