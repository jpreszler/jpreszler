<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.46" />
  <meta name="author" content="Jason Preszler">
  <meta name="description" content="Assistant Professor - Math, Statistics, and Computer Science">

  
  <link rel="alternate" hreflang="en-us" href="/post/2019-02-23-em-1/">

  
  


  

  
  
  
  
    
  
  
    
    
      
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
      
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  
  <link rel="stylesheet" href="/styles.css">
  

  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-111748163-1', 'auto');
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Jason I. Preszler">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="Jason I. Preszler">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/post/2019-02-23-em-1/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Jason I. Preszler">
  <meta property="og:url" content="/post/2019-02-23-em-1/">
  <meta property="og:title" content="Expectation-Maximization | Jason I. Preszler">
  <meta property="og:description" content="">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2019-02-23T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2019-02-23T00:00:00&#43;00:00">
  

  

  <title>Expectation-Maximization | Jason I. Preszler</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Jason I. Preszler</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        

        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#teaching">
            
            <span>Teaching</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#talks">
            
            <span>Talks</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <div class="article-inner">
      <h1 itemprop="name">Expectation-Maximization</h1>

      

<div class="article-metadata">

  <span class="article-date">
    
    <time datetime="2019-02-23 00:00:00 &#43;0000 UTC" itemprop="datePublished">
      Feb 23, 2019
    </time>
  </span>

  

  
  

  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Expectation-Maximization&amp;url=%2fpost%2f2019-02-23-em-1%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=%2fpost%2f2019-02-23-em-1%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fpost%2f2019-02-23-em-1%2f&amp;title=Expectation-Maximization"
         target="_blank" rel="noopener">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=%2fpost%2f2019-02-23-em-1%2f&amp;title=Expectation-Maximization"
         target="_blank" rel="noopener">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Expectation-Maximization&amp;body=%2fpost%2f2019-02-23-em-1%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


      <div class="article-style" itemprop="articleBody">
        <p>As part of some clustering work and learning about hidden Markov models, I’ve been doing some reading about the EM algorithm and it’s applications. It’s a pretty neat algorithm (I love iterative algorithms like Newton’s method and the Euclidean algorithm) so I thought I’d illustrate how it works.</p>
<p>I’ve also been doing a bit more python recently, so I thought I would do all this in python rather than R. However, this post is still done in RMarkdown using python code chunks! I know Jupyter notebooks have their fans, but as an authoring tool RMarkdown is plain text which makes it easier to create/edit documents and maintain them via tools like Git (unlike Jupyter notebooks).</p>
<div id="the-basic-idea" class="section level1">
<h1>The Basic Idea</h1>
<p>Like any iterative algorithm, the big picture behind the EM algorithm is to converge to values given some initial guess. For our purposes, we want to converge to the parameters of a Gaussian mixture model given observed data, a number of Gaussians being mixed, and guesses for the initial means and standard deviations of the mixture components. This is one of the major practical uses of the EM algorithm: determining unknown parameters of mixture model components.</p>
<p>The algorithm repeats two key steps: <em>E</em>xpectation and <em>M</em>aximization. The expectation step calculates probabilities of each data-point being in a mixture component. The maximization step uses these probabilities to update (a) means and standard deviations of mixture components and (b) the proportion of data-points in each component. There’s not much here that’s special to the case of Gaussian mixture models (GMM’s), except where we use normal distributions and the parameters we compute in the maximization step.</p>
<p><strong>Note</strong>: The following functions are intended to be easy to understand as opposed to highly optimized with great error handling. If you’re estimating components of a Gaussian Mixture model in a production environment, Scikit-Learn has excellent clustering routines that will be far more robust and efficient than anything here.</p>
<div id="expectation-details" class="section level2">
<h2>Expectation Details</h2>
<p>The expectation step computes the probability of a data point being in any one of the mixture components. If <span class="math inline">\(x_i\)</span> is the data-point, <span class="math inline">\(\mu_c,~\sigma_c\)</span> the mean and standard deviation for component <span class="math inline">\(c\)</span> and <span class="math inline">\(f_c\)</span> the fraction of data-points in component <span class="math inline">\(c\)</span>, then these probabilities are: <span class="math display">\[
p_{ic} = \frac{f_c*N(x_i | \mu_c, \sigma_c)}{\sum_k f_k*N(x_i | \mu_k, \sigma_k)}.
\]</span></p>
<p>This can be turned in the following python code (as well as some libraries we’ll need throughout the post):</p>
<pre class="python"><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy
from scipy.stats import norm
def expectation_probabilities_gmm(data, mean_vect, sd_vect, fracPerComp):
    ret_probs = np.zeros([len(data),len(mean_vect)])
    for i in range(len(data)):
        for j in range(len(mean_vect)):
            ret_probs[i,j] = (fracPerComp[j]*norm.pdf(data[i],mean_vect[j],sd_vect[j]))
        ret_probs[i,:] = ret_probs[i,:]/np.sum(ret_probs[i,:])
    return(ret_probs)</code></pre>
<p>This will return a <code>numpy</code> array with one row per data-point and one column per mixture component, where values are the probabilities. Clearly, we have to supply the data, and previous values for the means, standard deviations, and fraction of points per component. Initially, these will be our guesses, but subsequent iterations will get them from the maximization step.</p>
</div>
<div id="maximization-details" class="section level2">
<h2>Maximization Details</h2>
<p>Once we have a new batch of probabilities, we need to update values for the mixture model parameters. First, we calculate a <em>component weight</em> for each component which is the column sum of our probability array. We then use the component weight to compute weighted means, standard deviations, and fractions of points per component. <span class="math display">\[
w_c = \sum_i p_{ic},\\
\mu_c = \frac{\sum_i p_{ic}x_i}{w_c},\\
\sigma_c^2 = \frac{\sum_i (p_{ic}(x_i-\mu_c)^2)}{w_c}\\
f_c = \frac{w_c}{n}
\]</span></p>
<p>Here’s a python function that computes these and returns the <span class="math inline">\(\mu_c, \sigma_c, f_c\)</span> tuple given the data and expectation probabilities:</p>
<pre class="python"><code>def maximization_params_gmm(expect_probs, data):
    clust_weights = [np.sum(expect_probs[:,j]) for j in
        range(np.shape(expect_probs)[1])]
    new_means = np.divide([np.sum(expect_probs[:,j]*data[:]) for
        j in range(np.shape(expect_probs)[1])],clust_weights)
    new_sds = np.divide([np.sum(expect_probs[:,j]*(data[:]-new_means[j])*(data[:]-new_means[j])) for j in range(np.shape(expect_probs)[1]) ], clust_weights)
    new_sds = np.sqrt(new_sds)
    new_frac = np.divide(clust_weights,np.shape(expect_probs)[0])
    return new_means, new_sds, new_frac;</code></pre>
<p>Note that this is for a mixture of univariate Gaussian distributions, in higher dimensions you would need covariance matrices to be computed rather than just standard deviations.</p>
</div>
<div id="stopping-criteria" class="section level2">
<h2>Stopping Criteria</h2>
<p>A key part of an iterative algorithm is when to stop, which is achieved in the EM algorithm by looking at the convergence of the log-likelihood. Although in this post we’ll just run a fixed number of iterations.</p>
<pre class="python"><code>def log_likelihood(data, mean_vect, sd_vect, fracPerComp):
  ret_probs = np.zeros([len(data),len(mean_vect)])
  for i in range(len(data)):
      for j in range(len(mean_vect)):
          ret_probs[i,j] = (fracPerComp[j]*norm.pdf(data[i],mean_vect[j],sd_vect[j]))
      row_sums = np.sum(ret_probs[i,:])
  loglike = np.sum(np.log(row_sums))
  return(loglike)</code></pre>
</div>
</div>
<div id="observed-mixture-data" class="section level1">
<h1>Observed Mixture Data</h1>
<p>I don’t have a nice real-world GMM dataset, but we can easily manufacture some:</p>
<pre class="python"><code>np.random.seed(seed=123987) #reproducibility
vals = [np.random.normal(5, .5, 100), np.random.normal(3, 1, 100), np.random.normal(6.5,1.25,100)]
grps = [&#39;A&#39;,&#39;B&#39;,&#39;C&#39;]
#build dataframe
df = pd.concat([pd.DataFrame({&quot;group&quot;:grps[i], &quot;value&quot;:vals[i]}) for i in range(0,3)], axis=0, ignore_index=True)</code></pre>
<p>Initially I had written this with 3 separate vectors of values and 3 data-frames that I then glued together. Then I remembered one of my favorite things about python: list comprehensions! Clearly I’ve made heavy use of them in the above functions as well.</p>
<p>With the <code>group</code> column in our dataframe, we can easily see the three components of the mixture:</p>
<pre class="python"><code>#grouped
plt.figure(figsize=(3,3))
plots = sns.FacetGrid(data=df, hue=&quot;group&quot;, legend_out=True)
plots.map(sns.distplot,&#39;value&#39;, kde_kws={&quot;shade&quot;:False},rug_kws={&quot;alpha&quot;:.3}, rug=True, kde=True, hist=False)
plt.show()</code></pre>
<p><img src="/post/2019-02-23-EM-1_files/figure-html/unnamed-chunk-5-1.png" width="288" /></p>
<p>In practice though, we would only be able to see the full distribution of our data:</p>
<pre class="python"><code>plt.close() #get a blank plot rather than overlay on our last distplots
plt.figure(figsize=(3,3))
sns.distplot(df[&#39;value&#39;], rug=True, kde=True, hist=False)
plt.show()</code></pre>
<p><img src="/post/2019-02-23-EM-1_files/figure-html/unnamed-chunk-6-1.png" width="288" /></p>
<p>The “shoulders” on the curve given a hint that we may be dealing with an underlying mixture model of 3 components. But how “non-normal” is our data? We can run a Shapiro-Wilkes test and compare with the same number of points sampled from a single normal distribution to get an idea.</p>
<pre class="python"><code>##Shapiro-Wilkes test
print(scipy.stats.shapiro(df[&#39;value&#39;]))</code></pre>
<pre><code>## (0.9909396767616272, 0.0613214448094368)</code></pre>
<p>The second value in this tuple is the p-value associated to a null hypothesis that our data is from a single normal distribution. In this case we would fail to reject that our observed data is different from a sample from a single normal distribution at the customary <span class="math inline">\(0.05\)</span> significance level. More evidence that p-values and significance tests can miss important details.</p>
<p>We can also compare to simulated data sampled from a true normal distribution with the same mean and standard deviation as our observed data to see how close our data is to being normal.</p>
<pre class="python"><code>##compare with true normal sample
simDF = pd.DataFrame({&quot;type&quot;:&quot;simulated&quot;, &quot;value&quot;:np.random.normal(df[&#39;value&#39;].mean(), df[&#39;value&#39;].std(), 300)})
simDF = pd.concat([pd.DataFrame({&quot;type&quot;:&quot;sample&quot;, &quot;value&quot;:df[&#39;value&#39;]}), simDF], axis=0, ignore_index=True)
plt.figure(figsize=(3,3))
plotsim = sns.FacetGrid(data=simDF, hue=&quot;type&quot;)
plotsim.map(sns.distplot,&#39;value&#39;, kde_kws={&quot;shade&quot;:False},rug_kws={&quot;alpha&quot;:.3}, rug=True, kde=True, hist=False)
plt.show()</code></pre>
<p><img src="/post/2019-02-23-EM-1_files/figure-html/unnamed-chunk-8-1.png" width="288" /></p>
</div>
<div id="running-the-em" class="section level1">
<h1>Running The EM</h1>
<p>Here’s a function to handle calling the E and M steps and holding the results of each. This will be handle to look at how the process converges.</p>
<pre class="python"><code>def run_em(data, init_means, init_sds, init_frac, num_iterations):
    ret_means = np.zeros((num_iterations,len(init_means)))
    ret_sds = np.zeros((num_iterations,len(init_means)))
    ret_fracs = np.zeros((num_iterations,len(init_means)))
    ret_ll = np.zeros(num_iterations)
    ret_means[0,:] = init_means
    ret_sds[0,:] = init_sds
    ret_fracs[0,:] = init_frac
    for i in range(num_iterations-1):
        ret_ll[i]=log_likelihood(data, ret_means[i,:],ret_sds[i,:], ret_fracs[i,:])
        new_probs = expectation_probabilities_gmm(data, ret_means[i,:],ret_sds[i,:], ret_fracs[i,:])
        ret_means[i+1,:], ret_sds[i+1,:],ret_fracs[i+1,:] = maximization_params_gmm(new_probs, data)
    return ret_means, ret_sds, ret_fracs, ret_ll;</code></pre>
<p>Now we can make some guesses and run the EM algorithm. Based on the distribution of all the data, it looks like there are irregularities around 3, 5.5, and 7 - so those will be our means. We’ll assume variance of 1 and equal size for all groups.</p>
<pre class="python"><code>means = [3,5.5, 7]
sds = [1,1,1]
fracs = [1/3,1/3,1/3]
mu,sigma,frac,ll = run_em(df[&#39;value&#39;], means, sds, fracs, 50)</code></pre>
<p>We can then easily use <code>matplotlib</code> to see how these change over the iterations:</p>
<pre class="python"><code>plt.figure(figsize=(5,5))
plt.subplot(2,2,1)
plt.plot(mu)
plt.title(&quot;Means&quot;)
plt.xlabel(&quot;Iteration&quot;)
plt.xticks([10*(i+1) for i in range(5)])
plt.subplot(2,2,2)
plt.plot(sigma)
plt.title(&quot;Std. Deviations&quot;)
plt.xlabel(&quot;Iteration&quot;)
plt.xticks([10*(i+1) for i in range(5)])
plt.subplot(2,2,3)
plt.plot(frac)
plt.title(&quot;Fraction Per Cluster&quot;)
plt.xlabel(&quot;Iteration&quot;)
plt.xticks([10*(i+1) for i in range(5)])
plt.subplot(2,2,4)
plt.plot(ll[0:48]) #last ll value hasn&#39;t been calculated
plt.title(&quot;Log-Likelihood&quot;)
plt.xlabel(&quot;Iteration&quot;)
plt.xticks([10*(i+1) for i in range(5)])
plt.tight_layout()
plt.show()</code></pre>
<p><img src="/post/2019-02-23-EM-1_files/figure-html/unnamed-chunk-11-1.png" width="480" /></p>
<p>This looks like we had convergence fairly quickly, except the fraction of points per cluster - but our y-scale is very narrow. In the end our estimates are:</p>
<pre class="python"><code>print(&quot;Means:&quot;, mu[49,:])</code></pre>
<pre><code>## Means: [2.9767655  4.91279169 6.30925586]</code></pre>
<pre class="python"><code>print(&quot;Std. Dev.:&quot;, sigma[49,:])</code></pre>
<pre><code>## Std. Dev.: [0.83852695 0.45363153 1.15733853]</code></pre>
<pre class="python"><code>print(&quot;Fraction per Cluster:&quot;, frac[49,:])</code></pre>
<pre><code>## Fraction per Cluster: [0.28652637 0.32794345 0.38553017]</code></pre>
<p>Since our mixture components had means <span class="math inline">\(3, 5, 6.5\)</span> and standard deviations <span class="math inline">\(1, 0.5, 1.25\)</span> we did pretty good. The main difference is the points where the normal distributions overlap with “high” probability. This is where any clustering algorithm will break down. Despite how close our data was to a single normal distribution, the EM algorithm was able to divide the observations into 3 groups with means and standard deviations almost equal to the actual cluster parameters.</p>
<p>I want to show some visuals of our mean and std. dev. estimates with the data and how the clusters evolve, but this post has gotten a little long. Look forward to part 2 soon…</p>
</div>

      </div>

      


<div class="article-tags">
  
  <a class="btn btn-primary btn-outline" href="/tags/python">Python</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/clustering">clustering</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/seaborn">seaborn</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/pandas">pandas</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/matplotlib">matplotlib</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/em">EM</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/mixture-models">Mixture Models</a>
  
</div>



    </div>
  </div>

</article>



<div class="article-container article-widget">
  <div class="hr-light"></div>
  <h3>Related</h3>
  <ul>
    
    <li><a href="/post/2018-11-10-reticulate-mm/">Reticulated Mixture Models</a></li>
    
  </ul>
</div>




<div class="article-container">
  

</div>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2017 Jason Preszler &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/c.min.js"></script>
      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>

