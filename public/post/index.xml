<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Recent Posts on Jason I. Preszler</title>
    <link>/post/</link>
    <description>Recent content in Recent Posts on Jason I. Preszler</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Jason Preszler</copyright>
    <lastBuildDate>Fri, 06 Jan 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>When Traceplots go Bad</title>
      <link>/post/2019-09-28-bad-traceplots/</link>
      <pubDate>Wed, 02 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-09-28-bad-traceplots/</guid>
      <description>One of the first things I learned studying abstract mathematics was that having non-examples were as important as examples of abstract structure. For instance, the definition of a group and some examples of groups it really only part of the picture - having examples of sets that satisfy all but one of the requirements to be a group allows you to see how the pieces fit together and lays a foundation for more solid intuition for further study.</description>
    </item>
    
    <item>
      <title>Rise Above the Noise</title>
      <link>/post/2019-09-19-elevation/</link>
      <pubDate>Wed, 18 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-09-19-elevation/</guid>
      <description>I’ve done some analysis of my gps running data before, but mostly just some mapping. I’ve always wanted to bring in some more sophisticated analysis such as identifying runs with similar geographic features (e.g. track workouts) or identifying, categorizing, and comparing hills. To really get into either of these things, I first needed good elevation data which isn’t provided by my forerunner 220. In this post I’ll show some of the problems with the elevation data coming from my garmin 220, how to get elevation data from the RaceMap API (and compare a few other elevation api’s), and then examine how good the new elevation data is.</description>
    </item>
    
    <item>
      <title>Poisson Process Simulation</title>
      <link>/post/2019-09-09-poisson-simulation/</link>
      <pubDate>Mon, 09 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-09-09-poisson-simulation/</guid>
      <description>While continuing to work through BDA3, and decided to revisit some of the earlier exercises that I had done in R. Problem 9 of chapter 1 asks to simulate a medical clinic with 3 doctors, patients arriving according to an exponential distribution with rate 10 minutes between 9AM and 4PM and each patient needing an appointment length uniformly distributed between 5 and 10 minutes. We are interested in things like the number of patients seen, average wait time, number of patients who had to wait, and when the clinic closes based on 1 simulated day and 100 simulated days (with intervals of each aggregation).</description>
    </item>
    
    <item>
      <title>Mass Shooting Changepoint</title>
      <link>/post/2019-08-23-mass-shoot/</link>
      <pubDate>Fri, 23 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-08-23-mass-shoot/</guid>
      <description>Every time there’s news about a mass shooting I feel like doing some type of data analysis about gun violence. With the shootings in Dayton and El Paso, as well as news of several likely shootings being prevented, I thought I would actually follow through with some analysis. Having been a senior in high school (in California) when the Columbine shooting took place, and also living in Salt Lake during the Trolley Square shooting I’ve seen the impacts of these tragedies and feel as though they are happening more frequently.</description>
    </item>
    
    <item>
      <title>Pythonic SQL with SQLAlchemy</title>
      <link>/post/2019-05-18-pysql/</link>
      <pubDate>Sat, 18 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-05-18-pysql/</guid>
      <description>During my Applied Databases course in Spring 2019, I gave my students a choice of which language to use to interact with SQL and relational databases. They had already learned core SQL and I only gave them 3 options: C++, R, and Python. The choices of R and python are natural given my data science interests and experience. Last year I just showed them R and got some complaints on evaluations (some people don’t think R is a “real language”).</description>
    </item>
    
    <item>
      <title>Python Web Scraping</title>
      <link>/post/2019-04-21-pyscrape/</link>
      <pubDate>Sun, 21 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-04-21-pyscrape/</guid>
      <description>I was putting some data together about previous catalogs for students for projects in my Applied Databases course and realized that I was missing something. I had course info (subject, number, title and url) for the last 4 catalog years at the College of Idaho, but I didn’t have course descriptions! What a great chance to do some simple web scraping in python.
Data Import and Cleaning Since I have a csv file for each catalog year with a link to each course, I just needed to read the urls, extract the description from the page, and save the results.</description>
    </item>
    
    <item>
      <title>Mapping with an 800 Pound Gorilla</title>
      <link>/post/2019-03-18-matplotlib-map/</link>
      <pubDate>Mon, 18 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-03-18-matplotlib-map/</guid>
      <description>I’ve been focusing on python recently to become a bi-lingual data scientist. Probably my least favorite thing about python is its plotting libraries - there are too many options built on top of matplotlib which pre-dates pandas dataframes. This makes for some clunky code and blurry boundaries (both “is that a seaborn, pandas, or matplotlib function?” and situations with 3 equally messy solutions but in very different ways). In my opinion, ggplot2’s deep interplay with dataframes makes a lot more sense and ggplot’s layers make it easy to change plot type (just switch the geom_), add facets, and tweak aesthetics.</description>
    </item>
    
    <item>
      <title>Expectation-Maximization</title>
      <link>/post/2019-02-23-em-1/</link>
      <pubDate>Sat, 23 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-02-23-em-1/</guid>
      <description>As part of some clustering work and learning about hidden Markov models, I’ve been doing some reading about the EM algorithm and it’s applications. It’s a pretty neat algorithm (I love iterative algorithms like Newton’s method and the Euclidean algorithm) so I thought I’d illustrate how it works.
I’ve also been doing a bit more python recently, so I thought I would do all this in python rather than R.</description>
    </item>
    
    <item>
      <title>Tidy Clouds</title>
      <link>/post/2019-01-11-wc/</link>
      <pubDate>Fri, 11 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-01-11-wc/</guid>
      <description>In my data visualization class I had the students get a book from Project Gutenberg using the gutenbergr package and build a word cloud using tidytext and wordcloud. It’s much easier that the “old” corpus/text mapping approach, and when the students were sharing their clouds they started showing the cloud and having students try to guess the book. This made me think of using a Shiny runtime to make a little word cloud guessing game.</description>
    </item>
    
    <item>
      <title>Sankey Diagram</title>
      <link>/post/2018-12-27-sankey/</link>
      <pubDate>Thu, 27 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-12-27-sankey/</guid>
      <description>Update 7/23/2019 Various package updates have created problems with showing more than one javascript plot on a post. I’ve added calls to htlwidgets::onRender to get at least one plot displayed. I may revisit this, but the interaction between hugo, blogdown, and various javascript libraries (chorddiag, networkD3, D3, data tables, etc) is more than I’m able to dive into at the moment.
This post is about a type of visualization the will hopefully help see how students “flow” through college.</description>
    </item>
    
    <item>
      <title>PCA Overview</title>
      <link>/post/2018-11-24-pca/</link>
      <pubDate>Sat, 24 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-11-24-pca/</guid>
      <description>This post is primarily to give the basic overview of principal components analysis (PCA) for dimensionality reduction and regression. I wanted to create it as a guide for my regression students who may find it useful for their projects. First, let’s note the two main times that you may want to use PCA - dimensionality reduction (reducing variables in a dataset) and removing colinearity issues. These are not exclusive problems, often you want to do both.</description>
    </item>
    
    <item>
      <title>Reticulated Mixture Models</title>
      <link>/post/2018-11-10-reticulate-mm/</link>
      <pubDate>Sat, 10 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-11-10-reticulate-mm/</guid>
      <description>Clearly, there’s no such thing as a “reticulated mixture model” but if you create one I’ll gladly take credit for the name. Instead this post is a demonstration of using mixture models for clustering and the interplay of R and Python via RStudio’s reticulate package.
Mixture Model Basics The idea behind mixture models is that you have data containing information from two (or more) subgroups and you want to uncover structure of the subgroups.</description>
    </item>
    
    <item>
      <title>Ridges of Normality</title>
      <link>/post/2018-10-01-norm-ridge/</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-10-01-norm-ridge/</guid>
      <description>One of the classic assumptions of the linear regression models is that, conditioned on the explanatory variables, the response variable should be normally distributed. While teaching this the other day, I had a flash of insight into how to visualize this - ridge-line plots!
Data I’ve been using Matloff’s Statistical Regression and Classification book, which uses the mlb dataset from his freqparcoord package. This has data on heights, weights, ages, positions, and teams of over 1000 major league baseball players.</description>
    </item>
    
    <item>
      <title>What a Tangled Web We Weave...</title>
      <link>/post/2018-08-21-chords/</link>
      <pubDate>Tue, 21 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-08-21-chords/</guid>
      <description>Update 7/23/2019 Various package updates have created problems with showing more than one javascript plot on a post. I’ve added calls to htlwidgets::onRender to get at least one plot displayed. I may revisit this, but the interaction between hugo, blogdown, and various javascript libraries (chorddiag, networkD3, D3, data tables, etc) is more than I’m able to dive into at the moment.
cd &amp;lt;- chorddiag( xtabs(~MAJOR+minor, data = mmhl[mmhl$Grad.</description>
    </item>
    
    <item>
      <title>Lesser Known Verbs: top_n</title>
      <link>/post/2018-07-30-top_n/</link>
      <pubDate>Mon, 30 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-07-30-top_n/</guid>
      <description>I’ve been using R since 2006. That predates RStudio and the tidyverse. I remember the struggle of keeping track of the variants of apply and often fiddling with them to get code to work.
Then came plyr and the dplyr and my life has never been the same. The major verbs of dplyr include select, filter, mutate, group_by, summarise, and arrange; and if you are doing data analysis in R then you should be fluent in them.</description>
    </item>
    
    <item>
      <title>Catalog Evolution</title>
      <link>/post/2018-07-28-cat-evolve/</link>
      <pubDate>Sat, 28 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-07-28-cat-evolve/</guid>
      <description>Recently I’ve posted about the College of Idaho’s 2017-2018 and 2018-2019 course distribution. The second post showed how easy it was to reproduce everything, which was good because a colleague recently asked about the total number of courses in 2016-2017 for a funded grant related to curriculum review. These total numbers of courses of courses made me wonder about how the catalog has evolved over the last few years?</description>
    </item>
    
    <item>
      <title>Re-Counting Classes</title>
      <link>/post/2018-07-24-recount-class/</link>
      <pubDate>Tue, 24 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-07-24-recount-class/</guid>
      <description>Edit 7/27/2018 I realized that MFL’s name change to WLC didn’t change the prefix of their courses, this broke my scrapper. Below is an updated post that deals with this.
Back in early May, I wrote a post about scraping the College of Idaho catalog: Counting Classes. Below if the same post (boring…) except that the “current catalog” has been updated. This is really a demonstration of reproducibility, the upstream data has changed and ideally all my code still works.</description>
    </item>
    
    <item>
      <title>DT: When Tables are the Product</title>
      <link>/post/2018-06-28-dt/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-06-28-dt/</guid>
      <description>In RMarkdown documents I often have a need to display tables, which I usually try to keep small with only the most useful information displayed. However, a recent project made me look for a better way to share tabular data with non-data-scientists. The answer was R’s DT package, which allows for very powerful displays of tabular data.
Today’s data will be a summary of enrollment data from the College of Idaho:</description>
    </item>
    
    <item>
      <title>Maps Majors in Neo4J</title>
      <link>/post/2018-06-18-neo4j-majors/</link>
      <pubDate>Mon, 18 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-06-18-neo4j-majors/</guid>
      <description>UPDATE (6/20/2018) The cypher query for Table 3 only used components with “optional” courses so the capstone and topics compnents of the Math/CS major weren’t included in table 3.
UPDATE (6/19/2018) The original version of this post used incorrectly loaded data that caused to “Core” of every major to have the same classes attached to it. This was noticed by my colleague Dave Rosoff and has been corrected.</description>
    </item>
    
    <item>
      <title>Maps Minors in Neo4J</title>
      <link>/post/2018-06-16-neo4j-demo/</link>
      <pubDate>Fri, 15 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-06-16-neo4j-demo/</guid>
      <description>A college curriculum seems like something that is a natural fit for a graph database. My last post collected data from the College of Idaho’s online catalog, using that and some information about majors and minors I’ve populated a graph database in Neo4j. In this post I’ll show how to do some basic queries that return tabular data as well as graph data using .
Graph DB Basics For those who haven’t had much discrete math or computer science, a graph is a collection of nodes (aka vertices) and edges that connect nodes.</description>
    </item>
    
    <item>
      <title>Counting Classes: The Basics</title>
      <link>/post/2018-05-05-counting-class/</link>
      <pubDate>Sat, 05 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-05-05-counting-class/</guid>
      <description>At the College of Idaho, there’s been discussion about visualizing the curriculum as well as understanding the curriculum. Naturally this interests me as a chance to wallow in some complicated data (students are required to complete a major and 3 minors across 4 “peaks” rather than complete courses from a traditional “core”). I thought using R and a Neo4j graph database would be useful (something to look forward to) - but first I needed to get data from the catalogue!</description>
    </item>
    
    <item>
      <title>SQL in RMarkdown!</title>
      <link>/post/2018-04-01-sql-rmd/</link>
      <pubDate>Sun, 01 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-04-01-sql-rmd/</guid>
      <description>This semester I’m teaching Applied Databases for the first time and have been struggling with some notes and handouts for students; as well as simple, easy to use database interfaces that work well across platforms. I love RMarkdown, and today realized that knitr has an SQL code engine!
Basic Syntax I often give handouts on SQL statements as we learn about them, so I need a nice way to show commands.</description>
    </item>
    
    <item>
      <title>Running Tacoma: Maps</title>
      <link>/post/2018-3-11-run-tacoma1/</link>
      <pubDate>Sun, 11 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-3-11-run-tacoma1/</guid>
      <description>When I lived in Tacoma, I was running quite a bit. Since I moved away my training has become much more irregular, but I thought it would be interesting to take the Tacoma data from my current Garmin Forerunner 220 a take a look.
Data Prep The Garmin stores data in .fit format, but gpsbabel can translate to a nicely structured GPX file, which is what I’ll start with here.</description>
    </item>
    
    <item>
      <title>Idaho ACS Mapping</title>
      <link>/post/2018-01-27-acs-map/</link>
      <pubDate>Sat, 27 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-27-acs-map/</guid>
      <description>Recently some diversity stats have been circulated around the College of Idaho, and as new Idahoan I wondered about the general diversity (or lack thereof) in Idaho. I remembered seeing this post a while back about mapping in R, so I went to work.
Shapefiles First, we need shapefiles for both the Idaho country boundaries and census tracts, which will give finer detail for data. These can be downloaded from the [US Census Bureau] (https://www.</description>
    </item>
    
    <item>
      <title>Stock Random Walks</title>
      <link>/post/2018-01-15-stock-rw/</link>
      <pubDate>Mon, 15 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-15-stock-rw/</guid>
      <description>Introduction Recently a student in another course came to my office looking for someone “who could explain the Monte Carlo simulation” to her. I was caught a bit off-guard since (a) it was 10 minutes before my geometry class and (b) there is no single Monte Carlo simulation.
After a brief discussion, I found out she wanted to predict stock prices using Monte Carlo simulation, but she thought that the Monte Carlo simulation provided the prediction - she couldn’t say how the actual predictions were being made which is the crucial part.</description>
    </item>
    
    <item>
      <title>GGPlot and Geometric Transformations II: Inversions</title>
      <link>/post/2018-01-07-inversions/</link>
      <pubDate>Sun, 07 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-07-inversions/</guid>
      <description>This is the second part of two posts about using ggplot to visualize geometric transformations in the complex plane.
Inversions For this post we’ll focus on inversions, having already covered rotations, translations, and reflections. An inversion can be thought of as a reflection across a circle, the the inside of the circle gets flipped to fill the plane outside the circle and the outside is flipped into the circle. This is a more complicated transformation, both to visualize and to perform mathematically, but is essential to geometry.</description>
    </item>
    
    <item>
      <title>GGPlot and Geometric Transformations</title>
      <link>/post/2018-01-06-ggplot-and-geometric-transformations/</link>
      <pubDate>Sat, 06 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-06-ggplot-and-geometric-transformations/</guid>
      <description>I’m currently teaching a Geometry course, and wished there was an easy way to illustrate geometric transformations for my students. I’m sure they’ll agree I’m not a great artist.
Since R is my preferred way to draw any picture, I thought “Let’s use GGPlot to show transformations!”
For those not versed in geometry, we would like to easily visualize translations (shifts along a vector), rotations, and dilations of points (or collections of points) in the complex plane.</description>
    </item>
    
    <item>
      <title>Thoughts on Severe Class Imbalance </title>
      <link>/post/2018-01-01/</link>
      <pubDate>Tue, 02 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-01/</guid>
      <description>Besides lots of family time and the creation of this blog/website, this is what I’ve been thinking about over the winter break.
Background As part of my research in emergent reducibility, I’ve had to face a binary classification situation with severe class imbalance. Among brute-force searches, it seems that there’s roughly 1 case of emergent reducibility (what I’m looking for) for every 1 million irreducible cubic polynomials. It is known that there are infinitely many cubic polynomials with emergent reducibility.</description>
    </item>
    
  </channel>
</rss>