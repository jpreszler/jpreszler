error = function (...) {}, useInternalNodes = T)
# Get all elevations, times and coordinates via the respective xpath
elevations <- as.numeric(xpathSApply(pfile, path = "//trkpt/ele", xmlValue))
times <- xpathSApply(pfile, path = "//trkpt/time", xmlValue)
coords <- xpathSApply(pfile, path = "//trkpt", xmlAttrs)
speeds <- xpathSApply(pfile, path = "//trkpt/speed", xmlValue)
#convert speed from meters/sec to minutes/mile and clean
speeds <- 26.8224/as.numeric(speeds)
speeds[1] <- 0 #first speed is 0 m/s
speeds <- ifelse(speeds>12, mean(speeds),speeds)
speeds <- ifelse(speeds<5.5, mean(speeds),speeds)
#convert elevation to feet from meters
elevations <- elevations*3.28084
# Extract latitude and longitude from the coordinates
lats <- as.numeric(coords["lat",])
lons <- as.numeric(coords["lon",])
# Put everything in a dataframe and get rid of old variables
geodf <- data.frame(lat = lats, lon = lons, elev = elevations, time = times, pace=speeds)
rm(list=c("elevations", "lats", "lons", "pfile", "times", "coords", "speeds"))
geodf$time <- as.POSIXct(strptime(geodf$time, format = "%Y-%m-%dT%H:%M:%OS"))
geodf$elapsed.time <- difftime(geodf$time,geodf$time[1])/60
geodf$distance <- geodf$elapsed.time/geodf$pace
geodf$elev.offset <- geodf$elev - mean(geodf$elev[1:10])
geodf$elev.lag <- geodf$elev - lag(geodf$elev)
#geodf$pace.offset <- geodf$pace - lag(geodf$pace)
#geodf$total.elev.change <- cumsum(abs(geodf$elev.change))
return(geodf)
}
library(dplyr)
library(ggplot2)
indx <- read.table("/home/jpreszler/garmin-220/GPX/index-data.org", strip.white = TRUE, sep = "|", header=TRUE) %>% select(File, Location) %>% filter(Location == "Tacoma")
indxGC <- read.table("/home/jpreszler/garmin-220/GPX/gc-index.org", strip.white = TRUE, sep="|", header=TRUE) %>% select(File, Location) %>% filter(Location=="Tacoma")
for(i in 1:length(indx$File)){
run <- getRunDF(paste("/home/jpreszler/garmin-220/GPX/",indx$File[i],".gpx", sep="")) %>% mutate(Name = indx$File[i])
ifelse(i==1,runs <- run, runs <- rbind.data.frame(runs,run))
}
for(i in 1:length(indxGC$File)){
run <- getRunDF(paste("/home/jpreszler/garmin-220/GPX/from-gc/",indxGC$File[i],sep="")) %>% mutate(Name = indxGC$File[i])
runs <- rbind.data.frame(runs,run)
}
runs <- mutate(runs, apace = elapsed.time/cDist)
names(runs)
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
library(XML)
library(lubridate)
library(geosphere)
library(dplyr)
geocent.rad <- function(lat){
polar.rad <-6356.7523
equat.rad <- 6378.1370
sqrt(((equat.rad^2*cos(lat))^2+(polar.rad^2*sin(lat))^2)/((equat.rad*cos(lat))^2+(polar.rad*sin(lat))^2))
}
DHpath <- function(lon,lat,elevation){
d <- vector(mode="numeric", length=length(lon))
d[1] <- 0
for(i in 2:length(lon)){
d[i] <- distHaversine(c(lon[i-1],lat[i-1]),c(lon[i],lat[i]), r=geocent.rad(lat[i])*1000+(elevation[i]/3.28084))
}
return(d*0.000621371) #convert from meters to miles
}
getRunDF <- function(filename) {
pfile <- htmlTreeParse(filename,
error = function (...) {}, useInternalNodes = T)
# Get all elevations, times and coordinates via the respective xpath
elevations <- as.numeric(xpathSApply(pfile, path = "//trkpt/ele", xmlValue))
times <- xpathSApply(pfile, path = "//trkpt/time", xmlValue)
coords <- xpathSApply(pfile, path = "//trkpt", xmlAttrs)
speeds <- xpathSApply(pfile, path = "//trkpt/speed", xmlValue)
#convert speed from meters/sec to minutes/mile and clean
speeds <- 26.8224/as.numeric(speeds)
speeds[1] <- 0 #first speed is 0 m/s
speeds <- ifelse(speeds>12, mean(speeds),speeds)
speeds <- ifelse(speeds<5, mean(speeds),speeds)
#convert elevation to feet from meters
elevations <- elevations*3.28084
# Extract latitude and longitude from the coordinates
lats <- as.numeric(coords["lat",])
lons <- as.numeric(coords["lon",])
# Put everything in a dataframe and get rid of old variables
geodf <- data.frame(lat = lats, lon = lons, elev = elevations, time = times, pace=speeds)
rm(list=c("elevations", "lats", "lons", "pfile", "times", "coords", "speeds"))
geodf$time <- as.POSIXct(strptime(geodf$time, format = "%Y-%m-%dT%H:%M:%OS"))
geodf$elapsed.time <- as.numeric(difftime(geodf$time,geodf$time[1], units = "mins"))
geodf$distance <- as.numeric(geodf$elapsed.time/geodf$pace)
geodf$distH <- DHpath(geodf$lon,geodf$lat, geodf$elev)
geodf$cDist <- cumsum(geodf$distH)
geodf$elev.offset <- geodf$elev - mean(geodf$elev[1:3])
geodf$elev.lag <- geodf$elev - lag(geodf$elev)
geodf$updown <- ifelse(geodf$elev.lag>2,"up",ifelse(geodf$elev.lag<(-2),"down","flat"))
hills <- rle(geodf$updown)
geodf$hid <- rep(seq_along(hills$lengths), hills$lengths)
geodf$hlen <- hills$lengths[geodf$hid]
hilldf <- filter(geodf, hlen>4) %>% select(elapsed.time, cDist, pace, elev, hid, updown) %>% group_by(hid) %>% summarise(up=if(updown=="up") 1 else if(updown=="down") -1 else 0, grade = ((max(elev)-min(elev))/(max(cDist)-min(cDist)))*(1/5280)*up, hill.pace=mean(pace),hill.length = max(cDist)-min(cDist))
geodf <- left_join(geodf, hilldf, by="hid")
geodf$grade[is.na(geodf$grade)]<-0
#hillDF <- data.frame(values=hillDF$values,lengths=hillDF$lengths)
#geodf$pace.offset <- geodf$pace - lag(geodf$pace)
#geodf$total.elev.change <- cumsum(abs(geodf$elev.change))
return(geodf)
}
indx <- read.table("/home/jpreszler/garmin-220/GPX/index-data.org", strip.white = TRUE, sep = "|", header=TRUE) %>% select(File, Location) %>% filter(Location == "Tacoma")
indxGC <- read.table("/home/jpreszler/garmin-220/GPX/gc-index.org", strip.white = TRUE, sep="|", header=TRUE) %>% select(File, Location) %>% filter(Location=="Tacoma")
for(i in 1:length(indx$File)){
run <- getRunDF(paste("/home/jpreszler/garmin-220/GPX/",indx$File[i],".gpx", sep="")) %>% mutate(Name = indx$File[i])
ifelse(i==1,runs <- run, runs <- rbind.data.frame(runs,run))
}
for(i in 1:length(indxGC$File)){
run <- getRunDF(paste("/home/jpreszler/garmin-220/GPX/from-gc/",indxGC$File[i],sep="")) %>% mutate(Name = indxGC$File[i])
runs <- rbind.data.frame(runs,run)
}
names(runs)
summary(pace)
summary(runs$pace)
runs <- mutate(runs, apace = elapsed.time/cDist)
runs <- select(runs, -hid, -hill.pace, -hill.length)
summary(runs)
runs <- select(runs, -up, -updown, -distance, -hlen)
write.csv(runs, "/home/jpreszler/github-web/tacoma-runs/tacoma-runs.csv", row.names = FALSE)
rsconnect::deployApp(appDir = "/home/jpreszler/github-web/tacoma-runs/")
A <- 0+cumsum(rnorm(100, 0,1))
B <- 0+cumsum(rnorm(100, 0,1))
C <- 0+cumsum(rnorm(100, 0,1))
D <- 0+cumsum(rnorm(100, 0,1))
rw1d <- data.frame(A,B,C,D)
library(tidyr)
library(ggplot2)
head(rw1d)
rw1d$idx <- 1:100
rw1d %>% gather(key=walker, value=position, -idx) %>% ggplot(aes(x=position, y=idx, col=walker))+geom_line()
rw1d %>% gather(key=walker, value=position, -idx) %>% ggplot(aes(y=position, x=idx, col=walker))+geom_line()
install.packages("xkcd")
library(xkcd)
install.packages("xkcd", dependencies=TRUE)
install.packages("Hmisc")
install.packages("viridis")
devtools::install_github('sjmgarnier/viridis')
install.packages("xkcd", dependencies=TRUE)
library(xkcd)
rw1d %>% gather(key=walker, value=position, -idx) %>% ggplot(aes(y=position, x=idx, col=walker))+geom_line()+theme_xkcd()
rw1d %>% gather(key=walker, value=position, -idx) %>% ggplot(aes(y=position, x=idx, col=walker))+xkcdline()+theme_xkcd()
qt(.035, df=8)
1600-1573.23
#et-up a db connection
library(DBI)
db <- dbConnect(RSQLite::SQLite(), dbname = ":memory:")
?dbConnect
db <- dbConnect(RSQLite::SQLite(), dbname = "/home/jpreszler/gitlab/cs270-s18/data/jacs.sqlite")
dbListTables(db)
dbGetQuery(db, "SELECT DOI FROM Papers")
dbGetQuery(db, "SELECT DOI, COUNT(DISTINCT(paperID)) FROM Papers GROUP BY DOI")
db <- dbConnect(RSQLite::SQLite(), dbname = "/home/jpreszler/gitlab/cs270-s18/data/chinook.db")
dbListTables(db)
dbListFields(db, "sqlite_sequence")
dbListFields(db, "sqlite_stat1")
dbListFields(db, "genres")
dbListFields(db, "tracks")
shiny::runApp(appDir = "/home/jpreszler/gitlab/cs270-s18/SQL_Injection/")
shiny::runApp(appDir = "/home/jpreszler/gitlab/cs270-s18/SQL_Injection/")
shiny::runApp(appDir = "/home/jpreszler/gitlab/cs270-s18/SQL_Injection/")
shiny::runApp(appDir = "/home/jpreszler/gitlab/cs270-s18/SQL_Injection/")
shiny::runApp(appDir = "/home/jpreszler/gitlab/cs270-s18/SQL_Injection/")
?dbGetQuery
shiny::runApp(appDir = "/home/jpreszler/gitlab/cs270-s18/SQL_Injection/")
shiny::runApp(appDir = "/home/jpreszler/gitlab/cs270-s18/SQL_Injection/")
?dbSendQuery
?dbSendStatement
?dbExecute
shiny::runApp(appDir = "/home/jpreszler/gitlab/cs270-s18/SQL_Injection/")
install.packages("XKCDData")
install.packages("XKCDdata")
library(XKCDdata)
print_xkcd(327)
get_xkcd(327)
get_comic(327)
bt <-get_comic(327)
View(bt)
install.packages(c("BH", "DBI", "DT", "RSQLite", "Rcpp", "RcppArmadillo", "RcppEigen", "StanHeaders", "TTR", "XML", "bayesplot", "bindr", "bindrcpp", "blob", "blogdown", "bookdown", "broom", "callr", "caret", "curl", "dbplyr", "ddalpha", "devtools", "digest", "forcats", "forecast", "ggformula", "glmnet", "hms", "htmlwidgets", "httpuv", "igraph", "klaR", "knitr", "lava", "lmtest", "loo", "lubridate", "mapproj", "maps", "matrixStats", "mosaicData", "openssl", "pROC", "packrat", "pillar", "plogr", "prodlim", "psych", "quantmod", "rJava", "randomForest", "readxl", "rgdal", "rlang", "rmarkdown", "rsconnect", "rstantools", "selectr", "servr", "sfsmisc", "sp", "stringi", "stringr", "swirlify", "tibble", "tidyr", "tidyselect", "timeDate", "tseries", "viridis", "withr", "xts", "yaml"))
library(igraph)
p <- c("alex", "robert", "derek", "dominic", "kevin", "franco", "lucas", "andres", "mohammad")
sample(p, 3)
sample(p, 3)
sample(p, 3)
library(igraph)
library(gggraph)
install.packages("ggraph")
library(rvest)
install.packages("rvest")
library(rvest)
acc<-html("http://collegeofidaho.smartcatalogiq.com/current/Undergraduate-Catalog/Courses/ACC-Accounting")
acc<-read_html("http://collegeofidaho.smartcatalogiq.com/current/Undergraduate-Catalog/Courses/ACC-Accounting")
ac
acc
html_nodes(acc)
html_nodes(acc,*)
View(acc)
View(acc)
html_nodes(acc,'div id="main"')
html_table(acc)
html_table("http://collegeofidaho.smartcatalogiq.com/current/Undergraduate-Catalog/Courses/ACC-Accounting")
html_table('http://collegeofidaho.smartcatalogiq.com/current/Undergraduate-Catalog/Courses/ACC-Accounting')
?html_table
?html_node
html_nodes(acc, "div")
html_nodes(acc, "div id=\"main\"")
html_nodes(acc, "ul")
acc_ul_nodes<-html_nodes(acc, "ul")
acc_ul_nodes[1]
View(acc_ul_nodes)
xml_attrs(xml_child(xml_child(acc_ul_nodes[[5]], 1), 1))
xml_attrs(xml_child(xml_child(acc_ul_nodes[[5]], 1), 1))[["href"]]
xml_attrs(xml_child(acc_ul_nodes[[5]], 1))
xml_attrs(xml_child(xml_child(xml_child(acc_ul_nodes[[5]], 1), 1), 1))
subject_url <-"http://collegeofidaho.smartcatalogiq.com/en/current/Undergraduate-Catalog/Courses"
subs <- read_html(subject_url)
subs
subs[2]
subs[[2]]
View(subs)
xml_attrs(xml_child(subs, 2))
xml_attrs(xml_child(xml_child(subs, 2), 1))[["action"]]
html_nodes(subs, ".hasChildren")
subList <- html_nodes(subs, ".hasChildren")
subList[19]
View(subList)
subList[[13:(13+47)]]
subList[13:(13+47)]
install.packages("igraph")
library(rvest)
library(stringr)
library(dplyr)
#url for courses in catalog
base_url <- "http://collegeofidaho.smartcatalogiq.com"
base_url_ext <- "/en/current/Undergraduate-Catalog/Courses"
base_html <- read_html(paste0(base_url,base_url_ext))
#extract links from base page
subjectLinks <- html_nodes(base_html, 'a')
#convert links to text
subjectText <- html_text(subjectLinks)
#pick out corresponds to subject list
#we now have subject codes and names
subjectText <- subjectText[78:120]
subDF <- str_remove_all(subjectText, ' ') %>% str_split(pattern= '-', n=2, simplify=TRUE) %>% as.data.frame()
names(subDF) <- c("sub", "subject")
#get url for subject, for each row in subDF
subDF <-subDF %>% mutate(url= paste0(base_url,base_url_ext,'/',sub,'-',str_replace_all(subject,' ','-')))
library(dplyr)
install.packages("dplyr")
library(dplyr)
subDF <- str_remove_all(subjectText, ' ') %>% str_split(pattern= '-', n=2, simplify=TRUE) %>% as.data.frame()
names(subDF) <- c("sub", "subject")
#get url for subject, for each row in subDF
subDF <-subDF %>% mutate(url= paste0(base_url,base_url_ext,'/',sub,'-',str_replace_all(subject,' ','-')))
View(subDF)
library(purrr)
install.packages("purrr")
acc_url <- subDF$url
acc_links <- html_nodes(read_html(acc_url), 'a')
acc_html <- read_html(acc_url)
acc_url
acc_url <- subDF$url[1]
acc_links <- html_nodes(read_html(acc_url), 'a')
acc_text <- html_text(acc_links)
acc_text
acc_url <- subDF$url[2]
acc_links <- html_nodes(read_html(acc_url), 'a')
acc_text <- html_text(acc_links)
acc_text
library(purrr)
str_detect(acc_text, 'ART-')
acc_text[str_detect(acc_text, 'ART-')]
#url for courses in catalog
base_url <- "http://collegeofidaho.smartcatalogiq.com"
base_url_ext <- "/en/current/Undergraduate-Catalog/Courses"
base_html <- read_html(paste0(base_url,base_url_ext))
#extract links from base page
subjectLinks <- html_nodes(base_html, 'a')
library(rvest)
library(stringr)
library(dplyr)
library(purrr)
#url for courses in catalog
base_url <- "http://collegeofidaho.smartcatalogiq.com"
base_url_ext <- "/en/current/Undergraduate-Catalog/Courses"
base_html <- read_html(paste0(base_url,base_url_ext))
#extract links from base page
subjectLinks <- html_nodes(base_html, 'a')
#convert links to text
subjectText <- html_text(subjectLinks)
#pick out what corresponds to subject list
#we now have subject codes and names
subjectText <- subjectText[78:120]
subDF <- str_split(pattern= '-', n=2, simplify=TRUE) %>% str_trim(side="both") %>% as.data.frame()
subDF <- str_split(subjectText,pattern= '-', n=2, simplify=TRUE) %>% str_trim(side="both") %>% as.data.frame()
names(subDF) <- c("sub", "subject")
View(subDF)
subDF <- str_split(subjectText,pattern= '-', n=2, simplify=TRUE) %>% as.data.frame()
names(subDF) <- c("sub", "subject")
subDF <-mutate(subDF, sub=str_trim(sub, side="both"), subject=str_trim(subject, side="both"))
#get url for subject, for each row in subDF
subDF <-subDF %>% mutate(url= paste0(base_url,base_url_ext,'/',sub,'-',str_replace_all(subject,' ','-')))
get_class_list <- function(i){
#get list of links on subject page
class_links <- html_nodes(read_html(subDF$url[i]), 'a')
#turn links to text
class_list <- html_text(class_links)
#only keep links for classes, each subject has
#classes starting in a different position
class_list <- class_list[str_detect(class_list, paste0(subDF$sub[i],'-'))]
#now build class dataframe with sub,number,url - we'll get the
#name and other details on the next scrape
return(class_list)
}
map_df(1:length(subDF$sub), get_class_list)
get_class_list(3)
get_class_list(4)
get_class_list(5)
get_class_list <- function(i){
#get list of links on subject page
class_links <- html_nodes(read_html(subDF$url[i]), 'a')
#turn links to text
class_list <- html_text(class_links)
#only keep links for classes, each subject has
#classes starting in a different position
class_list <- class_list[str_detect(class_list, paste0(subDF$sub[i],'-'))]
#now build class dataframe with sub,number,url - we'll get the
#name and other details on the next scrape
print(head(class_list))
return(class_list)
}
map_df(1:length(subDF$sub), get_class_list)
install.packages("kableExtra")
library(rvest)
library(stringr)
library(dplyr)
library(purrr)
library(tidyr)
#url for courses in catalog
base_url <- "http://collegeofidaho.smartcatalogiq.com"
base_url_ext <- "/en/current/Undergraduate-Catalog/Courses"
base_html <- read_html(paste0(base_url,base_url_ext))
#extract links from base page
subjectLinks <- html_nodes(base_html, 'a')
#convert links to text
subjectText <- html_text(subjectLinks)
sub_url <- html_attr(subjectLinks, 'href')[78:120]
#pick out what corresponds to subject list
#we now have subject codes and names
subjectText <- subjectText[78:120]
subDF <- str_split(subjectText,pattern= '-', n=2, simplify=TRUE) %>% as.data.frame()
names(subDF) <- c("sub", "subject")
subDF <-mutate(subDF, sub=str_trim(sub, side="both"), subject=str_trim(subject, side="both"))
#get url for subject, for each row in subDF
#subDF <-subDF %>% mutate(url= paste0(base_url,base_url_ext,'/',sub,'-',str_replace_all(subject,' ','-')))
subDF <- mutate(subDF, url=paste0(base_url,sub_url))
#hack to fix ENV link issue -- not needed since url is pulled from html_attr
#envStud_url <- subDF[subDF$sub=='ENV',]$url
#subDF[subDF$sub=='ENV',]$url <- str_replace(envStud_url, 'Studies', 'Science')
get_class_list <- function(i){
#get list of links on subject page
class_links <- html_nodes(read_html(subDF$url[i]), 'a')
#turn links to text
class_list <- html_text(class_links)
class_url <- html_attr(class_links, 'href')
classDF <- data.frame(list=class_list, url=class_url)
#only keep links for classes, each subject has
#classes starting in a different position
classDF <- classDF %>% filter(str_detect(list, paste0(subDF$sub[i],'-')))
#now build class dataframe with sub,number,name,url - we'll get the
#other details on the next scrape
#We'll split on 1st space, discard everything after it and use
#what's before it to build the required DF
classDF <- separate(classDF, list, into=c("id", "name"),sep=" ",extra="merge")
#two theater classes have typos -THE-###
#this is solely dealing with that
classDF$id <- str_replace(classDF$id, "-THE", "THE")
#back to normal
classDF <- separate(classDF, id, into=c("sub", "number"), sep="-")
#the id field has the last part of the new url, we need the
#subject url with the course level (100,200,etc) then id
classDF <- mutate(classDF, url=paste0(base_url,url))
#several class names have '\n' in them, let's remove that now
classDF <- mutate(classDF, name=str_replace_all(name, '\n',' '))
return(classDF)
}
classes <- map_dfr(1:length(subDF$sub), get_class_list)
setwd("~/github-web/peak-neo4j")
write.csv(classes, "data/class-list-2017-2018.csv", row.names=FALSE)
setwd("~/github-web/jpreszler")
blogdown::serve_site()
install.packages("blogdown")
blogdown::serve_site()
install.packages("read_xml")
?read_xml
install.packages("xml2")
install.packages("xml2")
library(xml2)
blogdown::serve_site()
blogdown::serve_site()
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(rvest)
library(dplyr)
library(tidyr)
library(stringr)
library(purrr)
library(knitr)
library(kableExtra)
base_url <- "http://collegeofidaho.smartcatalogiq.com"
base_url_ext <- "/en/current/Undergraduate-Catalog/Courses"
#read base page
base_html <- read_html(paste0(base_url,base_url_ext))
#extract links from base page
subjectLinks <- html_nodes(base_html, 'a')
#convert links to text
subjectText <- html_text(subjectLinks)
subjectText <- subjectText[78:120]
#keep url for each subject also
sub_url <- html_attr(subjectLinks, 'href')[78:120]
subDF <- str_split(subjectText,pattern= '-', n=2, simplify=TRUE) %>% as.data.frame()
names(subDF) <- c("sub", "subject")
subDF <-mutate(subDF, sub = str_trim(sub, side = "both"), subject = str_trim(subject, side = "both"))
kable(head(subDF),"html") %>%  kable_styling( bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width=FALSE)
get_class_list <- function(i){
#get list of links on subject page
class_links <- html_nodes(read_html(subDF$url[i]), 'a')
#turn links to text
class_list <- html_text(class_links)
class_url <- html_attr(class_links, 'href')
classDF <- data.frame(list=class_list, url=class_url)
#only keep links for classes, each subject has
#classes starting in a different position
classDF <- classDF %>% filter(str_detect(list, paste0(subDF$sub[i],'-')))
#We'll split on 1st space, discard everything after it and use
#what's before it to build the required DF
classDF <- separate(classDF, list, into=c("id", "name"), sep = " ", extra = "merge")
#two theater classes have typos -THE-###
#this is solely dealing with that
classDF$id <- str_replace(classDF$id, "-THE", "THE")
#back to normal
classDF <- separate(classDF, id, into=c("sub", "number"), sep="-")
#the id field has the last part of the new url, we need the
#subject url with the course level (100,200,etc) then id
classDF <- mutate(classDF, url=paste0(base_url,url))
#several class names have '\n' in them, let's remove that now
classDF <- mutate(classDF, name=str_replace_all(name, '\n',' '))
return(classDF)
}
classes <- map_dfr(1:length(subDF$sub), get_class_list)
blogdown::serve_site()
blogdown::serve_site()
library(tm)
blogdown::stop_server()
library(tm)
install.packages("tm")
library(tm)
install.packages("SnowballC")
library(SnowballC)
wordFreq <- Corpus(VectorSource(classes$name))
wordFreq <- tm_map(wordFreq, PlainTextDocument)
wordFreq <- tm_map(wordFreq, content_transformer(tolower))
wordFreq <- tm_map(wordFreq, removePunctuation)
wordFreq <- tm_map(wordFreq, removeWords, c("a", "the", "and"))
wordFreq <- tm_map(wordFreq, stripWhitespace)
wordFreq <- tm_map(wordFreq, stemDocument)
View(wordFreq)
install.packages("wordcloud")
library(wordcloud)
wordcloud(wordFreq)
warnings()
wordTDM <- TermDocumentMatrix(wordFreq)
View(wordTDM)
wordTDM<- wordTDM %>% as.matrix() %>% rowSums() %>% sort()
wordDF <- data.frame(word=names(wordTDM), freq=wordTDM)
View(wordDF)
wordcloud(words=wordDF$word, freq = wordDF$freq)
wordcloud(words=wordDF$word, freq = wordDF$freq, min.freq = 10)
warnings()
install_github("lchiffon/wordcloud2")
library(wordcloud2)
library(devtools)
install.packages("devtools")
library(devtools)
install_github("lchiffon/wordcloud2")
library(wordcloud2)
install_github("lchiffon/wordcloud2")
library(wordcloud2)
letterCloud(wordDF, word="Yotes")
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
?tm_map
wordcloud(words=wordDF$word, freq=wordDF$freq, min.freq = 10, rot.per=.35, random.order = FALSE)
wordFreq <- Corpus(VectorSource(classes$name))
wordFreq <- tm_map(wordFreq, PlainTextDocument)
wordFreq <- tm_map(wordFreq, content_transformer(tolower))
wordFreq <- tm_map(wordFreq, removePunctuation)
wordFreq <- tm_map(wordFreq, removeWords, c("a", "the", "and", "for"))
wordFreq <- tm_map(wordFreq, stripWhitespace)
wordTDM <- TermDocumentMatrix(wordFreq)
wordTDM<- wordTDM %>% as.matrix() %>% rowSums() %>% sort()
wordDF <- data.frame(word=names(wordTDM), freq=wordTDM)
wordcloud(words=wordDF$word, freq=wordDF$freq, min.freq = 10, rot.per=.25, random.order = FALSE)
wordcloud(words=wordDF$word, freq=wordDF$freq, min.freq = 10,  random.order = FALSE)
?wordcloud
wordcloud(words=wordDF$word, freq=wordDF$freq, min.freq = 10,  random.order = FALSE, scale = (8,.5))
wordcloud(words=wordDF$word, freq=wordDF$freq, min.freq = 10,  random.order = FALSE, scale = c(8,.5))
wordcloud(words=wordDF$word, freq=wordDF$freq, min.freq = 10,  random.order = FALSE, scale = c(3,.5))
wordcloud(words=wordDF$word, freq=wordDF$freq, min.freq = 10,  random.order = FALSE, rot.per = .25, scale = c(3,.5))
pal <- brewer.pal(9, "Purples")[-(1:3)]
wordcloud(words=wordDF$word, freq=wordDF$freq, min.freq = 10,  random.order = FALSE, rot.per = .25, scale = c(3,.5), colors = pal)
blogdown::serve_site()
